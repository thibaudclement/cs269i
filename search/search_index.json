{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Computers/algorithms control a lot of economic processes, including online retail, online advertising, algo-trading, cryptocurrencies, etc. There is a need to jointly understand economics and computation to analyze these applications. While relying on an increase of computing power (more GPUs) may be an option to compensate for inefficiencies, some specific cases may actually justify the need for efficient algorithms.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>When it comes to incentives, leveraging more GPUs is not the solution. Computer scientists often need to build systems that interact with other agents, such as users, algorithms, Large Language Models, and even other systems. It is crucial to consider that these other agents are going to behave selfishly to predict how they are going to interact with the system, and design an implement said accordingly.</p>"},{"location":"#goals","title":"Goals","text":"<p>CS269I aims to teach students how to think about incentives (through lagnauge, frameworks, and practice), to get familiar with economic theory concepts (i.e. stable matching, proof-of-work, etc.), and analyze case studies to understand potential gaps between theory and practice.</p>"},{"location":"#credit","title":"Credit","text":"<p>These course notes for Stanford CS269I are based on the content of Aviad Rubinstein's lectures in the Spring 2025 quarter. This site is developed by Thibaud Clement, based on Eric Gao's original course notes from the Winter 2023 quarter.</p>"},{"location":"lecture1/","title":"One-Sided Matching and Serial Dictatorship","text":"<p>(3/31/2025)</p> <p>The Stanford Undergrad Housing Problem:</p> <ul> <li>There are \\( n \\) students, each with some preference over dorms.</li> <li>There are \\( m \\) dorms, each with some capacity (for simplicity, assume capacity is 1 for every dorm).</li> </ul> <p>How can we assign students to dorms?</p>"},{"location":"lecture1/#max-weight-matching","title":"Max Weight Matching","text":"<p>One possible solution is to maximize total happiness:</p> <p>Mechanism: Happiness Maximization</p> <ol> <li>Create a bipartite graph where:<ul> <li>One side represents students.</li> <li>The other side represents dorms.</li> <li>Each edge from a student to a dorm represents how much that student likes that dorm.</li> </ul> </li> <li>Determine the max-weight matching (i.e., Hungarian Algorithm).</li> </ol> <p>Key question: How does the algorithm determine how much each student likes each residency? Only the students themselves know how much they like each dorm, so we need to rely on students to tell the algorithm.</p> <p>Challenge: Each student has an incentive to exaggerate how much they like their favorite dorm and undercut how much they like other dorms (using a \"0\" or \"negative infinity\" weight for those dorms). In other words: max-weight bipartite matching allows students to game the system.</p> <p>This occurs because finding the matching that maximizes total happiness is conceptually right, but this naive max-weight matching fails to take incentives into account. Another possible algorithm is Serial Dictatorship.</p>"},{"location":"lecture1/#serial-dictatorship","title":"Serial Dictatorship","text":"<p>This is how Stanford actually assigns dorms to students.</p> <p>Mechanism: Serial Dictatorship</p> <ol> <li>Sort students in some fixed order (random, seniority, alphabetically, etc.).</li> <li>Go through the list in that order and allow each student (the \"dictator\") to select their most preferred available dorm.</li> </ol> <p>How is this better? There can still be students unhappy with their result. Indeed, a common complaint about Serial Dictatorship is unfairness: the first student chooses whichever dorm they want, while the last student gets only the last pick.</p> <p>When sorting is random, Serial Dictatorship guarantees ex-ante fairness: before the random sorting, all students have equal chances. However, after the sorting happens (even randomly), Serial Dictatorship loses fairness: there's no guarantee of ex-post fairness.</p> <p>Definition: Mechanism</p> <p>A mechanism consists of three things:</p> <ol> <li>A method of collecting inputs from agents,</li> <li>An algorithm that acts on the inputs,</li> <li>An action taken based on the output of the algorithm.</li> </ol> <p>In our context (Stanford Undergrad Housing Problem), based on inputs (dorm preferences) and the algorithm (available dorm after each student's pick), the mechanism takes actions (it assigns dorms).</p> <p>Note: All three components matter because there's a feedback loop: how we use inputs impacts what students report as preferences. The naive bipartite matching approach encourages students to game the system by misreporting their preferences.</p> <p>Thus, when designing a mechanism, we must consider:</p> <ul> <li>The algorithm itself,</li> <li>How/where inputs come from,</li> <li>Actions taken based on inputs,</li> <li>How promised actions affect reported inputs.</li> </ul> <p>Definition: Strategyproofness/Truthfulness</p> <p>A mechanism is strategyproof if it's in every agent's best interest to act truthfully, i.e., to report true preferences.</p> <p>Even more formally, game theorists somtimes use the term Dominant Strategy Incentive Compatible:</p> <p>Definition: Dominant Strategy Incentive Compatible - DSIC</p> <p>A mechanism is dominant strategy incentive compatible if truthfulness is a dominant strategy for each participant. That is, being truthful is a best response regardless of other players' actions.</p> <p>Theorem: You Cannot Game Serial Dictatorship</p> <p>It is in every student's best interest to choose their favorite available dorm in their turn. Formally, we say that Serial Dictatorship is strategyproof or truthful.</p> <p>Proof:</p> <ul> <li>Your room choice doesn't affect room availability before your turn.</li> <li>When your turn arrives, your best action is choosing your favorite available room.</li> </ul> <p>Why does this prove that Serial Dictatorship is strategyproof? Until your turn comes, it is other students who are bidding, and there is nothing you can do to affect it. However, when comes your turn, you choose what you get, so you should choose the best thing for you.</p> <p>Definition: Pareto Optimality</p> <p>An assignment \\( A \\) is Pareto optimal if, for any other assignment \\( B \\), there's at least one participant strictly preferring \\( A \\) over \\( B \\).</p> <p>Theorem: You Cannot Make Everyone Happier Without Making Someone Sadder</p> <p>Serial Dictatorship assignments are Pareto Optimal.</p> <p>Proof:</p> <p>Assume, for the sake of contradiction, that a different assignment exists making everyone as happy or happier.</p> <p>Consider the first student assigned differently: all better dorms are already assigned identically to students before them.</p> <p>Thus, the first differing student gets a worse dorm and becomes less happy\u2014we have reached a contradiction.</p> <p>Therefore, Serial Dictatorship is Pareto Optimal.</p>"},{"location":"lecture1/#additional-discussion","title":"Additional Discussion","text":"<p>Why is truthfulness good? It prevents corruption and ensures fairness by removing any insider advantage.</p> <p>When is truthfulness important? When optimizing happiness, truthful inputs ensure correct objectives.</p> <p>When is relaxing truthfulness acceptable?  In contexts with social norms where truthfulness isn't expected (e.g., poker games), strategyproofness isn't crucial.</p> <p>Fairness/equity issues with Serial Dictatorship: Fairness depends on sorting order. Seniority or randomness provide fairness ex-ante but not necessarily ex-post.</p> <p>Is there any other flaws in Serial Dictatorship? A dictator's seemingly insignificant decision may have huge impact on other agents. For instance, by the time my turn arrive, only my 9th and 10th choices are available. Since I am almost indifferent between them, I break ties and take my 9th choice. However, it is possible that my 9th choice was in fact someone else's top choice and they really wanted it. This may be even worse if their 2nd-10th choices happen to be already taken.</p>"},{"location":"lecture1/#recap","title":"Recap","text":"<ul> <li>Definition: Mechanism. A mechanism meaning soliciting inputs, running an algorithm, and taking actions.</li> <li>Definition: Strategyproof/truthful. A mechanism is strategyproof/truthful is misreporting preferences can never make a participant better off.</li> <li>Defintion: Pareto-optimal. An assignment A is Pareto-optimal if for any other assignment B, there is a participan that (strictly) prefers A over B.</li> </ul>"},{"location":"lecture2/","title":"Stable Two-Sided Matchings","text":"<p>(4/2/2025)</p> <p>After medical school, med students start their internship called a \"residency.\" Each (prospective) doctor has preferences over hospitals, and each hospital has preferences over doctors. How should doctors and hospitals be matched?</p> <p>Key Nuances:</p> <ul> <li>The biggest difference between doctor-residency matching and student-dorm matching is that we now have two-sided preferences (each doctor and hospital have preferences over each other).</li> <li>Matching students to dorms is a centralized process; matching doctors to hospitals requires incentivizing participants to use a centralized process to prevent side deals.</li> </ul>"},{"location":"lecture2/#stable-matching","title":"Stable Matching","text":"<p>Definition: Blocking Pair</p> <p>Given a match \\(M\\), the pair (doctor \\(i\\), hospital \\(j\\)) forms a blocking pair if they prefer each other to their current assignments in \\(M\\).</p> <p>For example, if Doctor \\(n\\) prefers Stanford over UCSF and Stanford prefers Doctor \\(n\\) over Doctor 1, who is currently matched, then (Doctor \\(n\\), Stanford) form a blocking pair. This results in an Unstable Matching.</p> <p>Note: Blocking pairs exist only in two-sided matching markets.</p> <p>Definition: Stable Matching</p> <p>A matching \\(M\\) is stable if there are no blocking pairs. Equivalently, for every unmatched pair \\((i,j)\\), either:</p> <ul> <li>Doctor \\(i\\) prefers Hospital \\(M(i)\\) over Hospital \\(j\\), or;</li> <li>Hospital \\(j\\) prefers Doctor \\(M(j)\\) over Doctor \\(i\\).</li> </ul> <p>Key Point: Stability removes incentives to deviate from the centralized matching.</p>"},{"location":"lecture2/#deferred-acceptance","title":"Deferred Acceptance","text":"<p>Main idea: Each doctor proposes to their favorite hospital that hasn't rejected them yet. Hospitals accept the best available candidate.</p> <p>Mechanism: Deferred Acceptance</p> <p>While there's an unmatched doctor \\(i\\):</p> <ol> <li>Doctor \\(i\\) proposes to their next-favorite hospital \\(j\\).</li> <li>If hospital \\(j\\) has no match, they accept doctor \\(i\\).</li> <li>Else, if hospital \\(j\\) prefers their current match over doctor \\(i\\), doctor \\(i\\) remains unmatched.</li> <li>Else, hospital \\(j\\) matches with doctor \\(i\\), releasing their previous match.</li> </ol> <p>The algorithm stops when everyone is matched.</p> <p>Example</p> <ul> <li>Doctors: Alice (X, Y, Z), Bob (Y, X, Z), Charlie (Y, Z, X).</li> <li>Hospitals: X (Bob, Alice, Charlie), Y (Alice, Bob, Charlie), Z (Bob, Charlie, Alice).</li> </ul> <p>Stable matching result: \\((Alice, X), (Bob, Y), (Charlie, Z)\\).</p> <p>Theorem: Runtime of Deferred Acceptance</p> <p>Deferred Acceptance runs in \\(O(n^2)\\) time.</p> <p>Proof:</p> <ul> <li>There are at most \\(n^2\\) proposals (each doctor proposes at most \\(n\\) hospitals).</li> <li>Each proposal is \\(O(1)\\), hence total runtime is \\(O(n^2)\\).</li> </ul> <p>Theorem: Deferred Acceptance is Stable</p> <p>Deferred Acceptance outputs a complete stable matching.</p> <p>Proof (sketch): Proven by three claims:</p> <ol> <li>Current match stable at each iteration.</li> <li>Once matched, hospitals remain matched.</li> <li>At completion, everyone is matched.</li> </ol> <p>Therefore, no blocking pairs exist.</p> <p>Theorem: Efficiency of Stable Matchings</p> <p>Every stable matching is Pareto-optimal.</p> <p>Proof: Any deviation from a stable matching worsens at least one participant\u2019s outcome.</p> <p>Theorem: Proposing Optimality</p> <p>Doctor-proposing Deferred Acceptance yields the doctor-optimal stable matching.</p> <p>Corollary: Doctor-proposing DA is strategyproof for doctors.</p> <p>Theorem: Receiving Non-Optimality</p> <p>Doctor-proposing Deferred Acceptance yields the worst stable matching for hospitals.</p> <p>Theorem: Receiving Non-Strategyproofness</p> <p>Hospitals can benefit from misreporting preferences.</p> <p>Think-Pair-Share: How to make DA hospital-optimal? Flip roles (hospital-proposing).</p>"},{"location":"lecture2/#deferred-acceptance-in-practice","title":"Deferred Acceptance in Practice","text":"<p>Why no DA in US undergrad admissions? Decentralized applications, holistic admission processes, larger applicant pools.</p> <p>Historical Doctor-Hospital Matching:</p> <ul> <li>1950s: DA-like algorithms initially used.</li> <li>1960s: Couples complicate preferences.</li> <li>1980s: Negative results\u2014stable matching existence with couples is NP-complete.</li> <li>1990s: Extended DA for couples adopted.</li> </ul> <p>Practical Doctor Ranking:</p> <ul> <li>Interviews (costly)</li> <li>Tests (changing role due to USMLE)</li> <li>Safety choices (not actually safer)</li> </ul> <p>Theorem: No Improvements from Safety Choices</p> <p>Misranking doctors (safety choices) does not secure a better match.</p> <p>Proof: Misreporting can only worsen or have no effect on outcomes.</p> <p>Hospitals use safety choices due to ignorance or prestige metrics (\"number needed to fill\").</p> <p>Other DA Applications:</p> <ul> <li>Routing Network Packets: Distributed, truncated preference lists.</li> <li>Stanford Marriage Pact: Originally DA-based, now max-weight matching; stability less critical.</li> </ul> <p>Recap: DA theoretically optimal but practical challenges (evaluating preferences) remain significant.</p>"},{"location":"lecture3/","title":"Online Learning and Regret Minimization","text":"<p>(4/9/2025)</p> <p>Previously, agents had dominant strategies. Now, what should agents do without dominant strategies or even knowledge of the game's rules? Let's explore from a single agent's perspective.</p>"},{"location":"lecture3/#regret-minimization","title":"Regret Minimization","text":"<p>Consider investing in stocks without knowledge of future performance:</p> <ul> <li>There are \\(n\\) possible actions (stocks).</li> <li>We run an algorithm over \\(T\\) days.</li> <li>On day \\(t\\), algorithm picks action \\(ALG(t)\\).</li> <li>Reward for action \\(i\\) on day \\(t\\) is \\(r_{i,t}\\), bounded by \\(-1 \\leq r_{i,t} \\leq 1\\).</li> </ul> <p>Our goal: Maximize \\(\\sum r_{ALG(t),t}\\).</p> <p>Key Idea #1: Use historical performance to inform decisions.</p> <p>Algorithm: Follow The Leader (FTL)</p> <p>On day \\(t\\): Take the action with the highest total reward up to day \\(t-1\\).</p> <p>To measure success, we use \"regret\":</p> <p>Definition: External Regret</p> <p>External Regret = \\( \\underset{i}{\\max} \\sum_t r_{i,t} - \\sum_t r_{ALG(t),t} \\)</p> <p>Claim 1: For independently, identically distributed (iid) rewards, FTL's expected regret is \\(O(\\sqrt{T \\log(n)})\\).</p> <p>Claim 2: No algorithm can outperform \\(\\Omega(\\sqrt{T \\log(n)})\\) with iid rewards.</p>"},{"location":"lecture3/#regret-minimization-algorithms","title":"Regret Minimization Algorithms","text":"<p>FTL fails under adversarial conditions. Thus, we add randomness:</p> <p>Key Idea #4: Balance randomness with historical performance.</p> <p>Algorithm: Follow The Regularized Leader (FTRL)</p> <p>On day \\(t\\), choose distribution \\(x\\) maximizing: [\\sum_i r_{x,t} - \\frac{1}{\\eta}\\varphi(x)] - \\(\\eta\\): Balances randomness and history. - \\(\\varphi\\): Regularizer penalizing unbalanced distributions.</p> <p>Alternative (equivalent) algorithm: Multiplicative Weight Update (MWU)</p> <p>Algorithm: Multiplicative Weight Update (MWU)</p> <ul> <li>Initialize weights \\(z_{i,0} = 1\\).</li> <li>At day \\(t\\):</li> <li>Choose action \\(i\\) with probability \\(\\frac{z_{i,t}}{\\sum_j z_{j,t}}\\).</li> <li>Update weights: \\(z_{i,t+1} \\leftarrow z_{i,t} e^{\\eta r_{i,t}}\\).</li> </ul> <p>MWU and FTRL achieve expected regret \\(O(\\sqrt{T \\log(n)})\\) even with adversarial input.</p>"},{"location":"lecture3/#swap-regret-minimization-algorithms","title":"Swap-Regret Minimization Algorithms","text":"<p>Definition: Swap-Regret (Internal Regret)</p> <p>Swap-Regret = \\( \\underset{\\Phi : [n] \\to [n]}{\\max} \\sum_t (r_{\\Phi(ALG(t)),t} - r_{ALG(t),t}) \\)</p> <p>where \\(\\Phi\\) maps each action to another action.</p> <p>Swap-regret is always at least external regret. Minimizing swap-regret is harder but better.</p> <p>Algorithm: Swap-Regret Minimization</p> <ul> <li>Define meta-actions for each possible swap choice \\(\\Phi\\).</li> <li>Run MWU/FTRL on meta-actions.</li> <li>At each iteration, solve for distribution \\(x_t = \\mathbb{E}[\\Phi(x_t)]\\).</li> </ul> <p>Total Swap-Regret: \\(O(\\sqrt{T n \\log(n)})\\)</p>"},{"location":"lecture3/#regret-minimization-with-bandit-feedback","title":"Regret Minimization with Bandit Feedback","text":"<p>With partial (bandit) feedback, we can't observe rewards of actions we didn't take.</p> <p>Algorithm: UCB1 (Upper Confidence Bound)</p> <p>Define: \\(UCB_i = \\text{avg reward}_i + \\sqrt{\\frac{2\\ln(T)}{\\text{# samples of } i}}\\).</p> <ul> <li>On day \\(t\\), pick arm with max UCB.</li> </ul> <p>With iid rewards, UCB1 regret is \\(O(\\sqrt{n T \\log(T)})\\).</p> <p>Algorithm: Exp3 (Exponential-weight for Exploration and Exploitation)</p> <ul> <li>On day \\(t\\), MWU chooses an arm.</li> <li>Pseudo-rewards:</li> <li>If arm not selected: \\(\\hat{r}_{i,t}=0\\)</li> <li>If arm selected: \\(\\hat{r}_{i,t}=\\frac{r_{i,t}}{Pr[\\text{selecting }i]}\\) (Inverse Propensity Score)</li> </ul> <p>Exp3 achieves expected regret \\(O(\\sqrt{n T \\log(n)})\\) with adversarial input.</p> <p>Think-Pair-Share: Should UCB or Exp3 be used practically (e.g., news feed editor)? Consider contextual factors, clickbait, user addiction, and fake news.</p>"},{"location":"lecture3b/","title":"Top Trading Cycles","text":"<p>(1/18/2023)</p> <p>Note: This lecture was skipped during the Spring 2025 quarter. Below are notes from the Winter 2023 quarter.</p> <p>Previously, we studied:</p> <ul> <li>Random Serial Dictatorship (one-sided matching)</li> <li>Deferred Acceptance (two-sided matching)</li> </ul> <p>This lecture: What happens when participants are already endowed with goods?</p> <p>Example: Stanford PhD housing renewal\u2014students face tradeoffs between renewal and lottery entry.</p> <p>Problem Setup:</p> <ul> <li>\\(n\\) students, \\(n\\) rooms</li> <li>Each student has a current room and preference over all rooms</li> </ul> <p>Mechanism: Top Trading Cycles (TTC)</p> <p>While there are unmatched students/rooms:</p> <ol> <li>Create a graph with unmatched rooms and students as nodes.</li> <li>Draw edges:</li> <li>from each room to its current owner.</li> <li>from each student to their most preferred available room.</li> <li>Identify cycles, remove them, and execute the trades indicated by cycles.</li> </ol> <p>Theorem: Runtime of TTC</p> <p>Top Trading Cycles runs in \\(O(n^2)\\) time.</p> <p>Proof: Constructing graph and edges is \\(O(n)\\). Finding cycles via directed edges visits at most \\(2n+1\\) nodes (pigeonhole principle), thus \\(O(n)\\). Each iteration removes at least one student; thus, overall complexity is \\(O(n^2)\\).</p> <p>Input size is \\(n^2\\) (preference lists of length \\(n\\)), so TTC is linear relative to input size.</p> <p>Definition: Individual Rationality (IR)</p> <p>A mechanism is individually rational if no agent is worse off after participating.</p> <p>Theorem: TTC is Individually Rational</p> <p>Proof: Students trade rooms only if they prefer the new room.</p> <p>Theorem: Strategyproofness of TTC</p> <p>Top Trading Cycles is strategyproof.</p> <p>This proof was noted as flawed; a corrected proof was expected.</p> <p>Theorem: Efficiency of TTC</p> <p>TTC allocation is Pareto-optimal.</p> <p>Proof: Equivalent to serial dictatorship in order cycles formed. Serial dictatorship is Pareto-optimal, thus TTC is Pareto-optimal.</p>"},{"location":"lecture3b/#top-trading-cycles-and-chains-ttcc","title":"Top Trading Cycles and Chains (TTCC)","text":"<p>Graduating students and new students (without rooms) require modifications:</p> <p>Mechanism: Top Trading Cycles with Chains (TTCC)</p> <ol> <li>Process students in random order.</li> <li> <p>Mark student \\(i\\) visited:</p> </li> <li> <p>If \\(i\\)'s top room is unoccupied, assign room, release previous.</p> </li> <li>If room occupied by unvisited student \\(j\\), move \\(j\\) ahead of \\(i\\).</li> <li>If room occupied by visited student \\(j\\), cycle identified.</li> </ol> <p>TTCC maintains strategyproofness, Pareto-optimality, individual rationality, and efficiency.</p>"},{"location":"lecture3b/#ttcc-in-practice","title":"TTC(C) in Practice","text":"<p>Why isn't TTCC common?</p> <ul> <li>Running TTCC repeatedly can break strategyproofness over multiple years.</li> <li>Increased strategic behavior over \"popular\" rooms.</li> </ul> <p>Application: School choice (New Orleans, 2011-12) used TTC but switched to DA (Deferred Acceptance) due to simplicity in explanation.</p> <p>College Admissions: Universities prefer specific applicants; direct trades don't apply.</p> <p>Kidney Transplants: Kidney exchange involves patient-donor compatibility issues and logistical constraints.</p> <p>Considerations in Kidney Exchange:</p> <ul> <li>Strategyproofness less critical (compatibility-based).</li> <li>Priority considerations (health urgency).</li> <li>Stability and central mechanism incentivization.</li> <li>Logistical challenge: Long cycles require simultaneous transplants; chains manageable.</li> <li>Dynamic arrivals/departures.</li> <li>Strategic hospitals may internally match.</li> <li>High failure probability (93% matches fail).</li> <li>Ethical issues and multi-organ exchange possibilities.</li> </ul>"},{"location":"lecture4/","title":"Equilibria in Games","text":"<p>(1/23/2023)</p> <p>Previously, agents had dominant strategies. Now, we consider what happens when no dominant strategy exists.</p> <p>Example: Penalty Kicks</p> Kick Left Kick Right Jump Left 0.5 0.8 Jump Right 0.9 0.2 <p>No pure-strategy equilibrium exists because each player has an incentive to deviate.</p> <p>Solution: Mixed strategies\u2014kicker left/right (0.6, 0.4), goalie left/right (0.7, 0.3). Expected scoring probability becomes equal (0.62), providing no incentive to deviate.</p> <p>Definition: Mixed Strategy</p> <p>A probability distribution over pure strategies.</p> <p>Definition: Nash Equilibrium</p> <p>A strategy profile where no player benefits from deviating unilaterally.</p>"},{"location":"lecture4/#equilibrium-in-2-player-zero-sum-games","title":"Equilibrium in 2-Player Zero-Sum Games","text":"<p>Assumptions: - Exactly 2 players. - Sum of payoffs is zero.</p> <p>Player one aims: \\(\\max_{s_1} \\min_{s_2} u(s_1,s_2)\\)</p> <p>Player two aims: \\(\\min_{s_2} \\max_{s_1} -u(s_1,s_2)\\)</p> <p>Theorem: Min-Max Payoffs</p> <p>In two-player zero-sum games, the max-min payoff equals Nash equilibrium payoff equals min-max payoff.</p> <p>Definition: Game Theory Terminology</p> <ul> <li>Game Node: State in the game.</li> <li>Game Tree: Graph showing reachable game nodes.</li> <li>Information Set: Game nodes indistinguishable given player's information.</li> </ul> <p>Example: Poker</p> <p>\"Heads Up\" poker has \\(10^{161}\\) states. Approximate strategies (blueprints) and regret minimization algorithms enable practical computation.</p> <p>Theorem: Regret Minimization Convergence</p> <p>In two-player zero-sum games, regret minimization converges to Nash equilibrium.</p>"},{"location":"lecture4/#nash-equilibrium-in-non-zero-sum-games","title":"Nash Equilibrium in Non-Zero-Sum Games","text":"<p>Theorem: Nash's Existence Theorem (1951)</p> <p>Every finite game has at least one Nash equilibrium (possibly mixed strategies).</p> <p>Issues: - Equilibria not unique. - Hard to compute. - May not make practical sense.</p> <p>Example: Grade Game</p> <p>Two students choose grades \\(x, y \\in \\{2,...,99\\}\\). Equilibrium: \\(x = y = 2\\).</p> <p>Example: Intersection Game</p> Go Wait Go (-99,-99) (1,0) Wait (0,1) (0,0) <ul> <li>Pure equilibria: (Go, Wait), (Wait, Go)</li> <li>Mixed equilibrium: small probability for simultaneous Go.</li> <li>Correlated Equilibrium: Randomize between (Go, Wait) and (Wait, Go).</li> </ul> <p>Definition: Correlated Equilibrium</p> <p>Players follow a correlated distribution of recommended actions. No player benefits from deviating given the recommendation.</p> <p>Correlated equilibria can be efficiently computed.</p> <p>Definition: Stackelberg Equilibrium</p> <p>Strategy profile with leader committing first, follower optimally responds, and leader optimally chooses commitment.</p> <p>Leader commitment can result in higher payoffs than correlated equilibria.</p> <p>Example: Security Games</p> <ul> <li>Airport security</li> <li>Infrastructure defense</li> <li>Cybersecurity</li> <li>Anti-poaching</li> </ul>"},{"location":"lecture5/","title":"P2P File-sharing Dilemma","text":"<p>(1/25/2023)</p> <p>Historical Context: Napster (1999-2001), Gnutella (2000 onwards). Issue: Free-riding.</p> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <p>Dominant Nash equilibrium: both free-ride (socially worst outcome).</p>"},{"location":"lecture5/#repeated-games","title":"Repeated Games","text":"<p>Iterating game \\(n\\) times. What happens?</p> <p>Definition: Subgame Perfect Nash Equilibrium (SPNE)</p> <p>A Nash equilibrium valid in every subgame.</p> <p>Repeated free-riding emerges by backward induction.</p>"},{"location":"lecture5/#repeated-games-with-discounting","title":"Repeated Games with Discounting","text":"<p>Probability \\(p\\) that the game stops each round (discount future payoffs).</p> <p>Grim Trigger Strategy: Free-ride forever if opponent ever free-rides.</p> <ul> <li>Always uploading payoff: \\(\\frac{2}{p}\\)</li> <li>Always free-riding payoff: \\(3\\)</li> </ul> <p>Equilibrium if \\(p &lt; 2/3\\).</p> <p>Tit-for-Tat: Initially upload, then match opponent\u2019s previous action. Robust and stable.</p>"},{"location":"lecture5/#bittorrent-strategies","title":"BitTorrent Strategies","text":"<p>BitTorrent is a popular P2P protocol (~30% upload traffic).</p> <p>Default Strategy: - Tracker coordinates peers. - Tit-for-tat-based uploads, optimistic unchoking for random peers.</p> <p>BitThief: Never upload, frequent peer querying.</p> <p>BitTyrant: Upload strategically to maximize future downloads (ratio-based).</p>"},{"location":"lecture6/","title":"Market Equilibrium","text":"<p>(1/30/2023)</p> <p>Previously, we examined scenarios without monetary transfers. Introducing money changes market dynamics substantially:</p> <ul> <li>Stanford housing would differ if rooms were auctioned.</li> <li>Buying organs is illegal in most countries.</li> <li>Hospitals-student monetary matches face legal restrictions.</li> </ul> <p>Issues with non-monetary markets: 1. Limited to ordinal rather than cardinal preferences. 2. Emergence of underground markets. 3. Potential exploitation (bots manipulating donor lists).</p> <p>Definition: Cardinal vs Ordinal Utilities</p> <ul> <li>Cardinal: Assign numeric values to preferences.</li> <li>Ordinal: Only ranks preferences by order.</li> </ul> <p>Cardinal utilities convey richer information (ordinal can be derived from cardinal), are intuitive in economic analysis, and typically monetarily measurable.</p> <p>Definition: Fungible vs Idiosyncratic Goods</p> <ul> <li>Fungible goods: Interchangeable units.</li> <li>Idiosyncratic goods: Unique goods.</li> </ul> <p>Many real-world goods blend these traits (e.g., ridesharing).</p> <p>Definition: Supply and Demand Curves</p> <ul> <li>Demand curve: Quantity consumers buy at each price.</li> <li>Supply curve: Quantity firms sell at each price.</li> </ul> <p>Typically:</p> <ul> <li>Price is vertical (\\(y\\)-axis).</li> <li>Quantity is horizontal (\\(x\\)-axis).</li> </ul> <p>Usually, demand slopes downward and supply upward. However, exceptions (monopoly markets, certain special goods) exist.</p> <p>Definition: Market Clearing Price</p> <p>The price where supply equals demand. All buyers and sellers transact, \"clearing\" the market.</p>"},{"location":"lecture6/#unit-demand-market-model","title":"Unit-Demand Market Model","text":"<p>Setup:</p> <ul> <li>\\(m\\) idiosyncratic goods.</li> <li>\\(n\\) buyers (each buys at most one good).</li> <li>Buyer \\(i\\) values good \\(j\\) at \\(v_{i,j}\\).</li> <li>Buyer \\(i\\)'s payoff for good \\(j\\): \\(U_{i,j} = v_{i,j} - p_j\\).</li> <li>Utility without buying: \\(U_{i,\\varnothing} = 0\\).</li> </ul> <p>Definition: Competitive Equilibrium</p> <p>A price vector \\(p = (p_1,\\dots,p_m)\\) and matching \\(M:\\{1,\\dots,n\\}\\to\\{1,\\dots,m\\}\\) satisfying:</p> <ol> <li>Buyers get their preferred good at price \\(p\\):    \\(\\(v_{i,M(i)} - p_{M(i)} \\geq v_{i,j} - p_j \\quad \\forall i,j.\\)\\)</li> <li>Unmatched goods have price zero.</li> <li>Buyers are unmatched only if \\(v_{i,j}-p_j&lt;0\\) for all goods \\(j\\).</li> </ol> <p>This implies individual rationality, ensuring buyers never choose negatively valued outcomes.</p> <p>Definition: Social Welfare</p> <p>Sum of buyers' values: \\(\\(U(M)=\\sum_i v_{i,M(i)}.\\)\\)</p> <p>Prices excluded as buyer costs equal seller profits.</p>"},{"location":"lecture6/#properties-of-competitive-equilibrium","title":"Properties of Competitive Equilibrium","text":"<p>Theorem: First Welfare Theorem</p> <p>Competitive equilibrium allocation maximizes social welfare: \\(\\(\\sum_i v_{i,M(i)} \\geq \\sum_i v_{i,M'(i)} \\quad \\forall M'.\\)\\)</p> <p>Proof: By competitive equilibrium definition, for any alternative matching \\(M'\\): \\(\\(\\sum_i(v_{i,M(i)}-p_{M(i)})\\geq \\sum_i(v_{i,M'(i)}-p_{M'(i)}).\\)\\) Since total prices paid are equal, we get welfare optimality.</p> <p>Theorem: Existence of Competitive Equilibrium</p> <p>Competitive equilibrium always exists in finite unit-demand markets with discrete prices.</p> <p>In more complex markets, equilibrium existence isn't guaranteed.</p> <p>Proof (via Deferred Acceptance with Prices):</p> <p>Construct ranked lists of (good, price) pairs for each buyer (discarding negative options). Buyers iteratively propose to their next-best option; goods tentatively accept highest offers. Termination guaranteed (similar to deferred acceptance).</p> <p>Resulting allocation meets competitive equilibrium conditions:</p> <ul> <li>Buyers matched to best possible option.</li> <li>Unmatched goods priced zero.</li> </ul> <p>Proposition: Deferred Acceptance with Prices</p> <p>The deferred acceptance with prices algorithm is strategyproof and buyer-optimal.</p> <p>Proof: Direct from standard deferred acceptance reasoning.</p>"},{"location":"lecture7/","title":"Market Failures","text":"<p>(2/1/2023)</p> <p>Previously, we assumed free markets converge naturally to optimal outcomes. However, markets often fail to achieve optimal outcomes\u2014known as market failures.</p>"},{"location":"lecture7/#externalities-and-public-goods","title":"Externalities and Public Goods","text":"<p>Externalities: Effects of transactions on third parties not directly involved.</p> <p>Public Goods: Goods accessible to everyone and not owned individually.</p> <p>Examples: - Environmental harm from air travel (negative externality). - WiFi bandwidth overuse (negative externality).</p> <p>Solutions: 1. Pigouvian Tax: Tax transactions proportional to the externality they cause. 2. Coasian Bargaining: Privatize public goods and let market bargaining reach social efficiency (Coase theorem).</p>"},{"location":"lecture7/#transaction-costs","title":"Transaction Costs","text":"<p>Costs associated with making transactions that prevent beneficial trades.</p> <p>Examples: - Sales tax blocking a mutually beneficial sale. - Time/information costs preventing trades (hungry student looking for apples).</p> <p>Transaction costs can be monetary or non-monetary (time, effort, information).</p>"},{"location":"lecture7/#market-thickness","title":"Market Thickness","text":"<ul> <li>Thick Market: Many buyers/sellers, competitive prices, efficient outcomes.</li> <li>Thin Market: Few buyers/sellers, monopoly pricing, inefficiencies.</li> </ul> <p>Why monopolies are problematic: - Reduced quantity sold. - Higher prices. - Lower consumer surplus.</p> <p>The first welfare theorem applies with cost-covering reserve prices, but these reserve prices aren't strategyproof:</p> <p>Proof:</p> <p>If reserve prices are nonbinding, sellers can slightly increase them and remain profitable. If binding (equal to cost), sellers earn zero profit, thus increasing prices slightly above cost yields positive profit. This creates a profitable deviation.</p> <p>Ways around this: - In large markets, deviations from strategyproofness diminish (strategyproof-in-the-large). - Limited seller information on buyer valuations restricts strategic price setting.</p> <p>Thickening Markets: - Encourage participation and retention. - Merge markets (e.g., larger kidney exchanges). - Batch transactions to aggregate buyers and sellers.</p>"},{"location":"lecture7/#timing-issues","title":"Timing Issues","text":"<p>Market Unraveling: - Matches occur prematurely (medical students matched early, creating mismatches over time).</p> <p>Exploding Offers: - Offers requiring rapid responses due to competitive pressures or regulatory constraints.</p> <p>Examples: - APPIC psychologist hiring (1970\u20131990s) faced early and premature matching despite rules.</p> <p>Resolving Timing Issues: - Centralized matching mechanisms. - Allow reneging on exploding offers to remove incentives to rush decisions.</p>"},{"location":"lecture7/#information-asymmetries","title":"Information Asymmetries","text":"<p>Sellers know more about goods than buyers, causing uncertainty and inefficiency.</p> <p>Example: Market for Lemons (used cars) - Good and bad cars indistinguishable by buyers. - Buyers pay less due to uncertainty. - Good cars leave market; only bad cars remain (adverse selection).</p> <p>Possible equilibria: - Bad equilibrium: only lemons sold at low prices. - Good equilibrium: enough good cars maintain higher average quality and price.</p> <p>Other Examples: - Health insurance markets (buyer health knowledge). - Clickbait (creators know content quality; users do not).</p> <p>Adverse Selection: Poor-quality products dominate markets due to asymmetry.</p> <p>Solutions: - Increase transparency (mandatory disclosure, reputation systems). - Restrict informational use (universal healthcare, insider trading rules). - Filter lemons through warranties or guarantees.</p>"},{"location":"lecture8/","title":"Single-Unit Auctions","text":"<p>(2/6/2023)</p> <p>Auctions are valuable in settings where price discovery is needed, such as:</p> <ul> <li>Monopolies: Wireless spectrum auctions.</li> <li>Niche products: Rare items on eBay.</li> <li>Specialized products: Ad auctions.</li> </ul> <p>Auctions intersect significantly with Computer Science:</p> <ul> <li>Ad auctions fund many CS researchers.</li> <li>Fast auctions necessitate algorithmic bidders.</li> <li>Complex auctions require algorithmic auctioneers.</li> </ul>"},{"location":"lecture8/#case-of-one-buyer","title":"Case of One Buyer","text":"<p>Motivation: Digital goods pricing for maximizing revenue.</p> <ul> <li>Demand curves derived from users' willingness to pay.</li> <li>Revenue = Price \u00d7 Number of buyers willing to pay that price.</li> <li>Roger Myerson (1981): For a demand curve \\(D\\), a formula \\(p(D)\\) exists that maximizes revenue (optimal reserve price).</li> </ul> <p>Application: Ad auctions with specialized advertisers (single bidder per spot) using prior beliefs as demand curves.</p>"},{"location":"lecture8/#case-of-multiple-buyers","title":"Case of Multiple Buyers","text":"<p>Model: 1. Set of bidders \\(I\\), each with valuation \\(v_i\\). 2. Seller doesn't know valuations but has prior beliefs. 3. Bidder payoff: \\(v_i - p_i\\) if they win, else \\(-p_i\\).</p>"},{"location":"lecture8/#first-price-auction","title":"First-Price Auction","text":"<ul> <li>Each bidder submits a sealed bid \\(b_i\\).</li> <li>Highest bidder wins and pays their bid.</li> </ul> <p>Note: Equilibrium bids are below true valuations (\\(b_i &lt; v_i\\)).</p>"},{"location":"lecture8/#all-pay-auction","title":"All-Pay Auction","text":"<ul> <li>Each bidder submits a sealed bid \\(b_i\\).</li> <li>Highest bidder wins the item.</li> <li>All bidders pay their bids.</li> </ul> <p>Used for modeling non-auction scenarios (political donations, animal contests). Equilibrium bids are lower than valuations and generally lower than in first-price auctions.</p>"},{"location":"lecture8/#second-price-auction","title":"Second-Price Auction","text":"<ul> <li>Each bidder submits a sealed bid \\(b_i\\).</li> <li>Highest bidder wins, paying the second-highest bid.</li> </ul> <p>Theorem: Equilibrium in Second-Price Auctions</p> <p>Second-price auctions are strategyproof: truthful bidding (\\(b_i = v_i\\)) is dominant and thus an equilibrium.</p> <p>Proof: Fix other bids \\(b_j\\), let \\(b^{(-i)} = \\max_{j \\ne i} b_j\\). Two cases:</p> <ol> <li>If \\(v_i &lt; b^{(-i)}\\), bidder \\(i\\) prefers losing; bidding truthfully is optimal.</li> <li>If \\(v_i &gt; b^{(-i)}\\), bidder \\(i\\) prefers winning; again, truthful bid is optimal.</li> </ol> <p>Theorem: Payoffs of Second-Price Auctions</p> <p>Second-price auctions are individually rational at equilibrium: \\(v_i - p_i \\geq 0\\).</p> <p>Proof: If bidder doesn't win, payoff = 0. If bidder wins, pays at most their valuation, thus non-negative payoff.</p> <p>Theorem: Optimality</p> <p>In equilibrium, second-price auctions allocate the good to the highest-valued bidder.</p> <p>Proof: In equilibrium \\(b_i = v_i\\), thus highest bidder corresponds to highest valuation.</p>"},{"location":"lecture8/#revenue-maximization","title":"Revenue Maximization","text":"<p>Example: Full Information Scenario</p> <ul> <li>\\(A\\) values item at 1, \\(B\\) values at 2.</li> <li>Second-price auction: \\(B\\) wins, pays 1.</li> <li>First-price auction: Equilibrium bids near 1, revenue approximately 1.</li> </ul> <p>To handle uncertainty, we define Bayesian Nash Equilibrium:</p> <p>Definition: Bayesian Nash Equilibrium</p> <p>A strategy profile where each player's strategy maximizes expected payoff given beliefs about others' strategies.</p> <p>Example: Bayesian Agents</p> <ul> <li>\\(A,B\\) values drawn uniformly from [0,1].</li> <li>Second-price auction expected revenue: \\(\\frac{1}{3}\\).</li> <li>First-price auction equilibrium bids: \\(b_i = \\frac{v_i}{2}\\), also yielding expected revenue \\(\\frac{1}{3}\\).</li> </ul> <p>Theorem: Revenue Equivalence</p> <p>At equilibrium, expected payments depend only on the auction\u2019s allocation rule.</p> <p>Corollary: First-price, second-price, and all-pay auctions yield identical equilibrium revenue under Bayes-Nash equilibria with standard allocation rules (highest bidder wins).</p>"},{"location":"lecture8/#deviations-from-optimal-allocation","title":"Deviations from Optimal Allocation","text":"<p>Auctions don't always allocate to the highest valuation bidder:</p> <ul> <li>Overbidding in second-price auctions (highest bidder might bid excessively).</li> <li>All-pay auctions may have equilibria not awarding highest value bidder.</li> <li>Reserve prices might result in no winner.</li> </ul>"}]}