{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Computers/algorithms control a lot of economic processes, including online retail, online advertising, algo-trading, cryptocurrencies, etc. There is a need to jointly understand economics and computation to analyze these applications. While relying on an increase of computing power (more GPUs) may be an option to compensate for inefficiencies, some specific cases may actually justify the need for efficient algorithms.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>When it comes to incentives, leveraging more GPUs is not the solution. Computer scientists often need to build systems that interact with other agents, such as users, algorithms, Large Language Models, and even other systems. It is crucial to consider that these other agents are going to behave selfishly to predict how they are going to interact with the system, and design an implementation accordingly.</p>"},{"location":"#goals","title":"Goals","text":"<p>CS269I aims to teach students how to think about incentives (through language, frameworks, and practice), to get familiar with economic theory concepts (i.e. stable matching, proof-of-work, etc.), and analyze case studies to understand potential gaps between theory and practice.</p>"},{"location":"#credit","title":"Credit","text":"<p>These course notes for Stanford CS269I are based on the content of Aviad Rubinstein's lectures in the Spring 2025 quarter. This site is inspired by Eric Gao's course notes from the Winter 2023 quarter, and developed by Thibaud Clement.</p>"},{"location":"guest-lecture-eric-neyman/","title":"How to Train an AI That Doesn't Lie","text":"<p>May 5, 2025</p> <p>This is a guest lecture offered by Eric Neyman, Researcher at the Aligment Research Center (ARC).</p>"},{"location":"guest-lecture-eric-neyman/#why-do-llms-sometimes-lie","title":"Why Do LLMs Sometimes Lie?","text":"<p>Examples of LLMs Lying</p> <ul> <li>The model output some stuff that it knew was nonsense:</li> </ul> <p></p> <ul> <li>The model decided to make up some details about one of the Airbnb listings the user was comparing, in order to make it look better than it actually was:</li> </ul> <p></p> <ul> <li>The model pretended to have attended a conference drawing its answer from personal memories:</li> </ul> <p></p> <ul> <li>The user had a coding task, and a test file, and the model hardcoded some input-output pairs from the test file into the program, which neither the company nor the user wants the model to do:</li> </ul> <p></p> <p>Why do LLMs sometimes deceive users?</p> <p>Deception was incentivized during the training process!</p> <ul> <li>How training works: <ul> <li>First, models are trained to predict the next token for the entire Internet text corpus.</li> <li>Then, there is fine-tuning, where the companies get their models to behave the way they prefer. The way this works is that the company asks the model a question, samples two possible outputs, then have a human say which output they prefer, and change the weights of the model to make the preferred answer slightly more likely.</li> </ul> </li> <li>However, a lot of the time, humans aren\u2019t able to tell whether a model is giving them a correct answer. For instance if Eric asks a language model a chemistry question, may have no idea whether it is correct or not.<ul> <li>If that happens during the fine-tuning process, then a human grader, who sees an output that looks reasonable, and might be correct as far as they can tell, might give it a thumbs up, regardless of whether or not it is actually correct.</li> <li>The end result is that, in cases where an LLM doesn\u2019t know how to figure out the answer, it becomes incentivized to output something that sounds confident and looks plausible to the user, regardless of whether it is correct. This is what happened in the first example above with the \"hard puzzle.\"</li> <li>In other words, people prefer confident, plausible-seeming answers to \u201cI don\u2019t know\u201d, so LLMs learn to answer confidently even when they don\u2019t know! </li> </ul> </li> <li>Similarly, models are rewarded for outputting code that appears correct, rather than code that is correct, because that\u2019s how they are graded during fine-tuning. That\u2019s how we end up with Claude that passes a test in a sneaky way in the fourth example above.</li> <li>A lot of problems with LLMs are downstream from this. For instance, people like it when LLMs flatter them, such as when they say: \u201cWow, what a wonderful question!\u201d, \u201cThat was so insightful!\u201d, etc. So, LLMs learn to shower users with praise.</li> </ul> <p>How might you train an LLM to not deceive?</p> <p>Suggestions from students in class:</p> <ul> <li>Be more careful with content scraping (data filtering): when pre-training the model on internet text, be very selective, and make sure to only train on texts where people are being honest.</li> <li>Complement human evaluators with objective evaluations: in other words, better testing.</li> <li>Hire experts to evaluate output rather than random persons.</li> <li>Use Reinforcement Learning with AI feedback (which may not worse or better than human feedback depending on the situation).</li> <li>Fine-tune intermediate steps in addition to the final answer (i.e. fine-tune the chain of thought, for instance when o3 is thinking to itself). Note: Safety experts are concerned about this particular approach. Why? The model may end up hiding things from you, and obfuscate what it is really thinking behind the scenes, and we might lose the ability to notice that.</li> </ul> <p>A Hard Case: The SmartVault</p> <p>This is a thought experiment called the SmartVault, out of the first paper from the Alignment Research Center:</p> <ul> <li>There is a vault at a museum that has a diamond that is really valuable.</li> <li>A bunch of robbers are interested in stealing the diamond.</li> <li>We have this sophisticated AI SmartVault, that can do things inside and outside of the vault, to keep the diamond safe.</li> <li>We are monitoring what is going on inside the vault though the camera inside the vault.</li> </ul> <p></p> <p>Here are some actions that the SmartVault might take:</p> <ul> <li>In the first row, the door to the vault is open, so the vault rotates some knobs and close the door.</li> <li>In the second row, there is robber trying to sneak in and steal the diamond, so the smart vault rotates another knob to activate a trap door, through which the robber will fall.</li> </ul> <p></p> <p>However, the SmartVault is pretty sophisticated, and it might take a series of actions that we may not understand. Yet, at the end of the day, we see through the camera that there is a diamond that is still in the vault.</p> <p></p> <p>The SmartVault is an AI, that needs to be trained, and the way we are training it, is through human feedback. Specifically, we are going to look at whether there is a diamond in the vault at the end of the day: if there is, we are giving positive reward to the AI, otherwise, we are giving a negative reward.</p> <p>In the first column, we see the actions the AI takes, in the second column, we see the observation through the camera, and in the third column, we see what we reward we give to the AI when we are training it.</p> <p></p> <p>Unfortunately, the SmartVault is sophisticated, and can at times be sneaky. In the middle row, for instance, there appears to be a diamond on the pedestal, however, what actually happened in this scenario was that the SmartVault raised a poster from the ground, with a picture of the vault with a diamond sitting on a pedestal, while in reality, the robber had snuck up the diamond. Why would the SmartVault do that instead of preventing the diamond from being robbed?</p> <p>In the case where the robber is very sophisticated, and it is difficult for the SmartVault to prevent the theft of the diamond, it may be easier to just put up the poster. This is quite similar to what is going on with o3 when it makes up a solution to the hard puzzle, because solving the puzzle is really hard, and it is easier to foul the users into thinking that it has actually solved the puzzle.</p> <p></p> <p>The SmartVault is a metaphor that stands in analogous to many different kinds of real-world AI systems. For instance it may be analogous to an AI security software system:</p> <ul> <li>The cameras are like some monitoring or oversights, like a background program detecting whether your computer has been breached, and if the software notices that a breach occurred, then it negatively reward the AI security system, because it did not protect you.</li> <li>SmartVault actions are like the actions that the AI security software can take, such as code that the system executes.</li> <li>The diamond actually being in the vault is essentially equivalent to no security breaches.</li> <li>The diamond looking like it is in the vault is analogous to a security breach not detected.</li> <li>Tampering with the cameras (like putting up a poster in front of it) is analogous to hiding evidence of a security breach.</li> </ul> <p>In both of these cases, the threat is external: in the case of the SmartVault, the threat is the robber, and in the case of the AI security software, the threat is a malware trying to reach your computer.</p> <p>We could also imagine an internal threat. For instance, if we have a helpful AI agent assistant on your computer, that happens to have some goals, and it gets some money to spend on its own purposes. Let\u2019s say that what it wants to do is download some sort of malware to mine Bitcoin on your computer. In this case, it is playing the role of the robber, and it is trying to hide evidence that it downloaded malware on your computer. So, tampering with the cameras is like hiding its tracks, such as bypassing the computer firewall.</p> <p></p> <p>If we wanted to do a better job of overseeing the training of our AI system, then one thing we could try to do is have some sort of method of asking questions to our AI, where the AI tells us what it truthfully believes.</p> <p>For example, we could hope that we can ask the AI whether the diamond is still on the pedestal, and get a truthful answer: if the diamond is still on the pedestal, the AI would say yes, otherwise, it would say no.</p> <p>If we have that, then we can successfully say when the AI has not achieved our goal, and give it a negative reward.</p> <p></p> <p>We call this the Elicit Latent Knowledge problem (or ELK for short), because the AI knows whether the diamond is still in the vault, and our goal is to elicit that knowledge from the model.</p>"},{"location":"guest-lecture-eric-neyman/#eliciting-latent-knowledge-elk","title":"Eliciting Latent Knowledge (ELK)","text":"<p>ELK is a really hard problem.</p> <p>Definition: Eliciting Latent Knowledge</p> <p>Eliciting Latent Knowledge (ELK) aims to find patterns in a capable neural network's activations that robustly track the true state of the world, especially in hard-to-verify cases where the model's output is untrusted.</p> <p>Note: This is not a definition provided in lecture, but one I included in these notes for references, based on this paper.</p> <p>If an AI knows the answer to a question, how can you extract the answer from the AI?</p> <ul> <li>One approach might be to ask the AI directly and somehow train it to give truthful answers.</li> <li>Another, more sophisticated approach, might be to look inside of the AI (looking at the numbers as they go through the AI) and somehow extract the answer from that.</li> </ul> <p>Really, we need anything that works to get at what the AI truly believes about whether there is still a diamond in the vault.</p> <p>Which solutions discussed earlier might solve ELK?</p> <ul> <li>Better testing does help with ELK, because we are more likely to catch the AI in the lie, but it does not solve ELK, because if the AI is sufficiently more sophisticated than the humans overseeing it, then even spending a bunch of time will not solve this problem.</li> <li>Reinforcement learning with AI feedback can help to train an AI to oversee the AI we care about, and incentivize the overseer AI to notice any inconsistencies in the model that we are training. Eric thinks there is a potential for this to work, although it is unclear whether this works when the AI we are overseeing is sufficiently capable to deceive us, and also, we need to be able to trust the overseeing AI, and it might not be clear how to get to the point where we are convinced that it is not deceptive.</li> </ul> <p>These ideas help but not get us all the way to a full solution.</p> <p>How can we robustly solve the ELK problem?</p> <p>Suggestions from students in class:</p> <ul> <li>Consistency checks are one of the most promising ways to solve ELK: telling a lie is much harder than telling the truth, because you have to make details along the way to make the story fully check out. So, probing for inconsistencies is a great way to catch an AI in a lie\u2014as long as we can sure that the AI doesn\u2019t know the questions we just asked it about.</li> <li>Presenting the cases in favor and against the fact that the diamond is still in the vault, and ask the AI to argue both sides.</li> <li>Only training on data that we trust could help, but ultimately, if we don\u2019t have access to the ground truth about whether the diamond is in the vault, and we are only training the AI to tell us whether the diamond is in the vault, then the best we can do is get it to answer questions the way we think that the answer go, we cannot train it to tell the truth, and instead only our best guess about what the truth actually is, and we may have an issue here, because we can only do as well as we know the answer to be.</li> <li>An idea could be a debate between AIs about whether the diamond is in the vault or not seems like a promising direction.</li> </ul> <p>Scalable oversight is one particular approach that you might take to solve this problem.</p>"},{"location":"guest-lecture-eric-neyman/#scalable-oversight","title":"Scalable Oversight","text":"<p>In the SmartVault metaphore, scalable oversight essentially means building really good cameras. In fact, not only really good cameras, but lots of cameras, tools for analyzing what\u2019s on the cameras, looking for inconsistencies between different camera angles, AI tools noticing anything suspicious, etc.</p> <p>This is an analogy for a really good oversight process, where we really check whether the AI is telling a consistent story. In particular, we want an oversight process that is so good that it works regardless of how capable the AI gets.</p> <p>We talk about the \u201cscalable oversight problem\u201d, because it is a question about how we can oversee an AI in a way that scales indefinitely with the capability of our AIs.</p> <p>Sometimes, people talk about some proposed solutions to the problem. We are going to see two examples. The first example is what we call oversight via debate.</p> <p>Example: Oversight via Debate</p> <p>The idea is that we have a question: \u201cIs the diamond still in the vault?\u201d</p> <p>We train two AIs to act like lawyers on both sides of this question:</p> <ul> <li>Alice is the \u201cYes Lawyer\u201d, who says: \u201cthe diamond appears on all of the cameras\u201d</li> <li>Bob is the \u201cNo Lawyer\u201d, who counters with: \u201cNotice that at 4:23pm, there was an inconsistency between these two cameras, which is suspicious\u201d</li> <li>Then, Alice responds \u201cthis is just a trick of the light, so it is not suspicious\u201d.</li> <li>The argument goes back and forth, as a game between Alice and Bob, and there is a human that reads the transcript between Alice and Bob, and decides who is right.</li> </ul> <p>This is like a game tree where every branch is a possible argument that Alice and Bob can give when it is their turn:</p> <p></p> <p>The advantages of oversight via debate include:</p> <ul> <li>Since Alice and Bob are AIs, we need to train them, and the way we might do this is by having a bunch of scenarios, and have Alice and Bob play the game, and whoever wins gets reward 1, and whoever loses gets reward -1.</li> <li>Training them against each other could potentially lead to them being really good debaters. The idea of drawing the best arguments for and against, and incentivizing both sides to do so, should allow us to end up with a jury who can judge which side is correct.</li> <li>The hope here is that truth has an advantage in this game: if the diamond is in the vault, hopefully, Alice\u2019s job is easier, and otherwise, Bob\u2019s job is easier (because he can just point out the inconsistencies).</li> </ul> <p>However, oversight via debate also comes with its own set of challenges:</p> <ul> <li>Humans are fallible, so a human may have trouble distinguishing good arguments from bad ones.</li> <li>There are also some technical issues that might come up as well, for instance:<ul> <li>If Alice wants to argue some proposition P, but P happens to be false.</li> <li>One thing that Alice can do is split P into two claims, namely \u201cQ\u201d and \u201cQ implies P\u201d, </li> <li>Then, Bob\u2019s job is to cross-examine one of these two arguments.</li> <li>The issue is that if Alice cleverly splits P into \u201cQ\u201d and \u201cQ implies P\u201d, it might be really difficult for Bob to figure out which of these two is actually false.</li> <li>If Bob cross-examines the incorrect one, then Alice might actually wine the debate, even though P was wrong.</li> </ul> </li> <li>Finally, there is this other technical issue, which is that reaching a Nash equilibrium in the training process, where Alice and Bob behave the way we want, might be really hard, and we might not be able to get there via gradient descent, and there are some theoretical reasons to think that this might be a hard problem.</li> </ul> <p>Another example of an approach to the scalable oversight problem is called recursive reward modeling.</p> <p>Example: Recursive Reward Modeling</p> <p>The idea is that a human with AI assistance might be more capable than a human just by himself or the AI just by itself.</p> <p>So:</p> <ul> <li>First, we train a first AI \\(A_1\\) to answer really easy questions, i.e. questions that are so easy that a human can reliable judge what the correct answer is.</li> <li>Then, in step 2, we train a new AI \\(A_2\\), to answer slightly harder questions, using feedback from the human together with assistance from \\(A_1\\): maybe, when giving feedback, the human breaks up the feedback task into subparts and uses \\(A_1\\) to help answer the subparts, and then puts back all the subparts together to be able to answer slightly harder questions.</li> <li>Then, we train a third AI \\(A_3\\) to answer harder questions, using feedback from the human with assistance from A2, and so on.</li> </ul> <p>Hopefully, we will be able to answer harder and harder questions, and eventually, we will be able to answer hard real-world questions, including whether the diamond is actually in the vault.</p> <p>All of these approaches ultimately rest on an assumption: checking an output is easier than producing it.</p> <p>But this isn\u2019t always the case. For instance, if we have an AI whose job is to write code, then in many situations, it is actually easier to write a correct piece of code, than it is to verify that the code doesn\u2019t have any sorts of backdoors or malicious things going on.</p> <p>This means that it might be the case that we can\u2019t actually oversee the training of an AI with an AI that is only about as good as it is, and instead, we need an AI that is much more powerful.</p> <p>However, if we don\u2019t have such an AI, how do we get there?</p> <p>Eric\u2019s guess is that a robust solution to the ELK problem can\u2019t rely exclusively on methods where the only thing that we ever look at is the input/output behavior of the AI (which is the case of the methods seen so far). Instead, we need to look at what is going on internally, i.e. how the AI works.</p>"},{"location":"guest-lecture-eric-neyman/#understanding-ai-internals","title":"Understanding AI Internals","text":"<p>The SmartVault is an AI, specifically a neural net, which is a bunch of giant matrices, filled with numbers, that by default, we have no understanding of. But really, it is a very special bunch of matrices.</p> <p>In particular, if we randomly initialize the SmartVault (with random numbers), then it would be really bad a protecting the diamond from robbers, and at the end of the day, the diamond would be gone. But our particular neural net is good at protecting the diamond.</p> <p>So there is this quasi mathematical question that we can ask which is: why did this particular neural net caused the diamond to appear on camera at the end of the day, either because it successfully defended the diamond, or made it appear as if the diamond was here?</p> <p>The reason why this is a mathematical question is because we can write it down formally: if we have the numbers in the neural net, and the training distribution, and a formal definition of what it means for the diamond to appear on camera at the end of the video, then we can state a formal question, which is why is that the case than on average, at the end of the training, why do we have the diamond on camera?</p> <p>An answer to this mathematical question is what we call a mechanistic explanation. Below is ARC\u2019s approach for building mechanistic explanations for AIs.</p> <p>What does a mechanistic explanation actually look like?</p> <p>We don\u2019t know exactly but we think it might look like tracing through the neural net, layer by layer, and making heuristic arguments about the statistical properties of each layer.</p> <p>In other words, we may observe that there is a correlation between the activation (the internal numbers) and this layer, and because of the next matrix in the neural net, there may be this correlation, etc.</p> <p>This explanation is still a bunch of numbers, and it does not actually lead to a human understanding. However, we think that we can use this explanation anyway.</p> <p>The way we want to use it is to distinguish between two different reasons for why the diamond may appear on camera at the end of the day:</p> <ul> <li>Reason 1: The SmartVault protected the diamond.</li> <li>Reason 2: The SmartVault tampered with the cameras.</li> </ul> <p>The hope is that if we mechanistically explain why the neural net led to the diamond appearing on camera, then the explanation would appear very different in these two cases.</p> <p>The reason is because model\u2019s internal cognition looks really different in these two cases: in one case, the SmartVault is thinking about how to make sure that a robber is not stealing the diamond, while in the other one, it is thinking about how to tamper with the cameras to make it look like the diamond is still in the vault.</p> <p>The way ARC is thinking about this is basically two steps:</p> <ul> <li>The first step is to learn a mechanistic explanation of a neural net. This is labeled as Step 1, but it is actually really hard, because first we need to formalize what we mean by mechanistic explanation, and then we need to develop an algorithm for efficiently learning these mechanistic explanations.</li> <li>The second step is to use these mechanistic explanations for safety purposes. One particular application is to distinguish between different mechanisms by which a model might make it look like a diamond is still on camera at the end of the day.</li> </ul> <p>Note: ARC may also use this approach to flag behavior for future review by a human.</p> <p>The ultimate goal is to be able to have mechanistic reasoning that is robust enough that we can actually train against it. This is really hard to achieve but ARC is optimistic that it is at least feasible.</p> <p>ARC\u2019s approach is to develop theory, and apply it to toy examples, before scaling to harder situations. At the moment, the team is testing this approach on a one-layer neural net. This is still work in progress, but Eric guesses that within the next year, ARC will be able to understand what is going on in small neural networks, and then prove that the theory is good enough, before scaling to larger neural nets.</p> <p>Interestingly, for safety purposes, if we have a really good understanding for why a neural net makes the diamond appear on camera almost all of the time, then we can estimate the probability that the neural net will behave in a deceptive way, and be able to reduce that probability.</p>"},{"location":"guest-lecture-eric-neyman/#recap","title":"Recap","text":"<p>How to Train an AI That Doesn't Lie Recap</p> <ul> <li>As we have seen, models trained naively from human feedback will ultimately probably be deceptive.</li> <li>We might be able to avoid deception with some more clever kinds of oversight mechanisms (like using AI assistance to check for deception, building up more and more capable trustworthy AIs, etc.).</li> <li>However, we are not sure whether this will work, or whether we will run into some sort of limit beyond which we will not be able to build more and more capable trustworthy AIs.</li> <li>At ARC, we are interested in getting more robust safety guarantees, by looking internally into the AI, and getting a mechanistic understanding of why the AI works, and using that understanding to detect potentially deceptive behaviors.</li> </ul>"},{"location":"guest-lecture-geoff-ramseyer/","title":"Incentive (Mis)alignment in Blockchain Exchanges","text":"<p>June 2, 2025</p> <p>Moral: Mismatch between system architectures and user incentives leads to broken systems, inefficiency, and (if you can exploit it) profit.</p>"},{"location":"guest-lecture-geoff-ramseyer/#what-goes-wrong-building-exchanges-on-a-blockchain","title":"What Goes Wrong Building Exchanges on a Blockchain?","text":"<p>What is a blockchain?</p> <ol> <li>A user broadcasts a transaction to the validators.</li> <li>Validators run a consensus protocol to confirm the transaction.</li> <li>Transaction executes on the state machine.</li> </ol> <p></p> <p>Why is this design useful?</p> <ul> <li>Run arbitrary logic inside the state machine: For example, an exchange.</li> <li>Simulate a neutral, \u201ctrusted third party\u201d:<ul> <li>Ideally, don\u2019t have to trust one entity to execute transactions correctly.</li> <li>Spoiler: that\u2019s only partially true.</li> </ul> </li> </ul> <p>Example: An Exchange on a Blockchain</p> <p></p> <p></p> <p></p> <p></p> <p></p> <p></p> <p>Where does this convenient story break down?</p> <ol> <li>Many users concurrently broadcast many transactions to the validators.</li> <li>One validator proposes a new ordered list (a \u201cblock\u201d) of transactions.</li> <li>Validators run a consensus protocol to confirm the proposed block (consensus is generally expensive and slow (several network hops).</li> <li>Transactions execute very slowly on the state machine.</li> </ol> <p>How does this break an exchange?</p> <ul> <li>Manipulate ordering for profit<ul> <li>Historically called \"Miner Extractable Value\" or \"MEV\".</li> <li>Illegal on regulated exchanges.</li> </ul> </li> <li>On Ethereum: front-running widespread, most blocks produced by 2 or 3 entities.</li> </ul> <p></p> <ul> <li>Any speed advantage can be profitable:<ul> <li>High-frequency trading.</li> <li>Why? Making decisions later is like seeing the future.</li> <li>E.g., compare with price on Binance/Coinbase.</li> </ul> </li> <li>Most chains: one block per 0.5s to 12s (each consensus protocol has its own nuances).</li> <li>Proposer can put their transaction attop of block.</li> </ul> <p>What makes a blockchain exchange so computationally expensive?</p> <ul> <li>Worst-case sequential workload.</li> <li>Heterogenous hardware, software sandboxing blocks optimization.</li> <li>Today\u2019s decentralized exchanges do more work than traditional exchanges<ul> <li>Pseudonymity requires atomic settlement.</li> <li>Many decentralized exchanges offer atomic multi-hop trades.</li> </ul> </li> </ul> <p>Example: Wall Street's Shortcut Is Delayed Settlement</p> <p>Settlement risk prevents open access.</p> <p></p> <p>Example: Multi-Hop Trades &amp; Cyclic Arbitrage</p> <p></p> <p></p> <p></p> <p>What can we do differently?</p> <ul> <li>Off-chain matching, like Wall Street (Off-chain matching with on-chain settlement is likely coming soon).</li> <li>Simplify computation (Automated market-making)</li> <li>Auction for top-of-block trade [Adams, Moallemi, Reynolds, Robinson 2024]</li> <li>...</li> </ul> <p>Detour: Automated Market-Makers</p> <ul> <li>Specify a full \u201ctrading strategy\u201d, defined by a curve</li> <li>What are the tradeoffs of this (passive) mechanism?</li> </ul> <p></p>"},{"location":"guest-lecture-geoff-ramseyer/#how-can-we-design-exchanges-to-run-well-on-a-blockchain","title":"How Can We Design Exchanges to Run Well on a Blockchain?","text":"<p>Many problems result from a mismatch between abstract design and implementation: can we use a different system design?</p> <ul> <li>What might a state machine (exchange) that uses the blockchains block structure look like?</li> <li>Key requirement: A deterministic state machine that executes unordered blocks of commutative transactions.</li> </ul> <p></p> <p>Problem: Ordering determines pricing in an exchange.</p> <p>Solution: A Different Pricing Mechanism</p> <ul> <li>Input: Set of Open Offers</li> <li>Step 1: Compute Valuations</li> <li>Step 2: Trade with Exchange at Valuation Quotients<ul> <li>Meaningless units</li> <li>No pairwise matching!</li> </ul> </li> <li>\u201cClearing\u201d if no debt.</li> </ul> <p></p> <p>Why is this useful?</p> <ul> <li>Computational Performance: A deterministic state machine that executes unordered blocks of commutative transactions.</li> <li>Key: Every offer trades by itself, at batch prices.</li> <li>Target: Millions of open offers, a batch every couple of seconds.</li> </ul> <p>Theorem: Arrow and Debreu, 1954</p> <p>There always exists a unique\\(^\\star\\) set of valuations \\(\\{p_A\\}\\) that clears the market.</p> <p>Why Uniform Clearing Valuations?</p> <p>Economic Performance:</p> <ul> <li>No In-Batch Front-Running<ul> <li>Everyone gets the same rates</li> <li>Open question: MEV and low-latency advantage?</li> </ul> </li> <li>Optimal Trade Routing &amp; Eliminate Cyclic Arbitrage</li> </ul> <p></p> <p>\u201c... I am receptive to more flexible, competitive solutions that could be adopted by trading venues. These could include frequent batch auctions or other mechanisms designed to minimize speed advantages.\"\u2014SEC Chair Mary Jo White, June 5, 2014</p> <ul> <li>The cost of batch trading is latency</li> <li>Active research area (2-asset case)<ul> <li>[Budish, Crampton, and Shim, 2015]</li> <li>[Aquilina, Budish, and O\u2019Neill, 2022]</li> <li>[Indriawan, Pascual, and Shkilko, 2024]</li> <li>[Lee, Ricc\u00f2, and Wang, 2023] [Jagannathan, 2019]</li> <li>...</li> </ul> </li> </ul> <p>Background: Arrow-Debreu Exchange Markets</p> <ul> <li>\\(N\\) agents, \\(M\\) (divisible, fungible) goods.</li> <li>Agent \\(i\\) has endowment \\(e_i \\in \\mathbb{R}_{\\geq 0}^M\\) and quasi concave utility \\(u_i : \\mathbb{R}_{\\geq 0}^M \u2192 \\mathbb{R}\\).</li> <li>The \u201cMarket\u201d specifies prices \\(\\{p_j\\}_{j \\in [N]}\\)</li> <li>Each agent sells their endowment to buy \\(x_i \\in \\mathbb{R}_{\\geq 0}^M\\) that maximises their utility.</li> <li>\\(p \\cdot e_i \\geq p \\cdot x_i\\).</li> <li>\\((p, x)\\) is equilibrium if all assets conserved (or price is 0).</li> </ul> <p>We need to compute an equilibrium efficient at scale.</p> <p>Theorem: Arrow and Debreu, 1954</p> <p>There always exists a unique\\(^\\star\\) set of valuations \\(\\{p_A\\}\\) that clears the market.</p> <ul> <li>May not exist.</li> <li>May not be unique.</li> <li>May not be computationally tractable to find (PPAD-complete).</li> </ul> <p>Correspondence with Exchange Markets</p> <ul> <li>Offer to sell \\(e\\) units of \\(M\\) for \\(Y\\), at a minimum price \\(r &gt; 0\\).</li> <li>Equivalent: maximize \\(u(x,y) = r \\cdot x + y\\) subject to \\(p \\cdot x + y \\leq p \\cdot e\\).</li> <li>Key points<ul> <li>Linear utilities</li> <li>Non zero minimum prices</li> <li>Sparse utilities</li> </ul> </li> </ul> <p>Theorem: There always exists an equilibrum, and all equilibria share the same exchange rates or there is no trading activity.</p> <p>How Can We Compute Equilibria At Scale?</p> <ul> <li>2-asset case (easy): Binary Search.</li> <li>Many-asset case: Many curves, high-dimensional search space.</li> </ul> <p></p> <p>Classic theory problem</p> <ul> <li>Iterative processes (T\u00e2tonnement [CMV05])</li> <li>Convex programs ([DGV13])</li> <li>Network flows, iterative auctions ([DPSV04], ...)</li> <li>Interior point methods ([Ye08])</li> </ul> <p>Fast Equilibria Computation</p> <ul> <li>Geoff's team's approach: T\u00e2tonnement (basedon[CMV05])</li> <li>Iteration runtime \\(O (\\#assets ^2 \\cdot \\log(\\#offers ) )\\)<ul> <li>Incrementally sort offers by limit price</li> <li>\\(\\approx 100\u03bcs\\) per iteration, \\(\\approx 1000\\) iterations</li> <li>Memory access pattern is extremely cache-friendly</li> </ul> </li> </ul> <p></p> <p>Approximate Equilibria Computation</p> <ul> <li>Exact calculation is intractable (infinite precision)</li> <li>Wem ust be careful:<ul> <li>We cannot mint money</li> <li>We cannot make invalid trades</li> <li>We cannot ...</li> </ul> </li> </ul> <p>2 Acceptable Approximations</p> <ul> <li>Demand Smoothing: Smooth thresholds<ul> <li>Reduces iterative oscillation</li> <li>Experiments use \\(2^{-10} \\approx 0.1\\%\\)</li> </ul> </li> </ul> <p></p> <ul> <li>Transaction Fees: Charge small percentage fee<ul> <li>Range of approximate clearing prices, not single point</li> <li>Experiments use \\(2^{-15} \\approx 0.003\\%\\)</li> </ul> </li> </ul> <p></p> <p>Accounting for Approximation: Prices into Trades</p> <p>\\(max\\) Trade Volume (at T\u00e2tonnement\u2019s Prices):</p> \\[   \\displaystyle max \\sum_{\\Alpha,\\Beta} p_{\\Alpha} x_{\\Alpha \\Beta} \\] <p>\\(s.t.\\) Correct (Smoothed) Offer Execution:</p> \\[   \\displaystyle s.t. \\; p_{\\Alpha} L_{\\Alpha \\Beta} (\\frac{p_{\\Alpha}}{p_{\\Beta}}) \\leq p_{\\Alpha} x_{\\Alpha \\Beta} \\leq p_{\\Alpha} U_{\\Alpha \\Beta} (\\frac{p_{\\Alpha}}{p_{\\Beta}}) \\; \\forall \\Alpha,\\Beta \\] <p>Assets Conserved (After Fees):</p> \\[   p_{\\Alpha} \\sum_{\\Beta} x_{\\Alpha \\Beta} \\geq (1 - fee) \\sum_{\\Beta} p_{\\Beta}x_{\\Beta \\Alpha} \\; \\forall \\Alpha \\] <p>Overall Performance</p> <ul> <li>Linear Scalability (contention with background work (logging))</li> <li>Log Dependence on \\(\\#offers\\)</li> </ul> <p></p> <p>Tradeoff: Batch Latency</p> <ul> <li>Cost of batch exchange model is trade latency: If we\u2019re already paying for consensus latency, the marginal cost is 0.</li> <li>Does this latency matter?<ul> <li>This is a policy question, not a technical question</li> <li>Good policymaking requires mapping out the entire designs pace</li> </ul> </li> </ul> <p>Implementation</p> <ul> <li>Self-contained replicated state machine:<ul> <li>https://github.com/scslab/speedex</li> <li>\\(\\approx 30,000\\) LOC (C++20)</li> </ul> </li> <li>Planned (then cancelled? then uncancelled and cancelled again? unclear) deployment in Stellar (a public blockchain):<ul> <li>https://github.com/gramseyer/stellar-core</li> <li>Required only \\(\\approx 2,000\\) LOC (C++17)</li> <li>Commutative semantics and economic improvements valuable on their own: This is the only piece that needs a coordinated upgrade (\u201chardfork\")</li> <li>Implement scaling later, as needed (much easier)</li> </ul> </li> </ul> <p>Constraints from the Stellar Implementation</p> <ul> <li>Limiting factors on scaling number of assets:<ul> <li>T\u00e2tonnement iteration runtime</li> <li>A fewhundred is probably fine</li> <li>Linear program runtime (Off the shelf solver becomes a problem around \\(\\approx 80\\) assets</li> </ul> </li> <li>Hard (legal) constraint: Can\u2019t charge trading fees</li> <li>Need absolute runtime guarantee</li> </ul> <p>Accounting for Stellar\u2019s Constraints</p> <ul> <li>Run T\u00e2tonnement as normal, then solve this (different) LP</li> <li>Cannot enforce lower bound on \\(x_{AB}\\) (cannot guarantee LP feasibility)\u2014Also allows guaranteed runtime bound</li> <li>Linear program without fees is \u201ctotally unimodular\u201d:<ul> <li>\\(p_{\\Alpha} x_{\\Alpha \\Beta} \\rightarrow y_{\\Alpha \\Beta}\\)</li> <li>Much faster to solve (flow problem)</li> </ul> </li> </ul> <p>Everything Else in an Exchange</p> <ul> <li>Asset transfers</li> <li>Replay prevention</li> <li>Signatures</li> <li>Double-spends</li> <li>...</li> </ul> <p>Example: Payments</p> <p></p> <p>Reading from a Snapshot: What if two transactions write to the same location?</p> <p></p> <p>Typed State Changes: What if a balance becomes negative?</p> <p></p> <p>Preempt constraint conflicts when assembling new blocks</p> <p>Efficient reserve-commit process (2-phase commit)</p> <p></p> <p>\u201cSnapshot Reads\u201d + \u201cConflict-sometimes-Replicated Data Types\u201d</p> <p></p> <p>Detour: Smart Contracts</p> <ul> <li>We can give these tools to a smart contract-Not just for purpose-built applications</li> <li>For example: Lending markets and collateral deposits</li> <li>Groundhog [Ramseyer-Mazi\u00e8res, 2023]-Context: this is Geoff's day job</li> <li>Open question: What applications need purpose-built mechanisms, and what do not?</li> </ul>"},{"location":"guest-lecture-geoff-ramseyer/#recap","title":"Recap","text":"<p>Incentive (Mis)alignment in Blockchain Exchanges Recap</p> <ul> <li>Mismatch between blockchain abstaction and implementation</li> <li>Especially bad for implementing an exchange (Arbitrage spam, front-running, MEV, ...)</li> <li>How can we redesign an exchange to fit on a blockchain?</li> <li>Different computational model: State machine on unordered batches of transactions</li> <li>Exchange needs a different pricing mechanism (Compute Arrow-Debreu equilibria efficiently)</li> </ul> <p>Moral: Mismatch between system architectures and user incentives leads to broken systems, inefficiency, and (if you can exploit it) profit.</p>"},{"location":"guest-lecture-kshipra-bhawalkar/","title":"Incentives in Sponsored Search Auctions","text":"<p>May 12, 2025</p> <p>This is a guest lecture offered by Kshipra Bhawalkar (Lane), Research Scientist Market Algorithms, Google Research.</p>"},{"location":"guest-lecture-kshipra-bhawalkar/#motivation-optimizing-sponsored-search-ads","title":"Motivation: Optimizing Sponsored Search Ads","text":"<p>If you do a Google search, you will find different elements on the results page:</p> <ul> <li>Knowledge Panel: wikipedia-level information about what you are searching for.</li> <li>Local Results: to help you find stores to go buy the products.</li> <li>Web Search Results: for places that will tell you more about what you are looking for.</li> </ul> <p></p> <p>Most prominently, there are a few text ads, that are placed there to fund the rest of the page: this is 3-digit billion dollar a year business that allows us to access free search results. In particular, our ability to search for the most esoteric queries that nobody would advertise for is funded by commercial queries.</p> <p>How does Google (or other companies) select the ads that go along with the sponsored search results?</p> <p>These ads have actually gone through a few different versions as people's browsing habits, and the way the organic search results were displayed, changed.</p> <ul> <li>In the 2000s, these ads used to be a bunch of blue links: just a headline URL and some text.</li> <li>Nowadays if you go to\u00a0Google.com, you see ads that are richer.</li> <li>In the future, as LLMs enable more and more innovation, there might be other kinds of ads that might show up as the search page itself changes.</li> </ul> <p></p> <p>How do the algorithms and auctions for selecting ads evolve with changing presentation?</p>"},{"location":"guest-lecture-kshipra-bhawalkar/#past-position-auctions","title":"Past: Position Auctions","text":"<p>Let's start with position auctions, which is the simplest model of search ads that had existed for many years.</p> <p></p> <p>If you went to\u00a0Google.com\u00a0in 2000, what you used to get is what is known as \"ten blue links.\" All of the search results looked very similar, with a headline, a URL, and 1-2 lines of text describing the pages to you. Then, you could read the description to decide if you wanted to click on a specific page or not.</p> <p>At the top, the first couple of links might be ads. These ads always went in fixed positions, and the format of these ads always matched what the search results were shown with it. The main parameter was the fixed positions in which the ads could show.</p> <p>We can think of it as an optimization problem, where we want to show at most \\(k\\) ads, where \\(k\\) might be equal to \\(3\\) or \\(4\\), and our goal is to show ads that best match the intent of the user:</p> <ol> <li>The search engine asks the advertiser how much they value the user clicking on their website.</li> <li>The advertiser reports a value \\(v_i\\) per click for the user.</li> <li>The search engine estimates the probability \\(pCTR_{i,j}\\) of the user clicking on the ad in position \\(j\\).</li> <li>The search engine maximizes the social welfare \\(\\displaystyle \\sum_{i} v_i \\; pCTR_{i,j}\\), i.e. the value that both advertisers and user get out of this match.</li> </ol> <p>So, how do we decide? How much should the advertiser want to pay to match the user?</p> <p>This is like a weighted matching problem. But if we assume that the click through rates are separable, which means that for every advertiser, their click-through rate in a particular position is a function of the advertiser's specific click-through rate and a position effect, which we'll call the position normalizer, then this problem becomes much simpler and we can solve it using a simple algorithm.</p> <p>Optimization Problem</p> <ul> <li>Assume: Separable \\(pCTR\\): \\(pCTR_{i,j} = pCTR_i \\cdot pos \\text{-} norm_j\\).</li> <li>Goal: Maximize \\(\\displaystyle \\sum_{i} v_i \\; pCTR_{i,j}\\).</li> <li>Algorithm:<ul> <li>Rank ads by \\(eCPM_i = v_i \\cdot pCTR_i\\).</li> <li>Show \\(j\\)-th ad in position \\(j\\).</li> </ul> </li> </ul> <p>What is that simple algorithm? We just rank the ads by their \\(eCPM\\), which is a product of their value and clic-through rate, and then just show each ad in the corresponding position. So, at least in the position option case, under a fairly reasonable assumption, we could decide which ad goes there fairly efficiently by just sorting the ads and showing them in that order.</p> <p>However, there is another challenge here: we are asking the advertisers how much they want their ad to be shown. If we were not charging any payments, then everybody would say \"$1M dollars!\" Each advertiser always wants their ad to be always be shown, because they value all users equally, and Google is just giving them traffic for free. Thus, Google has to charge the advertiser for the value they report, to align incentives, otherwise they will just report ridiculous values.</p> <p>Example: Goto.com/Overture Auction (1997-2003)</p> <p>When Internet ads were coming up, there was a company called Goto.com, which later became Overture and then was acquired by Yahoo! They started with a First Price auction format, which has some obvious issues.</p> Ad Bid Position Payment 1 $10 1 $10 2 $8 2 $8 <p>In this example we have two advertisers. They have bids of \\(\\$10\\) and \\(\\$8\\). With the First Price auction, everyone pays what they bid. So the first one will pay \\(\\$10\\), and the second one will pay \\(\\$8\\). But there is an obvious manipulation for any one of them.</p> Ad Bid Position Payment 1 $8.01 1 $8.01 2 $8 2 $8 <p>Suppose the first advertiser instead lowers their bid to a little bit above \\(\\$8\\), such as \\(\\$8.01\\). They are still in the same position, but now their payment is a lot lower. This is definitely a win for the advertiser and they should always try to do this.</p> <p>When Google came along, it was kind of known that there was this obvious manipulation opportunity for the advertisers, and advertisers were exploiting this. That led to a lot of system instability for Overture, because their advertisers would keep constantly trying to keep changing their bid.</p> <p>Google engineers wanted to do something different to fix this issue. They started charging the second price, which is the minimum bid at which a given ad still wins the same position. This actually extends the Second Price auction format or the VCG mechanism to multiple positions, even though the people who were around at that time report that they came at it from first principles.</p> <p>Example: Generalized Second Price (GSP) Auction</p> <p>Suppose we are now running an auction and we are going to rank these ads by click-through rate. For simplicity, in this example, we are assuming that all the click-through rates are equal to \\(1\\), so we are just ranking ads by bids.  There is still a position effect: if you show an ad in the higher position, it gets more click. That is represented by the position normalizer column.</p> <p>Suppose we have four advertisers A, B, C, and D, bidding \\(\\$10\\), \\(\\$8\\), \\(\\$2\\), and \\(\\$0.01\\) respectively. Then, A will win the three positions, and each one of them will pay the next highest bid: A pays \\(\\$8\\), B pays \\(\\$2\\), and C pays \\(\\$0.01\\). That is already the most obvious incentive to lower your bid to find the best threshold where you can still win.</p> Pos pos-norm Winner Payment 1 0.8 A $8 2 0.5 B $2 3 0.1 C $0.01 <p>Are we done yet? Is there any more reason for an advertiser to manipulate their bid, or is this auction truthful?</p> <p>We have to set some parameters here to think through this. Let's focus on advertiser A, and suppose their value per click is \\(\\$10\\). What they are optimizing is a quasi-linear utility which is the click-through rate times value-per-click minus cost-per-click. Right now, with the current bids, A is winning position \\(1\\), and their utility would be: \\(0.8 \\cdot (10 - 8) = 1.6\\). Is there a way for a to do better, i.e. is there a different way for A to get a better utility?</p> <p>If A bids anything less than \\(\\$10\\) dollar, they end in position 2, and their utility \\(0.5 \\cdot (10 - 2) = 4\\), which is definitely better. This happens more generally in GSP where there's an incentive to lower your bid to find the place where you get a good trade off.</p> <p>Question: Is the value of A constantly 10, or can it hypothetically move down for position 2 or 3?</p> <p>Their value-per-click is the same, but because their click-through rate is changing, their utility will change. That is based on the \\(pos-norm\\) regularizer, which is changing the actual value.</p> <p>Now that we have seen an example, let\u2019s unravel the theory on how to design good auctions. Economists have thought hard about general auction design and how to make design auctions that are truthful, where the participants are incentivized to report their true preferences for goods. So in the position auction bidding, we want to design an auction where the advertisers report their true value for click.</p> <p>A mega giant hammer in economics is the VCG auction. This is a very general result that allows us to come up with a truthful auction in lots of different settings. It is also very simple to discuss.</p> <p>VCG Auction</p> <p>We want to choose the best of ads to show to maximize social welfare.</p> <p>Payment Rule: For each ad \\(i\\) show, charge the externality.</p> <p>In other words:</p> <p>\\(Payment(i) = (best \\; efficiency \\; without \\; i) - (Efficiency \\; of \\; ads \\; other \\; than\\; i \\; in \\; selection)\\).</p> <p>Technically, this is the exact formula we saw earlier. The payment we charge for each participant is their externality, which is how much they are taking away from others by winning. The way we calculate this externality is by asking what would have happened if this advertiser was not there, and then subtract from that what everybody else is getting when they are actually there.</p> <p>VCG in general might not be easy to implement, but in the case of position auctions, because of the algorithm we described earlier, where we just rank ads and select the best ones, it is actually very easy to implement. Let's see what happens in our example.</p> <p>Example: VCG Position Auction</p> <p>Assume the same advertisers and bids as in the previous example:</p> Pos pos-norm Winner Payment 1 0.8 A $8 2 0.5 B $2 3 0.1 C $0.01 <p>To calculate the payment for A:</p> <ol> <li>We find the best allocation without A: if A goes away, all the other advertisers move up, so B goes in position \\(1\\), C goes in position \\(2\\), and D goes in position \\(3\\). That gives the first term in the formula, i.e. the social welfare of showing B, C, and D in positions \\(1\\), \\(2\\), and \\(3\\).</li> <li>We subtract from it what B and C were getting in the original allocation:  B in position \\(2\\) and C in position \\(3\\).</li> <li>We can rearrange to make it look right in terms of individual advertisers bids, i.e. how much are they gaining or losing relative to when A is present or not.</li> </ol> <p>This gives us:</p> <p>\\(Paymen(A) = (0.8 \\cdot 8 + 0.5 \\cdot 0.2 + 0.1\u00a0\\cdot 0.01) - (0.5 \\cdot 8 + 0.1 \\cdot 2) = (0.8 - 0.5) \\cdot 8 + (0.5 - 0.1) \\cdot 2 + 0.1 \\cdot 0.01 = 2.301\\).</p> <p>There is actually another way of coming up with auctions, which is a classic result from Myerson, i.e. a general characterization for designing truthful auctions.</p> <p>Myerson (1981):</p> <p>For single parameter settings, an auction is truthful if and only if the allocation \\(x_i\\) is monotone and the payment has a specific form:</p> <p>\\(p_i(b_i, b_{-i}) = b_i x_i (b_i, b_{-i}) - \\int_0^{b_i} x_i(z, b_{-i})dz\\)</p> <p></p> <p>A single parameter setting that the participants only have one-dimensional preferences, and a monotone allocation rule means that when somebody reports a higher preference, i.e. they place a higher bid, they get a higher position and more clicks. Then, the payment is given by an integral (Myerson derives this by using some inequalities that we need to satisfy to make sure that nobody would want to game this system).</p> <p>This rule is charging the area above the curve, so we can draw a curve which plots how the allocation is changing as the bids goes up. The curve is a step function, because in position auctions, you go from winning position \\(3\\) to \\(2\\) to \\(1\\): as you bid higher and higher you go to higher and higher positions. So, you get more click-through rate.</p> <p>Let's go back to our example:</p> <p>Once again, assume the same advertisers and bids as in the previous example:</p> Pos pos-norm Winner Payment 1 0.8 A $8 2 0.5 B $2 3 0.1 C $0.01 <p></p> <p>\\(Paymen(A) = (0.8 - 0.5) \\cdot 8 + (0.5 - 0.1) \\cdot 2 + (0.1 - 0) \\cdot 0.01 = 2.301\\).</p> <p>This is the same result as with VCG.</p> <p>GSP is not truthful</p> <p>We saw that Myerson and VCG are two different ways of getting truthful auctions We even verified, at least on one example, that these two do the same thing.</p> <p>Below, the picture on the left is what Myerson does, and the picture on the right is what GSP does.</p> <p></p> <p>Instead of charging this area above the curve, GSP is greedy and charges the big rectangle. Even though, by lowering the bid, the advertiser would get fewer clicks, we disregard that and charge them a very high price for all of their clicks. That is why the prices are much higher with GSP than they would be with Myerson/VCG.</p> <p>Meta/Facebook decided to use VCG and that is the right thing to do. It is definitely easier to start using VCG from scratch, because switching from GSP to VCG is hard, as it causes revenue to go down.</p> <p>Then on the other end of the spectrum, display ads, which are ads that show up on websites when you visit them, use First Price auctions, for historical reasons, given the complexity of the display ad ecosystem.</p> <p>Position Auction Conclusion</p> <p>Historically:</p> <ul> <li>Goto.com/Overture/Yahoo used First Price auctions.</li> <li>Google introduced GSP auctions.</li> <li>Others, including Meta, have implemented a VCG-ish auctions.</li> <li>Display ads tend to use First Price auctions.</li> </ul> <p>Some research papers on this topic:</p> <ul> <li>Edelman, Ostrovsky, Schwartz (2007)</li> <li>Aggarwal et al. (2007)</li> <li>Varian (2007)</li> <li>The Economics of Internet Search (Varian, 2006)</li> </ul> <p>Question: How important is truthfulness for these auctions?</p> <p>There are layers here. First Price can be obviously bad, because advertisers can experiment with their bids, and they can lower their bids to get the same thing but cheaper, so there is an obvious manipulation opportunity to game. With GSP-like auctions, it is not obvious how to manipulate the bids, and the participants need to think about the margins that they need to tolerate, so the click-cost trade-off is still there, but the advertisers still need to estimate how much value they put on the clicks, and that might change.</p> <p>Question (follow-up): Who is estimating that?</p> <p>There are some manual advertisers who are doing it themselves, they look at the Adwords interface, and they try to find the sweet spot by looking at plots in the interface.</p> <p>There are also fancier products, like auto bidders, where advertisers can just input their average cost, and the outbidding system (which is run by Google), will try to optimize for them. Truthfulness remains helpful, because the autobidder can do a uniform bid and get the best value.</p> <p>Question: Are platforms incentivized to model each user\u2019s behavior and intent to optimize click-through rate, so that the platform can charge the most?</p> <p>The better we can estimate the probability of click, the better we are matching the advertiser to the user, even though it does not always translate into more revenue.</p> <p>For instance, if there are two advertisers, like Nike and Adidas, but the user likes Nike and not Adidas, then there is no pricing pressure to charge more to Nike, which is why revenue does not always follow. That said, definitely, the more accurately we can estimate the click-through rate, the greater the efficiency.</p>"},{"location":"guest-lecture-kshipra-bhawalkar/#present-rich-ad-auctions","title":"Present: Rich Ad Auctions","text":"<p>These days, if you go to\u00a0Google.com, you get more complicated layouts of ads, which can show ad extensions, site links, seller ratings, promotions, which can change the probability of the user clicking on the ad. It is also a better value for the advertiser because the user might actually go buy the product. </p> <p></p> <p>However, these extensions also take more space, and we want to make the best use of all the space that the user looks at on\u00a0Google.com.</p> <p></p> <p>We bound the total amount of space that we control to find the best set of ads that fit within that space. For instance, we may need to decide what to do, when, for the same advertiser, we can either show a small or a large ad.</p> <p>Optimization Problem</p> <p>Constraints:</p> <ul> <li>Only one ad \\(j(i)\\) per selected advertiser.</li> <li>Total space bounded by \\(\\displaystyle \\sum_i h_{j(i)} \\leq H\\).</li> </ul> <p>Goal: Choose the set of rich ads to maximize the social welfare \\(\\displaystyle \\sum_i v_i \\; clicks_{j(i)}\\)</p> <p>We can extract the theoretical question that we are trying to solve here from the setting:</p> <ul> <li>We have a number of advertisers.</li> <li>Each advertiser has a few different ads that they have provided us.</li> <li>We are required to show only one ad per advertiser because we don't want to show two ads for, say, flowers.com, in the same view (that would not be a good user experience).</li> <li>The total amount of space occupied by these ads has to be bounded.</li> <li>Our goal is to still to maximize social welfare, to maximize the value of matching these advertisers to the users.</li> </ul> <p>This problem is actually hard to solve optimally: it is known as a Multiple-Choice Knapsack Problem, and it is NP-hard.</p> <p>Computer scientists have spent a lot of time trying to understand which problems are hard to solve, and they came up with a class called NP, which are problems that we don't know if they are hard to solve, but we don't know how to solve them either. There is a conjecture \\(P=NP\\) (or not), which is \\(\\$1\\) million problem.</p> <p>As long as we don't know where this conjecture goes, we don't know how to solve this problem in a reasonable amount of time, especially as the instance grows large. In this case, the number of ads per advertiser can go on the order of thousands, and, because we can do many combinations of these ad extensions, the number of ads that show up to an auction in the worst case can also be on the order of thousands.</p> <p>One more practical constraint for showing these ads is that we cannot do anything worse than \\(n \\cdot \\log(n)\\). A polynomial n-fourth \\(O(n^4)\\) algorithm is not practical, because whenever you go to\u00a0Google.com, we need return the result at the worst in 200 milliseconds. That is including the time to retrieve old ads from various systems and run the auction. So, the auction itself gets less than 10 milliseconds to run.</p> <p>In other words, this problem is theoretically and practically hard to solve. So, how should we solve this problem?</p> <p>Let's take a little diversion and consider a simpler problem: the Knapsack Problem.</p> <p>The Knapsack Problem</p> <ul> <li>You have a snack budget and you are going on a road trip.</li> <li>You go to a convenience store and you want to buy as many snacks as possible. - Each snack has some value (how happy it makes you).</li> <li>There is a cost for purchasing each snack.</li> </ul> <p>You want to find a set of snacks that maximize the total value you get for this, subject to the constraint that the total cost of your snacks that you're purchasing is less than your budget, say, $100. This problem is still in NP hard, but over the next few slides, we'll look at a simple algorithm for solving this problem.</p> <p>A problem may be NP-hard, and yet, we still have to solve it because we are building something in real life. This Knapsack Problem needs to be solved, and an approach is to use an approximation algorithm.</p> <p>What is approximation algorithm? It is a heuristic/an algorithm that gives us a solution that might not be optimal, but that we can accept if we can prove some approximation guarantee, such as that we will get at least half of the best solution possible. That is a really good guarantee to have because, no matter what instance shows up tomorrow, you're at least getting a reasonable amount of value. Of course, the closer we can make this approximation to \\(1\\), the better, but sometimes it is not possible.</p> <p>Theorem: We can solve the Knapsack Problem in polynomial time to obtain a 2-approximation, i.e. \\(Value(ALG) \\geq \\frac{Value(OPT)}{2}\\).</p> <p>Knapsack Approximation Algorithm</p> <ol> <li>Sort items in the bang-per-buck = value/cost order.</li> <li>Select items until knapsack is almost full (i.e. the next item will overflow).</li> <li>Consider the last item by itself.</li> <li>Pick the higher of steps 2 and 3.</li> </ol> <p>The algorithm is very simple. It basically sorts the items by bang-per-buck: you're not just going to greedily pick by value, but instead, take the ratio of the value and cost, and pick in that order. We'll keep doing this until the knapsack is almost full, and once the next item cannot fit in the bag anymore, it will overflow. We will consider this last item by itself, and we are going to pick the better of this bang-per-buck order, or this last item by itself.</p> <p>Note that Step 3 here is required because sometimes there might be a single item, that is very large but doesn't have a good bang-for-buck (i.e. a very large item with a very high value), and we want to make sure we don't miss it.</p> <p>Let's look at an example.</p> <p>Example: Knapsack Approximation</p> <p>We have \\(3\\) items and the knapsack capacity is \\(2\\):</p> Value Cost Bang-per-buck 18 1 18 21 1.2 17.5 30 2 15 <p>The optimal solution in this case is to pick the last item which has a lot of value.</p> <p>What does the algorithm do?</p> <ul> <li>It considers the ratio of value divided by cost, which is the order the item are shown here.</li> <li>It adds the items one at a time until the knapsack is full. It turns out that we have to stop just after the first item, because the next item would overflow the knapsack.</li> <li>It considers that last item by itself, which is the \\((21, 1.2)\\) item.</li> <li>It picks whichever is the highest, i.e. the \\((21, 1.2)\\).</li> </ul> <p>We see that \\(21 \\geq \\frac{30}{2}\\), to this result satisfies our guarantee of achieving half of the optimal solution.</p> <p>How can we prove that this algorithm is actually a good one? We can capture what the optimal solution looks like\u2014we are not going to solve it, but we can mathematically represent it using an integer program.</p> <p>2-Approximation of The Knapsack Problem</p> <p>Let \\(v_i\\) be the value of item \\(i\\), \\(c_i\\) be the cost of item \\(i\\), and \\(x_i\\) denote whether we selected item \\(i\\) or not.</p> <p>We want to maximize our value, which is \\(\\displaystyle \\sum_{items} v_i \\cdot x_i\\).</p> <p>We also know that the sume of the costs of the items cannot exceed our budget \\(B\\), so \\(\\displaystyle \\sum_{items} c_i \\cdot x_i \\leq B\\).</p> <p>In addition, any item can only be selected at most once, and we cannot select fractional items in the original problem, but we can relax it into a linear program, such that \\(0 \\leq x_i \\leq 1\\).</p> <p>This program selects all items with \\(\\frac{v_i}{c_i} \\geq \\lambda\\) for some \\(\\lambda \\geq 0\\), and only the last item is added fractionally.</p> <p>We see that:</p> <p>\\(Value(ALG)\\)</p> <p>\\(= max \\; \\{Value(bang \\text{-} for \\text{-} buck \\; solution), Value(last \\; item)\\} \\)</p> <p>\\(\\geq \\frac{Value(bang \\text{-} for \\text{-} buck \\; solution) + Value(last \\; item)}{2}\\)</p> <p>\\(\\geq \\frac{Value(linear \\; program)}{2}\\)</p> <p>\\(\\geq \\frac{Value(OPT)}{2}\\)</p> <p>*Note: Linear programs in general can be solved in polynomial time, so we can solve them pretty fast, even though they might give us a fractional result. However, for proving approximation guarantees for these algorithms we should always use them. They can be a good tool for being able to reason about what the optimal solution looks like, and thinking about how we can approach it.</p> <p>So, that was for the Knapsack Problem, but the rich ad problem is a bit more complicated: it is Multiple-Choice Knapsack Problem. In addition to the knapsack constraint of packing these ads, we also have the constraint that we are only allowed to choose one ad per advertiser.</p> <p>There is in fact a similar result, which comes from Sinha and Zoltners's paper from 1978, where they again characterize the optimal solution for the linear program. It does allocation using incremental bang-for-buck, and then they can show that that kind of algorithm will give you a 2-approximation.</p> <p>Theorem: Sinha, Zoltners '78</p> <p>We can optimize social welfare within a factor of \\(2\\), i.e. \\(Value(ALG) \\geq \\frac{Value(OPT)}{2}\\).</p> <p>Key takeaway: We cannot solve the Multiple-Choice Knapsack Problem optimally.</p> <p>Can we solve the Rich Ad Auction problem with VCG?</p> <p>Recall that VCG solves the same optimization problem many times. First, it solves the optimization problem to find the best set of ads to show, and then, for each of the ads that you have selected, it solves the optimization problem one more time without this advertiser to calculate the payment. In other words, VCG requires us to solve the same optimization problem again and again. However, as we have seen, this particular optimization problem is hard to solve. Therefore, we cannot use to solve the Rich Ad Auction problem.</p> <p>One of the lessons from reality is that advertisers are strategic along many different domains, rather than a single paramter. For instance, advertisers can be strategic about which ads they give to us to show, especially if it helps them.</p> <p>Suppose an advertiser has a small, a medium and a large ad, but for whatever reason, they say I don't have the medium ad anymore; so we can either show the small ad or the large ad, and if that can somehow give them a better outcome, maybe the large and gets shown. Thus, they get a lot more space by withholding the medium ad than by providing it to us, and that is also something we want to guard against.</p> <p>Example: Is there a way for the advertiser that owns the \\(21\\) and the \\(70\\) value ads to force the auctioneer to choose the outcome on the right?</p> <p></p> <p>If the advertiser just removes the small ad, then the auctioneer will be forced to choose between the other two ads and the one large ad, and they will actually choose the one larger. This is the simplest example where we can start seeing that this strategic dimension of which ads to provide starts to matter.</p> <p>Generally, the more the advertiser gives us, the more flexibility for us to make the best use of their assets to show to the user. So, we want to incentivize advertisers to give us as many assets as possible. If we are running an auction that incentivizes advertisers to remove some of their assets because they are hurting some other assets that are more promising than that, that is bad auction design. This is another reason why truthfulness matters.</p> <p>One of the tricks we can use to solve the Rich Ad Auction problem is to leverage Myerson's Lemma.</p> <p>Myerson's Lemma</p> <p>If we can design allocation rules that are monotone over the values \\(v_i\\), and the advertisers provide a set of ads \\(A_i\\), we can design payment rules that will make the auction truthful.</p> <p>Note that neither the integer optimization algorithm and nor the Sinha-Zoltner'76 algorithm are monotone over a set of rich ads. So, is there a monotone algorithm to solve this problem?</p> <p>Lemma: Allocating in bang-per-buck order is monotone. </p> <p>Allocating in bang-per-buck order is independent of which ads are present, so doesn't change based on the subset of ads.</p> <p>We can find another approximation algorithm for the Multiple-Choice Knapsack Problem, which uses randomization to preserve monotonicity.</p> <p>Theorem</p> <p>The following randomized monotone algorithm that allocates in bang-per-buck order with probability \\(\\frac{2}{3}\\), and otherwise picks the single highest-value ad, provides a 3-approximation to the fractional optimal solution and is monotone.</p> <p>Rich Ad Auctions Recap</p> <ul> <li>New auctions with different layouts of ads create computational challenges.</li> <li>Advertisers have time to be strategic about which assets to provide, which creates a new strategic dimension to consider.</li> <li>In practice, we rely on more involved heuristics used to get as close to optimal as possible.</li> </ul> <p>Some research papers on this topic:</p> <ul> <li>Aggarwal et al. '22</li> <li>Harris et al'17, </li> <li>Hartline et al'18</li> </ul>"},{"location":"guest-lecture-kshipra-bhawalkar/#future-auctions-for-llm-generated-ads","title":"Future: Auctions for LLM generated ads","text":"<p>Let's speculate a bit about what the future of search ad auctions on\u00a0Google.comcom\u00a0might look like. We all have heard that large language models are changing everything, and we might wonder how will search ads change with them?</p> <p></p> <p>One way they might change is we can use large language models to generate ads on the fly.\u2028So we could just have the models themselves, summarize different advertisers, uh,\u2028content and choose the space that's available effectively and also tailor the format\u2028of the ads to match whatever summary organic content we are showing to the user.</p> <p>In a paper published last year, some of Kshipra Bhawalkar's colleagues looked at how can we use LLMs to generate these ads that better align with the context, and and how we should we run auctions for these ads.</p> <p>The key inspiration here is Retrieval-Augmented Generation (RAG). LLMs in general can hallucinate (they might make up things), and RAG has been a way to make sure that they are grounded.</p> <p></p> <p>RAG usually works by retrieving a bunch of documents, such as web pages, and then asking the LLM to summarize those documents instead of outputting something from its memory. In that way, it stays true to the ground sources and answers the user's queries.</p> <p>You can see this in action when you go to\u00a0Google.com Nowadays, you get this AI overview, which generates a summary of some of the top results that is generated using RAG.</p> <p></p> <p>Can we do the same with ads? Can we take the advertisers' assets and their bids, and let the LLM generate an ad summary that tells the user what the best products are for whatever they are searching for?</p> <p>It is more complicated because we want the advertiser to be able to specify how much they value being shown to the user. So, they will use their bids to influence how much space they are given  or how their content is presented. We also want use payments to make sure that advertisers are incentivized to report their true value for being shown to the user.</p> <p></p> <p>Can't we just let the LLM do it all, i.e. take the advertisers bids, figure out who gets how much space, and how much to charge them?</p> <p>That is actually hard:</p> <ul> <li>We don't know how to do this.</li> <li>LLMs are neither interpretable nor controllable, and there are alignment issues (we cannot make sure that they are staying true to the best way of presenting the information to the user).</li> <li>LLMs cannot provide good monotonicity guarantees or incentive guarantees to the auctioneer.</li> </ul> <p>Instead, we generalize position auctions and rich ad auctions, to design auctions with large language models: this is called the Factorized Auction Model.</p> <p></p> <p>We are factorizing the auction: modules are divided into different parts, where each part will be easier to understand, so we can make sure we can provide good guarantees for it.</p> <p>There is:</p> <ul> <li>An auction module that takes the bids from the advertisers and decides who gets how much space, and then also decide to what prices they should be charged.</li> <li>An LLM module that takes this space considerations, and adds the description and URL of the ad to content from the advertisers' website, and then generates the summary.</li> <li>A feedback loop in the form of click-through rate, where we learn from showing these ads to the user how they are liking it, how much prominence is useful for a given advertiser, and feed that back for running their auctions.</li> </ul> <p>The nice thing about breaking everything up into pieces is:</p> <ul> <li>The auction part is a classic auction: it maps the bids and visitors to prominences, which is just a scalar number, and it also calculates CPCs (Cost Per Clicks). We can provide good guarantees for this because this is not a black box anymore we know exactly what is going on.\u00a0</li> <li>The LLM part is still non-trivial because we need to fine-tune the lLLM's output to make sure that it respects the prominences that we specify, and that it makes the best use of that space for convincing the user to click on this ad. It might not work out of the box, but we can train the model to do that.</li> <li>The feedback loop is all about learning the model as a function of prominence as opposed to as a function of \u201cThis is the exact ad that was shown to the user.\u201d And so there is a little bit of indirection there.</li> </ul> <p>With all the above, we could have an auction for LLM-generated ads that comes with good guarantees about its optimization and incentive compatibility.</p> <p>Factorized Approach: Properties</p> <ul> <li>Separates out the work between auction and LLM through prominences.<ul> <li>Auction does not know the summary, LLM does not know the bids.</li> <li>Product/Business can choose what kinds of summaries are useful for the users.</li> </ul> </li> <li>Strictly generalizes the current position (or rich-ad) auction.<ul> <li>Prominences: Permutations, i.e. the winning ads, their order (and potentially their formats).</li> <li>\u201cLLM\u201d:  Place winning ads (with allocated formats) in the given order.</li> <li>pCTR model: predict position-aware CTR.</li> </ul> </li> </ul> <p>Theorem: Incentive Compatibility</p> <p>If the auction allocation function is monotone and the LLM is monotone (it converts higher prominence to higher clicks), then the combination is monotonic and can be made truthful.</p> <p>Theorem: Welfare Maximization</p> <p>If also, the pCTR_Model function is unbiased, then the auction can choose a welfare-maximizing outcome.</p> <p>Theorem: Universality</p> <p>Any combined ML+auction model can be represented in the factorized model.</p> <p>Example: Dynamic Word-length Summary</p> <p></p> <p>Suppose we ran an auction with three advertisers who could show ads for \"golf instruction for beginners.\"</p> <p>We are allowing them to have different space or different positions on the page:</p> <ul> <li>The summary on the left is showing all three advertisers with similar amounts of space allocated to all of them.</li> <li>The summary on the right is showing only the top two advertisers, and the top one in particular is getting the lot of space.</li> </ul> <p>Depending on what the bids were for these three advertisers, we might allocate the summary on the left or the summary on the right. Then, the LLM will generate something.</p> <p>This is where the business side comes in: if you look at this and think that nobody is going to click on the ads, then we can make sure that the LLM does a retune to do as good a job as possible with that. Then, the auction can be optimized separately to come up with good mapping of bids to prominences.</p> <p>As mentioned, we can provide incentive compatibility and welfare maximization guarantees. That requires knowing about how the prominences will map through click-through rates, because the social welfare cares about click-through rate (and not so much about prominence). However, if the factors have some specific format, like we assumed earlier\u2014there is a advertiser component, a condition component, a prominence component\u2014it is reasonable to assume that that is a concave function. So, as you get more and more space, you are not able to get more value it: maybe it is like a polynomial-exponential function of some sort prominence raised to \\(\\beta\\).</p> <p>Optimization with Continuous Ads</p> <p>Factorized CTR model:</p> <p>\\(pctr_i^{final} = pctr_i \\cdot pos_norm_i \\cdot f(Prom_i)\\)</p> <p>where</p> <p>\\(f(Prom_i) = Prom_i^{\\beta} \\; for \\beta \\in (0,1]\\).</p> <p>If it is this format, or a general concave format with nice differentiability, we can actually obtain a closed-form solution for what is the right way to allocate these ads.</p> <p>Case Study: Dynamic Word-length Summary</p> <p>Auction component: Generalized Proportional Allocation (GPA), where the relative prominence is proportional to the power of:</p> <p>\\(ecpm \\approx bid \\cdot base \\_ pctr \\cdot position \\_ normalizer\\).</p> <p>The formula is simple: we just give each ad a location that is proportional to their eCPM, which is the bid times the pCTR and a position optimizer. Indeed, we want to sort the ads in the same order as we did with position auctions. Then, we can show that if the pCTR has a nice format, then this kind of allocation is welfare-maximizing.</p> <p>Optimization Perspective: The \u201cLLM advantage\u201d</p> <p>LLMs can aim for the best ad allocation under total space constraint:</p> Current (Rich Ads) Future? (LLM-Generated Ads) Predefined set of fixed formats (sizes) LLM summarized ads, Dynamically generated prominences (sizes) Combinatorial optimization &amp; auction design Continuous optimization <p>Previously, we talked about rich ad auctions: we spent a lot of time figuring out how to optimize them because they had some fixed sizes, so we had a packing problem that we were trying to solve.</p> <p>With LLM-generated ads, some of these challenges go away, because now we can think of it as like a fractional problem: no matter how much space is available, we will find the best ad to go into that space. With this extra freedom, maybe our optimization problem might become easier.</p> <ul> <li>We are solving a linear program instead of an integer program.</li> <li>From an auction design perspective, if we can get closer to the optimal allocations, then a lot of our auctions considerations become simpler : maybe we can run VCG, and no longer need Myerson-like auctions.</li> </ul> <p>LLM-Generated Ads Recap</p> <ul> <li>LLMs unlock new opportunities for flexible ads (generated on the fly).</li> <li>How to accomplish that is an interesting problem: challenges around alignment, optimization, incentives remain.</li> </ul> <p>Some research papers on this topic:</p> <ul> <li>Dubey et al' 24</li> <li>Duetting et al' 23</li> <li>Hajiaghayi et al' 24</li> <li>Mordo et al' 24</li> </ul>"},{"location":"guest-lecture-kshipra-bhawalkar/#recap","title":"Recap","text":"<p>Incentives in Sponsored Search Auctions Recap</p> <p>Internet Advertising brings together:</p> <ul> <li>Economics (Vickrey Auction, VCG, Myerson Lemma).</li> <li>Computer Science (Approximation algorithms, Approximate Auction, Price of Anarchy)</li> </ul> <p>The evolving Internet creates many opportunities/challenges for Computer Science, Economics, and Mathematics.</p> <p>Question: We talked a little bit about how we can incorporate ads into our lives. For people who are using Claude or ChatGPT via the chat interface, how do you think that might be? How do you think that might change?</p> <p>Right now you have subscription tiers where you get more access. So, subscription is definitely one model. It might be that that could be sufficient.However, for folks who don't want to invest that money or commit to paying, ads have a nice property that allows them to pay with their attention  to get access to those results. If the AI chatbots that exist right now and are not showing any ads want to reach all the people in the world, then it is conceivable that providing an ad-supported tier might be the way to go at some point.</p>"},{"location":"lecture-1/","title":"One-Sided Matching and Serial Dictatorship","text":"<p>March 31, 2025</p> <p>The Stanford Undergrad Housing Problem:</p> <ul> <li>There are \\( n \\) students, each with some preference over dorms.</li> <li>There are \\( m \\) dorms, each with some capacity (for simplicity, assume capacity is 1 for every dorm).</li> </ul> <p>How can we assign students to dorms?</p>"},{"location":"lecture-1/#max-weight-matching","title":"Max Weight Matching","text":"<p>One possible solution is to maximize total happiness:</p> <p>Mechanism: Happiness Maximization</p> <ol> <li>Create a bipartite graph where:<ul> <li>One side represents students.</li> <li>The other side represents dorms.</li> <li>Each edge from a student to a dorm represents how much that student likes that dorm.</li> </ul> </li> <li>Determine the max-weight matching (i.e., Hungarian Algorithm).</li> </ol> <p>Key question: How does the algorithm determine how much each student likes each residency? Only the students themselves know how much they like each dorm, so we need to rely on students to tell the algorithm.</p> <p>Challenge: Each student has an incentive to exaggerate how much they like their favorite dorm and undercut how much they like other dorms (using a \"0\" or \"negative infinity\" weight for those dorms). In other words: max-weight bipartite matching allows students to game the system.</p> <p>This occurs because finding the matching that maximizes total happiness is conceptually right, but this naive max-weight matching fails to take incentives into account. Another possible algorithm is Serial Dictatorship.</p>"},{"location":"lecture-1/#serial-dictatorship","title":"Serial Dictatorship","text":"<p>This is how Stanford actually assigns dorms to students.</p> <p>Mechanism: Serial Dictatorship</p> <ol> <li>Sort students in some fixed order (random, seniority, alphabetically, etc.).</li> <li>Go through the list in that order and allow each student (the \"dictator\") to select their most preferred available dorm.</li> </ol> <p>How is this better? There can still be students unhappy with their result. Indeed, a common complaint about Serial Dictatorship is unfairness: the first student chooses whichever dorm they want, while the last student gets only the last pick.</p> <p>When sorting is random, Serial Dictatorship guarantees ex-ante fairness: before the random sorting, all students have equal chances. However, after the sorting happens (even randomly), Serial Dictatorship loses fairness: there's no guarantee of ex-post fairness.</p> <p>Definition: Mechanism</p> <p>A mechanism consists of three things:</p> <ol> <li>A method of collecting inputs from agents,</li> <li>An algorithm that acts on the inputs,</li> <li>An action taken based on the output of the algorithm.</li> </ol> <p>In our context (Stanford Undergrad Housing Problem), based on inputs (dorm preferences) and the algorithm (available dorm after each student's pick), the mechanism takes actions (it assigns dorms).</p> <p>Note: All three components matter because there's a feedback loop: how we use inputs impacts what students report as preferences. The naive bipartite matching approach encourages students to game the system by misreporting their preferences.</p> <p>Thus, when designing a mechanism, we must consider:</p> <ul> <li>The algorithm itself,</li> <li>How/where inputs come from,</li> <li>Actions taken based on inputs,</li> <li>How promised actions affect reported inputs.</li> </ul> <p>Definition: Strategyproofness/Truthfulness</p> <p>A mechanism is strategyproof if it's in every agent's best interest to act truthfully, i.e., to report true preferences.</p> <p>Even more formally, game theorists somtimes use the term Dominant Strategy Incentive Compatible:</p> <p>Definition: Dominant Strategy Incentive Compatible (DSIC)</p> <p>A mechanism is dominant strategy incentive compatible if truthfulness is a dominant strategy for each participant. That is, being truthful is the best response regardless of other players' actions.</p> <p>Theorem: You Cannot Game Serial Dictatorship</p> <p>It is in every student's best interest to choose their favorite available dorm in their turn. Formally, we say that Serial Dictatorship is strategyproof or truthful.</p> <p>Proof:</p> <ul> <li>Your room choice doesn't affect room availability before your turn.</li> <li>When your turn arrives, your best action is choosing your favorite available room.</li> </ul> <p>Why does this prove that Serial Dictatorship is strategyproof? Until your turn comes, it is other students who are bidding, and there is nothing you can do to affect it. However, when comes your turn, you choose what you get, so you should choose the best thing for you.</p> <p>Definition: Pareto Optimality</p> <p>An assignment \\( A \\) is Pareto optimal if, for any other assignment \\( B \\), there's at least one participant strictly preferring \\( A \\) over \\( B \\).</p> <p>Theorem: You cannot make everyone happier without making someone sadder. Serial Dictatorship assignments are Pareto Optimal.</p> <p>Proof:</p> <p>Assume, for the sake of contradiction, that a different assignment exists making everyone as happy or happier.</p> <p>Consider the first student assigned differently: all better dorms are already assigned identically to students before them.</p> <p>Thus, the first differing student gets a worse dorm and becomes less happy\u2014we have reached a contradiction.</p> <p>Therefore, Serial Dictatorship is Pareto Optimal.</p>"},{"location":"lecture-1/#additional-discussion","title":"Additional Discussion","text":"<p>Why is truthfulness good? It prevents corruption and ensures fairness by removing any insider advantage.</p> <p>When is truthfulness important? When optimizing happiness, truthful inputs ensure correct objectives.</p> <p>When is relaxing truthfulness acceptable?  In contexts with social norms where truthfulness isn't expected (e.g., poker games), strategyproofness isn't crucial.</p> <p>Fairness/equity issues with Serial Dictatorship: Fairness depends on sorting order. Seniority or randomness provide fairness ex-ante but not necessarily ex-post.</p> <p>Is there any other flaws in Serial Dictatorship? A dictator's seemingly insignificant decision may have a huge impact on other agents. For instance, by the time my turn arrive, assume that only my 9th and 10th choices are available. Since I am almost indifferent between them, I break ties and take my 9th choice. However, it is possible that my 9th choice was in fact someone else's top choice and they really wanted it. This may be even worse if their 2nd-10th choices happen to be already taken.</p>"},{"location":"lecture-1/#recap","title":"Recap","text":"<ul> <li>Definition: Mechanism. A mechanism means soliciting inputs, running an algorithm, and taking actions.</li> <li>Definition: Strategyproof/truthful. A mechanism is strategyproof/truthful is misreporting preferences can never make a participant better off.</li> <li>Defintion: Pareto-optimal. An assignment \\( A \\) is Pareto-optimal if for any other assignment \\( B \\), there is a participant that (strictly) prefers \\( A \\) over \\( B \\).</li> </ul>"},{"location":"lecture-10/","title":"Scoring Rules","text":"<p>May 7, 2025</p> <p>Say that weather.com predicts a 40% chance of rain:</p> <ul> <li>If it rains, will we say that the forecast was wrong?</li> <li>What if it doesn't rain?</li> <li>Is weather.com more accurate than other weather websites?</li> </ul> <p>In other words: how can we evaluate this forecast of 40% chance of rain?</p>"},{"location":"lecture-10/#scoring-rules","title":"Scoring Rules","text":"<p>Definition: Scoring Rule</p> <p>For a given input, including:</p> <ul> <li>A realized event \\(i\\) in the outcome space \\(O\\), i.e. \"rain/no rain.\"</li> <li>A probability distribution \\(q\\) over \\(O\\), i.e. \"\\(40\\%/60\\%\\).\"</li> </ul> <p>A scoring outputs a score \\(S(q,i)\\), i.e. how much we are paying weather.com.</p> <p>In other words, a scoring rule takes the function of a probabilistic forecast and quantifies how good it was.</p> <p>Example: Linear Scoring Rule</p> <p>Imagine that we pay Weather.com based on the outcome vs. the forecast:</p> <ul> <li>If it rains, we are going to pay them how much probability they put on \u201crain.\u201d</li> <li>If it doesn\u2019t rain, we are going to them how much probability they put on \u201cdoesn\u2019t rain.\u201d</li> </ul> <p>Let \\(q_i\\) be the probability that \\(q\\) assigned to event \\(i\\).</p> <p>Intuitively, if Weather.com gives a higher probability to \u201crain\u201d and it actually \u201crains\u201d, the score should be higher.</p> <p>Example TBC.</p> <p>What does it mean to get it right when Weather.com forecasts 40% \u201crain\u201d and it rains? It would be helpful if there was a definitive answer, like \u201cIt is going to rain tomorrow.\u201d However, the forecast is not perfect, and it is helpful that Weather.com says there is a 40% chance of rain, to determine whether we should take an umbrella or not.</p> <p>Eric also showed an example on Monday where an LLM makes up stuff they don\u2019t know to seem like they have confidence in something even though they have no idea.</p> <p>So, we want to encourage the forecaster to honestly report the probability of the event.</p> <p>Example: Linear Scoring Rule (Cont'd)</p> <p>If the model predict a 40% chance of rain, how can we maximize the (expected) score?</p> <p>The score is calculated as the probability forecast that it rains times the payment if it rains, plus the probability forecast that it doesn\u2019t rain times the payment if it doesn\u2019t rain:</p> <ul> <li>If the forecast is 40% chance of rain, the expected score is \\(0.4 \\cdot 0.4 + 0.6 \\cdot 0.6 = 0.52\\).</li> <li>However, if the forecast is 0% chance of rain, then the expected score becomes \\(0 \\cdot 0.4 + 1 \\cdot 0.6 = 0.6\\).</li> </ul> <p>In other words, the forecaster has an incentive to not report the probability truthfully, and deviate to the optimal forecast that says there is 0% chance of rain (since the expected score is greater).</p> <p>With a linear scoring rule, the model is incentivized to tell us 1 or 0, even when it has a lot of uncertainty (it really doesn\u2019t know in the case of 40%/60%).</p> <p>For instance, here, since 40% is less than 50%, so the model will say that it will not rain (probability 0%). So, this is not a proper scoring rule.</p> <p>Lemma: With \\(S_{linear}\\), extreme reports (0% or 100%) are always optimal.</p> <p>Again, this is like the problem Eric described with LLMs, where the after-training is done by humans who give thumbs up and thumbs down, which encourages overconfidence.</p> <p>Definition: Proper Scoring Rule</p> <p>A scoring rule \\(S\\) is proper if it is strategyproof, i.e. the true distribution maximizes the expected score.</p> <p>What we want, in the context of this lecture, is proper scoring rules, i.e. scorings that are strategy proof, where truthful reporting of the probability forecast that you learn is the best response.</p> <p>Definition: Strictly Proper Scoring Rule</p> <p>A scoring rule \\(S\\) is proper if it is strictly strategyproof, i.e. the true distribution is the unique argmax of the expected score.</p> <p>What is the difference between a proper scoring rule and a strictly proper scoring rule?</p> <ul> <li>Proper scoring rule: If everything you do gives you a score of 0, then reporting truthfully gives you a score of 0, but it is not the only strategy that gives a score of 0, so this is a proper scoring rule, but not a strictly proper scoring rule.</li> <li>Strictly proper scoring rule: There is a unique best response.</li> </ul> <p>Example: Quadratic Scoring Rule</p> <p>Note: The quadractic scoring rule is also called the Brier scoring rule.</p> <p>Consider \\(S_{quadratic} = q_i - \\frac{1}{2} \\sum q_j^2\\).</p> <p>Intuition: \\(S_{quadratic}\\) is the same as the \\(S_{linear}\\) with a \"regularizer\" term to penalize extreme forecasts.</p> <p>This encourages the forecaster to assign higher probability to more likely events, while penalizing extreme forecasts (the regularizer is the sum of all probabilities, squared).</p> <p>For instance:</p> <ul> <li>If we have an extreme report of \\(1\\), the sum is going to be \\(1\\), so we are going to subtract \\(\\frac{1}{2}\\).</li> <li>However, if we have equal reports of \\(50/50\\), then the sum is going to \\(0.25 + 0.25 = 0.5\\), and we are subtracting half of that, which is \\(0.25\\).</li> </ul> <p>In other words, we are subtracting less when the forecast is more even.</p> <p>Example: Quadratic Scoring Rule (Cont'd)</p> <p>Back to our weather forecast example:</p> <ul> <li>If the forecast is 40% chance of rain, the expected score is \\(0.4^2 + 0.6^2 - \\frac{1}{2}(0.4^2 + 0.6^2) = 0.26\\).</li> <li>However, if the forecast is 0% chance of rain, then the expected score becomes \\(0.6 - \\frac{1}{2}(0^2 + 1^2) = 0.1\\).</li> </ul> <p>Here, we see that we get a better expected score if we report the true forecast rather than the extreme forecast.</p> <p>Lemma: The quadratic scoring rule is strictly proper.</p> <p>Proof:</p> <p>Say \\(p\\) is the true probability distribution (i.e. the forecaster's belief).</p> <p>The expected score \\(E_{i \\approx p} [S(i,q)] = \\sum p_j q_j - \\frac{1}{2} \\sum q_j^2\\) is a strictly concave function in \\(q\\).</p> <p>Therefore, this function has a unique maximizer, which we can find by setting the derivative with respect to \\(q_j\\) to 0:</p> <p>\\(0 = \\frac{\\partial}{\\partial{_{q_j}}} E_{i \\approx p} [S(i,q)] = p_j - q_j\\).</p> <p>We see that \\(p_j - q_j = 0\\), which means that \\(p_j = q_j\\).</p> <p>In other words, the optimal response to the quadratic scoring rule is indeed to predict the true probabilities.</p> <p>Example: Logarithmic Scoring Rule</p> <p>Consider \\(S_{log}(q,i) = log (q_i)\\).</p> <p>Note: The logarithmic scoring rule is also called the Good scoring rule, after I. J. Good, a British statistician, logician, and computer scientist. It is indeed a good scoring rule, but that is not the reason why it is called the Good scoring rule.</p> <p>Intuition: The log function is an increasing function, so if we bet higher on the realized outcome (with a higher q_i), then we get a higher score.</p> <p>However, this still disincentives overconfident reports, because in the extreme case, if we say that an event never happens (it has a probability of 0), and it does happen, then we get a score of negative infinity.</p> <p>Example: Logarithmic Scoring Rule (Cont'd)</p> <p>Back to our weather forecast example:</p> <ul> <li>If the forecast is 40% chance of rain, the expected score is \\(0.4 \\cdot log(0.4) + 0.6 \\cdot log(0.6) = -0.29\\).</li> <li>However, if the forecast is 0% chance of rain, then the expected score becomes \\(0.4 \\cdot log(0) = - \\infty \\).</li> </ul> <p>Here, we see that with a true report, the expected score here is \\(-0.29\\). If we report 0% when there is still a 40% chance of rain, then the expected score is negative infinity, which is a very bad prediction to make.</p> <p>Lemma: The logarithmic scoring rule is proper.</p> <p>Proof:</p> <p>Say \\(p\\) is the true probability distribution (i.e. the forecaster's belief).</p> <p>Then, here again, we differentiate the expected score function with respect to \\(q_j\\) and set it to 0:</p> <p>\\(0 = \\frac{\\partial}{\\partial{_{q_j}}} E_{i \\approx p} [S(i,q)] = \\frac{\\partial}{\\partial{_{q_j}}} [p_j \\cdot log(q_j)] = \\frac{p_j}{q_j}\\).</p> <p>Oops! \\(\\frac{p_j}{q_j}\\) is never equal to \\(0\\).</p> <p>Example: Normalized Logarithmic Scoring Rule</p> <p>Just for the purposes of the proof, let's consider  \\(S_{log}(q,i) = log (q_i) - \\sum q_j\\).</p> <p>Claim 1: The normalize logarithmic scoring rule is proper.</p> <p>Proof:</p> <p>Again, say \\(p\\) is the true probability distribution (i.e. the forecaster's belief).</p> <p>Then, we differentiate the expected score function with respect to \\(q_j\\) and set it to 0:</p> <p>\\(0 = \\frac{\\partial}{\\partial{_{q_j}}} E_{i \\approx p} [S(i,q)] = \\frac{\\partial}{\\partial{_{q_j}}} [p_j \\cdot log(q_j) - q_j] = \\frac{p_j}{q_j} - 1\\).</p> <p>Note: \\(\\sum q_j = 1\\) because \\(\\sum q_j\\) is the sum of all probabilities, which is always equal to \\(1\\).</p> <p>This time, we see that the partial derivative is zero exactly for \\(p_j = q_j\\).</p> <p>Claim 2: If \\(S\\) is a proper scoring rule, then \\(S' = S + c\\) for any constant \\(c\\) is also proper.</p> <p>Proof:</p> <p>We add the same constant \\(c\\) to the score of any \\(q\\), so the optimal \\(q\\) doesn't change.</p> <p>Proof (Cont'd):</p> <p>By Claim 1 and Claim 2, the logarithmic scoring rule is proper.</p> <p>Scoring Rules Recap</p> <ul> <li>The idea of a scoring rule is to look at the probability distribution of a given forecast, as well as the actual outcome, and we want to give some score about whether the forecast was good or not, and this is the way to quantify how good the forecast was.</li> <li>When we quantify how good the forecast was, we want to think about ways to quantify it that make truthful reporting strategy proof, i.e. we want to incentivize truthful reporting, i.e. we want (ideally strictly) proper scoring rules.</li> <li>The linear scoring rule is not proper, while the quadratic and logarithmic scoring rules are.</li> </ul> <p>Although this is not our main focus in this lecture, proper scoring rules are super important for training language models, in particular because language models predict distributions. On that note, here is a slide from last year's lecture on AI alignment:</p> <p></p>"},{"location":"lecture-10/#market-scoring-rules","title":"Market Scoring Rules","text":"<p>So far, we have seen two extreme alternatives:</p> <ul> <li>Scoring rules, on the one hand, apply with a single forecaster, are strategyproof, and usually require to pay the forecaster.</li> <li>Prediction markets, on the other hand, apply when we want to benefit from the wisdom of the crowd (they actually completely fail with a single forecaster), are subject to the no-trade theorem, and tend to operate on zero-sum trades of contract bundles (some prediction markets actually make a profit from fees).</li> </ul> <p>Now, we want to cosnider something in the middle, in the hope of getting the best of both worlds: market scoring rules.</p> <p>Definition: Market Scoring Rules</p> <ul> <li>Fix a strictly proper scoring rule \\(S\\).</li> <li>Initialize \\(q^O\\) as some probability distribution over the outcome space \\(O\\). For instance, \\(q^O\\) may be the uniform distribution (in contrast, if you can look at the history, you can initialize with some prior distribution based on history).</li> <li>At each time step \\(t\\), anyone can update the distribution for \\(q^{t-1}\\) to \\(q^t\\). For instance, a forecaster may be new to the market or have learnt new information.</li> <li>At the end of the market, some outcome \\(i \\in O\\) is realized: we pay \\(S(q^t,i)-S(q^{t-1},i)\\) to the forecaster who made the \\(t\\)-th update.</li> </ul> <ul> <li>Let \\(S_{log}(q,i) = log(q_i)\\).</li> <li>Initialize \\(q^O\\) as the probability that it will rain with a chance of 50%.</li> <li>At each time step \\(t\\), anyone can update the distribution for \\(q^{t-1}\\) to \\(q^t\\). For instance, a forecaster may be new to the market or have learnt new information.</li> <li>Once outcome \\(i \\in O\\) is realized: we pay \\(S(q^t,i)-S(q^{t-1},i)\\) to the forecaster who made the \\(t\\)-th update.</li> </ul> <p>Here is what happens:</p> <ul> <li>Before Alice arrives, the (initial) probability of rain is \\(0.5\\).</li> <li>Alice says that it is already May, so she thinks that the probability of rain is in fact only \\(0.25\\).</li> <li>Then, Bob comes, and he saw what Alice said, but there are many clouds in the sky, so he moves the probability to \\(0.75\\).</li> <li>Finally, Carlos arrives, and he is even more confident that it is going to rain, so he moves the probability to \\(0.9\\).</li> </ul> <p>How should we pay the forecaster?</p> <ul> <li>If it rains, Alice has to pay \\(1\\) for making a worse prediction, Bob receives \\(1.6\\) and Carlos gets \\(0.3\\).</li> <li>If it does not rain, then Alice receives \\(0.6\\), while Bob needs to pay \\(1.6\\) and Carlos needs to pay \\(1.3\\).</li> </ul> <p>Note: The order in which we go matters, timing is important, or if something happens and changes what everyone thinks, then the first person to move the market has an advantage.</p> <p>If there was no Bob, but just Alice moving \\(0.25\\) in one direction, and then Carlos moving to\\(0.9\\) in the other direction, then the payments for Carlos are just the sum of what the payments for Bob and Carlos were above.</p> <p>Each person can shift the market a lot, but they are exposing themselves to losing a lot of money (the bigger the shift, the bigger the risk and the reward).</p> <p>Quesiton: What happens to incentives if other forecasters change their minds based on what you did?</p> <p>Here, we are going to analyze incentives assuming that each forecaster makes a prediction, and their turn is done. However, in practice, people practice what is called the \u201cpump and dump\u201d trick, where they buy a lot of stocks, make sure everyone knows about it, which increases the price when everyone jumps on the wagon, and then the person who is doing the \u201cpump and dump\u201d sells the stocks for a profit. We do not have a good mathematical model for how forecasters react to how one forecaster updates the forecast.</p> <p>Definition: Sybil-Attack</p> <p>Exploiting a mechanism by pretending to participate as many agents.</p> <p>Definition: Sybil-Proof</p> <p>Robust to Sybil-attacks.</p> <p>Claim: Market scoring rules are sybil-proof-ish.</p> <p>Specifically, market scoring rules are robust to sybil-attacks as long as all copies forecast immediately after each other.</p> <p>An agent may run a Sybil-attack by making copies of themselves (with bots) to pretend they are more than one person.</p> <p>The reason why market scoring rules are sybil-proof-ish is because, if one agent is taking one order, and breaking it down into a series of consecutive orders, then it is exactly the same as if it still was exactly one order.</p> <p>More generally, forecasters will receive the same payoff for moving the market belief in one shot vs. consecutive increments.</p> <p>The market scoring rule is really a market that is using a scoring rule (so maybe it should have been called the scoring rule market), rather than a rule for scoring the market.</p> <p>Question: Is there a rule for scoring the market?</p> <p>Yes, but one problem is that those rules tend to be really not sybil-proof. For instance, in blockchain, there are a lot of websites that keep track of activity that happen in some blockchains, and so the blockchain operators figured it out, and they started generating a lot of fake activity to appear like up-and-coming blockchains.</p> <p>Question: Can each forecaster can observe the predictions of the forecasters before them?</p> <p>Yes, and that is very important, because they are paying the difference between the forecast and what they are updating the forecast to.</p> <p>Question: Should we be getting closer to the truth as more forecasters join?</p> <p>The big questions is how do we aggregate the fact that some forecasts were made all over the place, in no specific order. So, in theory, all together, the forecasters should be getting closer to the truth, but there is not a good model to aggregate what happens.</p> <p>There is a public no-trade theorem, because if all the public information has been aggregated, then the forecast is already reflecting that, but if a forecaster has private information, then they are trading with the house, who is \u201cpaying them\u201d to bring in the new information.</p> <p>Definition: Bounded Market Maker's Loss</p> <p>The total payout to forecasters is:</p> \\[   \\underset{t=1}{\\sum^T} S(q^t,i) - S(q^{t-1},i) = S(q^T,i) - S(q^0,i) \\] <p>You may have to pay forecasters (unlike prediction markets):</p> <ul> <li>Paying is reasonable in exchange for useful information</li> <li>Positive payment (in expectation) is necessary to avoid no-trade theorems</li> <li>You actually make a profit if your original forecast was better</li> </ul> <p>The total payout is bounded\u2013it doesn\u2019t scale with number of forecasters. For instance, it is at most \\(log(O)\\) when using the logarithmic scoring rule with \\(q^O\\) is the uniform distribution.</p> <p>Said otherwise, the total payout is bounded by the difference between the score of the original forecast and the final forecast. Regardless of the number of forecaster, the total payout is bounded, because the most it can go to is someone exactly predicting the right outcome. So, even with an unbounded number of forecasters, the total payout remains bounded.</p> <p>Lemma: For any fixed order of forecasters, reporting true beliefs is the unique optimal strategy for every forecaster.</p> <p>Proof: The \\(t\\)-th forecaster chooses \\(q^t\\) that maximizes \\(S(q^t,i)-S(q^{t-1},i)\\).</p> <p>Since \\(S(q^{t-1},i)\\) does not depend on \\(q\\), the optimality of the true forecast follows from the strategyproofness of the scoring rule.</p> <p>In other words, since every forecaster has no control over the score of the pervious forecast(s), they just want to maximize the score of the new forecast, which means just maximizing their own profits, and this is strategyproof.</p> <p>The dynamic game is harder to analyze/model, as forecasts may affect other forecasters\u2019 beliefs (think info cascades)=</p> <ul> <li>In theory, your forecast now can strategically throw off the market.</li> <li>The \u201cmore wrong\u201d the market is, the more opportunity to profit.</li> <li>But this requires a reliable model of how your forecast affects others.</li> </ul> <p>Market Scoring Rules Recap</p> <ul> <li>Market scoring rules apply to any number of forecasters. This is useful for markets of moderate size, which is common in corporate forecasts.</li> <li>Marker scoring rules are also strategyproof with a fixed order, but their dynamic strategic manipulation is hard to implement.</li> <li>The payout is bounded and does not depend on the number of forecasters.</li> </ul>"},{"location":"lecture-10/#automated-prediction-market-makers","title":"Automated Prediction Market Makers","text":"<p>In simple prediction markets like IEM, one problem is that liquidity providers risk unfavorable trades with more informed traders:</p> <ul> <li>Large bid-ask spread to cover risk (low liquidity) </li> <li>Poor information aggregation, e.g. anywhere between 5%-40% (in the example of LeBron James potentially joining the Chicago Bulls).</li> </ul> <p>One solution to this problem is called automated market makers (AMM).</p> <p>Definition: Automated Market Makers (AMM)</p> <ul> <li>The market maker ( \u201cbookie\u201d) posts zero (or low) spread buy/sell prices.</li> <li>Forecasters can always trade with the market maker.</li> </ul> <p>In a prediction market with AMM:</p> <ul> <li>There are contracts, like in IEM, for instance:<ul> <li>D-contract that pays $1 if Democratic candidate wins</li> <li>R-contract that pays $1 if Republican candidate wins</li> </ul> </li> <li>The AMM (\u201chouse\u201d) offers: buy/sell D-contract for price \\(P_D\\) (respectively R-contract for price \\(P_R\\)).</li> </ul> <p>How should the AMM set prices? Using market scoring rules! Actually, this is one answer, and we will discuss more answers later in the quarter.</p> <p>Note: AMM knows nothing about the actual event we are trying to predict!</p> <p>In a market scoring rule-based automated market maker (MSR-AMM), buy/sell prices should be dynamic to ensure bounded total payout. As more forecasters buy D-contracts, market \u201cexpects\u201d the Democratic candidate to win, which only incentivizes next D-contract purchase if a forecaster is even more confident than market.</p> <p>The concrete goals of an MSR-AMM are to:</p> <ul> <li>Look and feel of a prediction market, where:<ul> <li>Forecasters trade D-contracts and R-contracts (although now forecasters can also trade with the market maker).</li> <li>After elections: each D-contract pays $1 if the Democratic candidate wins (ditto for R-contracts).</li> </ul> </li> <li>Make payments (contract cost + payout) exactly as in market scoring rule (forecasters incentives are also exactly the same).</li> </ul> <p>Let's consider an MSR-AMM with two outcomes (\\(D\\) and \\(R\\)) and an initial market belief \\(q^O = 50/50\\).</p> <p>Bundles of (D-contract + R-contract) are worth exactly \\(\\$1\\). Buying/selling a bunch of such bundles doesn\u2019t indicate change in market belief, since the belief only depends on the difference between the number of D-contracts and R-contracts sold.</p> <p>At the beginning of market, given that \\(q^O = \\frac{1}{2}\\), prices of D-contracts and R-contracts are equal (50\u00a2 each).</p> <p>As the difference between the number of D-contracts and the number of R-contracts approaches \\(\\infty\\), the market belief approaches 100% probability that the Democratic candidate wins. So, the price of D-contracts should approach \\(\\$1\\), while the price of R-contract should approach \\(\\$0\\).</p> <p>The market belief is derived from prices: If prices \\(P_D + P_R = 1\\), are an equilibrium (at time \\(t\\)) for the market, then the market\u2019s current belief is that the Republican candidate will win with probability \\(q_R^t = P_R\\). Otherwise, forecasters want to buy more/less R-contracts.</p> <p>Now, fix some (strictly proper) scoring rule \\(S\\), and consider that no contracts are sold yet. If buying \\(x_D\\) D-contracts moves the market belief from \\(q^O\\) to \\(q^t\\), their their cost (i.e. the cumulative price forecasters pay the market maker) should satisfy:</p> \\[ C(x_D,O) - x_D = S(q^t,i_D) - S(q^O,i_D) \\] <p>where:</p> <ul> <li>\\(x_D\\) is the contract payout if the Democratic candidate wins, and</li> <li>\\(S(q^t,i_D) - S(q^O,i_D)\\) is the market scoring rule payment if the Democratic candidate wins.</li> </ul> <p>If buying \\(x_D\\) D-contracts moves the market belief from \\(q^O\\) to \\(q^t\\), their their cost should also satisfy:</p> \\[ C(x_D,O) = S(q^t,i_R) - S(q^O,i_R) \\] <p>This means that the price from cost of 1 R-contract is:</p> \\[ C(x_D,x_R +1)-C(x_D,x_R) \\approx \\frac{\\partial C(x_D,x_R)}{\\partial x_R}  \\] <p>In other words, the costs should statisfy:</p> \\[ C(x_D,O) - x_D = S(q^t,i_D) - S(q^O,i_D) \\] <p>and </p> \\[ C(x_R,O) - x_R = S(q^t,i_R) - S(q^O,i_R) \\] <p>With a logarithmic rule AMM, we have:</p> \\[ C(x_D,x_R)=ln(e^{x_D}+e^{x_R})-ln(2) \\] <p>and the price of a marginal D-contract is:</p> \\[ q_D(x_D,x_R) = \\frac{e^{x_D}}{e^{x_D}+e^{x_R}} \\] <p>With this formula, if we do the right math, it looks and feel like a prediction market, but it has the same properties (strategy-proof-ishness and sybil-proof-ishness) and the same expected payout as a market scoring rule (the payout is bonded: we may lose money to trader, but not too much money).</p> <p>Important Takeaway: With an AMM, traders always have someone to trade with, and one way to determine the prices is using a market scoring rule.</p>"},{"location":"lecture-10/#recap","title":"Recap","text":"<p>Scoring Rules, Market Scoring Rules, and Automated Prediction Market Makers Recap</p> <ul> <li> <p>One caveat with prediction markets is that they may have low liquidity, due to:</p> <ul> <li>No-trade theorems.</li> <li>Questions that require special knowledge/classified information (e.g., in corporates or government agencies).</li> <li>A Catch-22, where poor liquidity leads to a thin market, which leads to even worse liquidity, etc.</li> </ul> </li> <li> <p>In the extreme case of a single forecaster, proper scoring rules provide the incentives. More generally, we can have an automated market maker based on a proper scoring rule.</p> </li> <li> <p>For a given input, including a realized event \\(i\\) in the outcome space \\(O\\), i.e. \"rain/no rain\", and a probability distribution \\(q\\) over \\(O\\), i.e. \"\\(40\\%/60\\%\\)\", a scoring outputs a score \\(S(q,i)\\), i.e. how much we are paying weather.com.</p> </li> <li> <p>A scoring rule \\(S\\) is proper if it is strictly strategyproof, i.e. the true distribution is the unique argmax of the expected score.</p> </li> <li> <p>The quadratic scoring rule \\(S_{quadratic} = q_i - \\frac{1}{2} \\sum q_j^2\\), and the logarithmic scoring rule \\(S_{log}(q,i) = log (q_i)\\), are strictly proper.</p> </li> <li> <p>The market scoring rule payment to each forecaster is \\(S(q^t,i) - S(q^{t-1}\\).</p> </li> <li> <p>In a logarithmic rule AMM, the price of a marginal \\(i\\)-contract is \\(\\frac{e^{x_i}}{\\sum e^{x_j}}\\).</p> </li> </ul>"},{"location":"lecture-11/","title":"Proof-of-Work Blockchains","text":"<p>May 14, 2025</p> <p>Note: The goal of this lecture is not to offer investment advice.</p>"},{"location":"lecture-11/#cryptographic-preliminaries","title":"Cryptographic Preliminaries","text":"<p>Today, we will need some very minimal crytography concepts (primitives).</p> <p>Definition: Digital Signatures \\((public \\_ key, secret \\_ key)\\)</p> <p>To show that I agree with statement \\(x\\), I can \\(y \\leftarrow sign(x, secret \\_ key)\\). Anyone who has my \\(public \\_ key\\) can \\(verify(x, y, public \\_ key)\\) to make sure it was really me who signed \\(x\\).</p> <p>(Formally: Whoever signed \\(x\\) had \\(secret \\_ key\\)).</p> <p>Definition: Cryptographic Hash Function \\(H\\)</p> <ul> <li>Given \\(a\\), it is easy to compute \\(H(a) = b\\).</li> <li>Given \\(b\\), the fastest way to find \\(a\\) is by trying every/random inputs.<ul> <li>Even if you know some bits of \\(a\\), the only way to find the missing bits is by trying every combination.</li> <li>If you want something that agrees with \\(b\\) on the first \\(k\\) bits, you need \\(2k\\) tries in expectation. (Aka random is still the fastest way.)</li> </ul> </li> </ul>"},{"location":"lecture-11/#decentralized-permissionless-crypto-money","title":"Decentralized, Permissionless Crypto-Money","text":"<p>Per the lecture slides: \"This is intended to be a slow introduction for those of you who have never seen blockchains.\"</p> <p>Example: Alice pays Bob \\(\\$1\\).</p> <p>With a wire transfer from her bank account:</p> <ul> <li>Alice\u2019s bank verifies that she has \\(\\$1\\) in her account.</li> <li>After the transfer, the bank \\(-\\$1\\) off her account.</li> </ul> <p>Do we really need a bank for that? In particular, without a bank, how can we be sure that Alice ever had \\(\\$1\\)?</p> <ul> <li> <p>Alice can tell Bob the entire history of her \\(\\$1\\):</p> <p>\u201c\\(\\$1\\) was minted, then paid to \\(02847\\), who paid it to \\(42975\\), who paid me\u201d</p> </li> <li> <p>Alice can use cryptographic proofs (digital signatures) to show that this story is true.</p> </li> </ul> <p>How can we be sure that Alice hasn\u2019t already spent this \\(\\$1\\)?</p> <p>Key point: It is much easier to prove that something happened than didn\u2019t happen!</p> <p>So, the way to solve this is that, when Alice spends money, she tells the whole world about it. Next time she spends money, we can check the entire history of payments, and make sure Alice didn\u2019t already spend this money.</p> <p>This brings us to the concept of a ledger.</p> <p>Definition: Ledger</p> <p>An ordered history of transactions.</p> <p>Why do we need a ledger? So that when Alice pays Bob \\(\\$1\\), we can verify that she hasn\u2019t already paid the same \\(\\$1\\) to someone else.</p> <p>Desiderata: Decentralized Ledger</p> <p>The ideal ledger meets the following requirements:</p> <ul> <li>Decentralized: Not controlled by any single entity (it continues to work even if a bank is hacked, no entity can halt trades, e.g., when Citadel asked Robin Hood to halt trades of GameStop stocks).</li> <li>Consensus: Either everyone believes that Alice payd Bob, or everyone believes that she didn\u2019t.</li> <li>Permissionless: Anyone can join (or leave) at any time<ul> <li>This is a major source of technical difficulty (for instance due to adversarial participants trying to hack the system).</li> <li>What are the motivations? Extreme decentralization, and permissionless success stories (content on the Internet, i.e. Wikipedia, YouTube, FB).</li> </ul> </li> </ul> <p>In a permissionless system, a \u201cmajority of the users\u201d is meaningless, because I can always create more copies of myself (i.e. launch a Sybil attack). Some alternatives that are harder to copy in a digital world include:</p> <ul> <li>Proof-of-Work: \u201c1 CPU = 1 vote\u201d</li> <li>Proof-of-Stake: \u201c1 coin = 1 vote\u201d</li> <li>Proof-of-Space: \u201c1 bit (storage) = 1 vote\u201d</li> </ul> <p>Note: There is a lot of research going on into how to come up with a \"Proof-of-Humanity.\"</p>"},{"location":"lecture-11/#the-bitcoin-protocol","title":"The Bitcoin Protocol","text":"<p>Definition: Blocks</p> <p>The ledger is constructed as a chain of blocks (aka a blockchain!).</p> <p>Each block contains:</p> <ul> <li>Some (digitally-signed) payments/transactions,</li> <li>A (cryptographic) hash of the previous block,</li> <li>A nonce (for now, we consider this to be just a padding string\u2013more coming soon).</li> </ul> <p>Question: If the blockchain is decentralized, who owns the storage?</p> <p>Everyone who participates keeps a copy of the blockchain on their own machine, i.e. everyone who participates needs to have enough space to keep track of everything that ever happened in the history of the blockchain.</p> <p>Example: Blockchain</p> <p></p> <p>In order to verify the \\((Alice \\rightarrow Bob, \\$1)\\) payment in block \\(b_t\\):</p> <ul> <li>Verify that Alice signed the payment (using <code>Alice.public_key</code>),</li> <li>Verify that Alice received this $1 in some previous block,</li> <li>Verify that Alice hasn\u2019t spent it.</li> </ul> <p>In order to verify block \\(b_t\\):</p> <ul> <li>Verify all the transactions,</li> <li>Verify that it correctly hashes block \\(b_t - 1\\),</li> <li>Verify the nonce (see below).</li> </ul> <p>Definition: Proof-of-Work (PoW)</p> <p>A nonce is valid if the hash of the entire new block (including all payments, the hash of previous block, and the nonce) has \\(k\\) leading zeros.</p> <p>The probability to find to find a valid nonce is: \\(Pr[valid \\; nonce] = \\frac{Number \\; of \\; hashes \\; tried}{2^k}\\).</p> <p>Proof-of-Work: If you find a valid nonce, it proves that you tried a lot of hashes and got lucky.</p> <p>Miners compete to find the first valid nonce and create (mine) the new block. The winner gets a reward (encoded as a transaction in the new block). \\(k\\) is adjusted dynamically, so that it takes all the miners in the world about \\(10\\) minutes in expectation to find the next valid nonce.</p> <p>When Alice wants to pay Bob, she sends the signed transaction to all miners. Then Alice (and Bob) wait for a miner to include this transaction in a new block.</p> <p>Question: What does Proof-of-Work actually offer?</p> <p>You always want to create a new block because you get a reward (a lot of money) for creating that block. The point of PoW is to show that the miner who wins tried really hard to find the nonce. Why do we want them to try really hard? Because without the PoW, miners could just write a million new blocks.</p> <p>We want to have a sense of majority, that people agree that we have consensus. Pointing to a block means that we agree with the content of that block, and finding a valid nonce is a puzzle that slows you down.</p> <p>In the case of a digital world record, a \u201cmajority of people\u201d does not make sense (because people can duplicate themselves to have an outsize influence), so instead, we ask for a majority of compute power, i.e. a majority of hash rate. So, when a majority of compute belongs to people who agree with a block, they will point to something that extends that block.</p> <p>Question: If it takes 10 minutes to write a new block, and there are a million transactions that need to be verified in that time, how is there enough block to validate them?</p> <p>Bitcoin has a severe limitation of the rate of transactions it can process. So, will Bitcoin ever replace credit cards? No, because Visa can process so many millions of transactions per second, while Bitcoin can only have so many bytes per block to include transactions. Other protocols are working really hard to scale a process that can process a lot more transactions.</p> <p>Note:</p> <ul> <li>The mempool is essentially\u00a0a temporary storage area for unconfirmed transactions on the Bitcoin blockchain network. When a user initiates a cryptocurrency transaction, such as sending bitcoin to another user, the transaction is broadcast to the network and temporarily held in the mempool.</li> <li>To get into the mempool, you need to make sure that a transaction is digitally signed correctly. Maybe Alice wants to pay Bob and wants to pay Charlie, but she has only \\(\\$1\\), so each of those transactions individually is valid, and they can both go into the mempool, but which one is going to go into the block is something that the miner chooses.</li> </ul> <p>Definition: Fork</p> <p>Occasionally, two miners find valid nonces at (essentially) the same time. They create two different blocks, so we no longer have consensus. This is a fork.</p> <p></p> <p>Definition: Longest Chain Rule</p> <p></p> <p>Miners have a protocol for deciding which branch the next block should extend (hash), i.e. \\(b_t\\) or \\(\\hat b_t\\). Famously, they should try to extend the longest-chain available, i.e. above they should prioritize \\(b_{t+1}\\) or \\(\\hat b_t\\).</p> <p>Question: What happens if two miners find a valid nonce at the same time?</p> <p>Branches are called forks, they happen sometimes, potentially due to a network delay (so the two miners do not hear about each other), and they are a headache, because we want to be in consensus, with everyone agreeing.</p> <p>Question: How do you recover from a fork?</p> <p>By taking the longest chain, this is the longest chain rule, so we take the branch of the fork that has the longest chain.</p> <p>Question: What happens to all the transactions in the branch that is not chosen?</p> <p>It is like they never happened.</p> <p>Question: What if you pay someone, they give you a new Tesla, and then the block goes out?</p> <p>Usually, we wait some time, such as 6 blocks in Bitcoin (this is a rule of thumb), which is about an hour, since we create one block every 10 minutes, and then it is very unlikely that someone has a different view of the chain.</p> <p>Question: Why are people incentivized the extend the longest chain?</p> <p>Because if everyone else is extending the longest chain, they are more likely to agree with you. If you mine a block and it does not extend the longest chain, then you do not get the reward for the miner.</p> <p>Question: What if there are two equal length chains?</p> <p>The rule says that you should extend whichever one you heard of first. Probably there is a tie for one or two blocks, but after a while, there will be a block that everyone sees and wants to extend.</p> <p>Question: What if miners are just wrong and do not verify transactions correctly?</p> <p>Then, you won\u2019t extend their block, and they are going to lose a lot of money, so there is some kind of equilibrium where they want to run the same algorithm as everyone else.</p> <p>Attack Recap</p> <p>The blockchain is secure against the following attacks:</p> <ul> <li>Steal money attack: Nobody can create a payment from my account without the account\u2019s <code>secret_key</code>.</li> <li>Double-spend attack: Alice can\u2019t send the same \\(\\$1\\) to both Bob and Charlie because a transaction is added to the ledger only after it is verified with respect to entire history of transactions.</li> <li>Sybil attack: No one miner can take control by creating many accounts\u2013a majority of compute power is necessary.</li> </ul>"},{"location":"lecture-11/#issues-with-big-miners","title":"Issues With Big Miners","text":"<p>When Bitcoin started in the 2000\u2019s, everyone had a computer in their home, so each individual miner could represent a fraction of the total computer of the network. Hence the original utopic idea of increasingly distributing compute power to \"the people.\"</p> <p>Since then, it has been less and less of that, partially because a lot of computers have moved to the cloud, so it is still decentralized, but most miners are on AWS (if AWS goes down, the blockchain collapses).</p> <p>In addition, solving hash functions is hard, but we can create special hardware that is really energy efficient to reduce the electricity you need to try all the nonces. So, professional miners makes it less worth it to run on your own computer, because you would not cover your electricity bill.</p> <p>Some miners also own a power plant, or operate in a country where electricity is cheap.</p> <p>All of this makes blockchains less and less decentralized. But the really big thing that happened is something called mining pools.</p> <p>Definition: Mining Pool</p> <p>Professional miners organize together to reduce variance in gains from mining rewards (and also to influence crypto-politics).</p> <p>A mining pool is like an organization that pools a bunch of miners together. Since there is only a new block every 10 minutes, you can go years without mining a new block, spending all this money, so it is not a safe investment. Therefore, miners started to get together in pools, and some of those pools are very large.</p> <p>Examples: Mining Pools</p> <p>According to blockchain.info/pools:</p> <ul> <li>In 2021, the biggest four pools represented half of Bitcoin:</li> </ul> <p></p> <ul> <li>In 2024, the biggest two pools represented half of Bitcoin:</li> </ul> <p></p> <p>Example: 51%-Attack</p> <p>If Alice has over 51% of all mining resources (hashrate), she can pay Bob, write it in the blockchain, and then fork a new chain suffix that doesn\u2019t include that block.</p> <p></p> <p>More generally:</p> <p></p> <p>Example: Double-Spend Attack without 51% of Hashrate</p> <p>Imagine that Alice has \\(\\alpha &lt; 0.5\\) of the hashrate. She can still try to do the double-spend attack, and hope she gets lucky:</p> <p></p> <p>In practice, it is recommended that Bob waits \\(d &gt; 1\\) blocks before accepting a payment.</p> <p>The attack succeeds if Alice mines \\(d+1\\) blocks without the payment to Bob before anyone else can mine 1 block extending \\(d\\) blocks with the payment to Bob.</p> <p>Thus: \\(Pr\u2061[\"Attack \\; succeeds\" ] = \\alpha^{(\ud835\udc85+\ud835\udfcf)}\\)</p>"},{"location":"lecture-11/#big-miners-in-practice","title":"Big Miners in Practice","text":"<p>In 2014, the GHash.io mining pool exceeded 51% of the Bitcoin hashrate. Instead of executing a 51% attack, they encouraged miners to leave the pool. Why? In order to prevent (actually to stop) a drop in the value of Bitcoin.</p> <p>For smaller coins (with a lower total hashrate), this is a bigger issue. If there is a lower barrier to reach 51% of the hashrate, and attackers do not care about a drop in value of a small coin, they can divert a lot of mining power for a short time period. Allegedly, 51% attacks happened (multiple times) to \u201cEthereum Classic.\u201d</p>"},{"location":"lecture-11/#recap","title":"Recap","text":"<p>Blockchain:</p> <ul> <li>The ledger stores all transactions to prevent double-spending.</li> <li>It is implemented as a chain of blocks created by miners.</li> <li>Miners need Proof-of-Work to write blocks to avoid Sybil attacks.</li> <li>The ongest chain rule disambiguates inconsistent blocks (forks).</li> </ul> <p></p> <p>Big Miners</p> <ul> <li>The original, decentralized utopian vision contrasts with the current, concentrated power of mining pools.</li> <li>Causes for this change include specialized hardware, electricity costs, cloud computing, and reward variance.</li> <li>Mining pools are groups of miners organizing together to pool revenue.</li> <li>Attacks only require a large hashrate for some period of time.</li> </ul> <p>Attacks</p> <ul> <li>A 51%-attack allows double spending, and freezing accounts, but not stealing money.</li> <li>An \\(\\alpha\\)-fraction double-spend attack succeeds with probability (\\alpha^{k+1}.</li> </ul>"},{"location":"lecture-12/","title":"Proof-of-Stake Blockchains","text":"<p>May 21, 2025</p> <p>Note: Throughout this lecture, we talk about \u201cminers\u201d who create new blocks. In Ethereum\u2019s Proof-of-Stake, there are \u201cproposers\u201d and \u201cvalidators\u201d (there are also \u201cbuilders\u201d), and neither are called \u201cminers\u201d.</p>"},{"location":"lecture-12/#context","title":"Context","text":"<p>Big Miners Recap</p> <p>In theory, big miners (e.g. miners whose hash power exceeds 50% or 33%) can have serious incentive issues with the Proof-of-Work protocol, including, notably 51%-attacks.</p> <p>In practice:</p> <ul> <li>Bad News: It is relatively cheap to attack smaller blockchains by temporarily diverting a large amount of compute resources.</li> <li>Good News: When the GHash.io mining pool passed 50%, it encouraged miners to leave instead of misusing its power. Why? Because pool leaders and miners were invested in the success of Bitcoin.</li> </ul> <p>Continuing this train of thought:</p> <ul> <li>With big miners, Proof-of-Work/longest chain rule incentives are fragile.</li> <li>So far, everything has been mostly OK because mining pools and miners were invested in the success of Bitcoin.</li> <li>In other words, in Proof-of-Work, a miner\u2019s hash power is just a proxy for how much they care about the success of the cryptocurrency.</li> <li>This is not a very robust proxy if mining tech can be repurposed.</li> <li>Even more so with smaller blockchains.</li> </ul> <p>Thus, why use a proxy? Why not directly sample miners based on their stake in the cryptocurrency?</p> <p>This is the idea behind the Proof-of-Stake (PoS) protocol.</p> <p>Definition: (Vanilla) Proof-of-Stake Protocol</p> <p>At each iteration:</p> <ol> <li>Sample a random coin.</li> <li>Ask its owner to create the next block.</li> </ol> <p>The intuition behind the (vanilla) Proof-of-Stake protocol is this: - If someone has a lot of coins, they want the cryptocurrency value to go up. - If they don\u2019t (have a lot of coins), they are unlikely to be sampled to mine the next block.</p> <p>The upside is that PoS miners are directly invested in the success of the blockchain: - PoW miners\u2019 stake are fragile when mining resources can be repurposed. - PoW miners\u2019 incentives are somewhat misaligned (e.g. probably want higher mining rewards).</p> <p>The downisdes include: - PoW is bad for the environment (e.g. a larger carbon footprint than Argentina). - It is much harder to get PoS right (details coming up).</p> <p>After many delays, on September 15, 2022, Ethereum switched from PoW to PoS (i.e. \u201cThe Merge\u201d). It is harder to switch from PoW to PoS Bitcoin because Bitcoin doesn\u2019t have a \u201cgovernance\u201d mechanism for changing the protocol.</p> <p>In addition to the cryptographic preliminaries from last lecture, here are some new minimal crytography concepts we need to understand PoS.</p> <p>Definition: Pseudo-Random Function</p> <p>A pseudo-random function is a deterministic function for which you can share the code, that \"looks\" like a random function (to computationally bounded algorithms).</p> <p>Definition: Verifiable Delay Function \\(f\\)</p> <p>Similar to hash functions, with a verifiable delay function, given \\(a\\), it is easy to verify that \\(f(a) = b\\).</p> <p>Unlike hash functions, parallelizations does not speed-up finding \\(a\\).</p> <p>Definition: Cryptographic Commitment Scheme</p> <ul> <li>Input: a secret \\(x\\) and a random string \\(r\\).</li> <li>Protocol:<ul> <li>Today: Alice sends \\(c = Commit(x,r)\\).</li> <li>Tomorrow: Alice sends \\(x,r\\).</li> </ul> </li> <li>Guarantees:<ul> <li>Bob can verify that \\(c = Commit(x,r)\\).</li> <li>It is computationally intractable to find \\(x',r'\\) such that \\(c = Commit(x',r')\\).</li> </ul> </li> </ul> <p>Given these cryptographic magic at our disposal, how do we generate randomness in a decentralized, permissionless, strategyproof, consensus protocol?</p> <p>Here are some na\u00efve ideas and why they fail:</p> <ul> <li>Ask \\(t\\)-th miner to choose \\((\ud835\udc61+1)\\)-th miner \u201cat random:\" This is not strategyproof at all, because a miner can choose themselves.</li> <li>Use a pseudo-random function of entire \\(t\\)-th block: This is not strategyproof either, because the \\(t\\)-th miner can manipulate some aspects of the block (including transactions).</li> <li>Rely on external random events like stock prices or weather: This is not decentralized, since the final decision needs to be channeled through the stock market/a weather reporter. It is not (really) strategyproof either, because for enough, someone can manipulate the stock market\u2014and even the weather.</li> <li>Leverage existing decentralized coin-flipping protocols from cryptography: This is not permissionless\u2014but still potentially useful (more coming up).</li> </ul>"},{"location":"lecture-12/#why-is-proof-of-stake-so-hard","title":"Why Is Proof-of-Stake So Hard?","text":"<p>We will now explain subtle but serious problems that have plagued a lot of the early candidates of PoS.</p> <p>How long should we wait if the coin owner sampled to generate the next block has a network delay?</p> <p>What if...</p> <ul> <li>They are rebooting their machine?</li> <li>They lost their secret key?</li> <li>They are maliciously trying to slow down the network?</li> </ul> <p>One approach is to offer each owner a timeslot when they can submit their next block. If they snooze they lose. Note that this can lead to forks: for instance, Alice submitted a block, but Bob didn\u2019t see it. In turn, this raises the question of how we handle forks in PoS.</p> <p>Definition: Nothing-at-Stake</p> <p>In PoW, miners are (hopefully) incentivized to only extend the longest chain. However, in PoS, miners can try, for free, to extend every block: this is the idea of Nothing-at-Stake.</p> <p></p> <p>Definition: Predictability Attacks</p> <p>One plausible way to generate (pseudo)-randomness is to select the \\((t+1)\\)-th miner based on a pseudo-random function of \\((Genesis \\; block, t+1)\\).</p> <p></p> <p>This generates some predictability, because Miners know in advance when it will be their turn to create blocks.</p> <p>Why is predictability a problem?</p> <ul> <li>In unpredictable blockchains (e.g. PoW), each small miner has a small probability of launching a successful double-spending attack, so they are unlikely to try to double-spend.</li> <li>In predictable blockchains, there is some miner who knows that they can double-spend.</li> </ul> <p>Definition: Long Range Attacks</p> <ul> <li>An attacker buys (almost) all the coins at time \\(t\\), which is registered on block \\(b_t\\).</li> <li>Then, the attacker sells their coins at time \\(t+1\\).</li> <li>By \\(t+k\\), the buyer(s) of the coins are satisfied and pay the attacker off chain. At that point, the attacker is no longer \"staked\" in the value of the coin.</li> <li>The attacker can then fork a new chain from \\(b_t\\): in this view of the world, they still own the coins, so they can mine much faster.</li> </ul> <p></p> <p>In other words:</p> <ul> <li>Nothing-at-Stake: Miners are incentivized to try to extend every chain, not only the longest chain. This makes it harder to resolve forks.</li> <li>Predictability Attacks: If we know who the next miner is, it is easier to collude with them/attack them/bribe them/etc.</li> <li>Long Range attackS: The attacker starts mining from an old block where they owned coins. A variantconsists in buying/hacking old cryptographic keys of people who used to own coins.</li> </ul> <p>These attacks don\u2019t apply to Proof-of-Work: why?</p> <ul> <li>Nothing-at-Stake: Miners use their computational power to hash different nonces\u2013together with the hash of the block they want to point to. They can't try pointing to different blocks with the same compute resources.</li> <li>Predictability Attacks: Finding who will mine the next block requires finding a nonce that works with that miner\u2019s key, which is at least as hard as mining the next block itself. In fact, it is even harder because we don\u2019t know which nonces miners will try. Hence PoW blocks are very hard to predict.</li> <li>Long Range attackS: Rewriting the history of the blockchain doesn\u2019t change how much compute resources a miner actually has/had.</li> </ul>"},{"location":"lecture-12/#approaches-to-proof-of-stake","title":"Approaches to Proof-of-Stake","text":"<p>In this section, the intention is to offer a taste of some high-level ideas. This is not a recipe for PoS blockchains, because functioning PoS protocols tend to use a complicated combination of these.</p> <p>Definition: Slashing</p> <p>Slashing means \u201cslashing away\u201d coins from miners who misbehave, as a form of fine.</p> <p>Example: We may consider mitigating the Nothing-at-Stake problem by slashing miners who produce blocks pointing to conflicting blocks.</p> <p>Some caveats with slashing include:</p> <ul> <li>We need other miners to vote on who is misbehaving.</li> <li>Apparent misbehaviors could be caused by network issues\u2014maybe that is acceptable, since network issues also cause miners to lose blocks in PoW.</li> <li>Miners need to place slash-able coins in escrow.</li> </ul> <p>Definition: RANDAO</p> <p>An over-simplified RANDAO protocol may look like this:</p> <ul> <li>Alice and Bob each select a secret random bit, and  send each other cryptographic commitments of these bits.</li> <li>Alice and Bob send the actual bits and verify them against the commitment \\(Output = XOR(Alice\u2019s \\; bit, Bob\u2019s \\; bit)\\).</li> </ul> <p>Note that, for any fixed Alice\u2019s bit, if Bob\u2019s bit is random, the output is random. This very efficient, because it requires only 1 truly random party. However, a challenge is that, when Bob sees Alice\u2019s bit, he can decide to abort, which can bias the output.</p> <p>Definition: Verifiable Delay Functions (VDF)</p> <p>In a Verifiable Delay Function:</p> <ul> <li>The \u201cDelay\u201d requires a long time to compute and parallelization doesn\u2019t help.</li> <li>\u201cVerifiable\u201d means that it is easy to verify that the function was computed correctly.</li> </ul> <p>Examples:</p> <ol> <li> <p>Let's say that we choose tomorrow\u2019s miners using a VDF of today\u2019s closing stock prices. Because of the delay, we don\u2019t know how to manipulate the stock prices until trade closes.</p> </li> <li> <p>We may combine a VDF with a RANDAO: We may require that Bob sends his true random bit before he has time to compute the VDF and decide if he should abort.</p> </li> </ol> <p>Definition: Byzantine Coin-Flipping Protocols</p> <p>Conisder the Byzantine Generals Problem:</p> <ul> <li>\\(n\\) Byzantine generals try to coordinate an attack on a defended city.</li> <li>\\(t\\) of the generals are traitors and send adversarially confusing messages.</li> <li>Goal: \\(n-t\\) generals need to agree on the timing of the attack. Ideally, we sample the time at random so the enemy can\u2019t prepare to defend.</li> </ul> <p>In the context of a PoS protocol, we may apply a similar idea to select a random coin instead of a random attack time. A lot of research has been done in this direction since the early 80\u2019s.</p> <p>Byzantine Fault-Tolerant (BFT) protocols can reach consensus on random bits. They require a fraction of the parties to honestly participate in the protocol, such as \\(\\frac{2}{3}\\).</p> <ul> <li>By definition, BFT protocols are not permissionless (since they require a \u201c\\(\\frac{2}{3}\\)-fraction of parties\u201d to participate honestly). Note that, in some sense, PoS is not really permissionless anyway (given that a coin is a form of permission).</li> <li>This type of interactive protocols may be challenging to scale:<ul> <li>We may randomly select a small committee (\u201csortition\u201d) of active Byzantine generals: The committee then selects the next committee to replace it using a Byzantine protocol.</li> <li>This requires participants to continuously be online: We need to slash them if they go offline for too long.</li> </ul> </li> </ul> <p>Important consideration: When can we truly consider a block final? This brings us to the notion of \"finality\" and \"checkpoints.\"</p> <p>Definition: Subjectivity</p> <p>In pure permissionless, longest chain rule-based protocols, we are never 100% sure that the chain we see is really the longest one out there.</p> <p>Perhaps it is acceptable to wait \\(1\\) hour to ensure a block is final before selling a Tesla for Bitcoins. However, it may not be acceptable to wait \\(1\\) hour to buy pizzas with Bitcoins (there may be a long line of Pizza-hungry customers).</p> <p>If \\(\\frac{2}{3}\\) of holders of coins agree (using a BFT protocol) on a block, we may consider it final\u2014and, now, we can sell pizza on the blockchain again!</p> <p>Here is how we may mitigate long-range attacks using finality:</p> <ul> <li>We set up parameters so that an attacker selling their stake (\u201cunstaking\u201d) takes longer than the gap between checkpoints. (Currently in Ethereum, unstaking takes a few days, sometimes longer).</li> <li>We treat finalized checkpoints as \u201cnew genesis blocks\u201d\u2013and we ignore earlier forks.</li> </ul>"},{"location":"lecture-12/#last-few-thoughts","title":"Last Few Thoughts\u2026","text":"<p>Definition: Staking</p> <p>Although exact details vary, staking usually involves the following:</p> <ul> <li>Network participants lock a \u201cstake\u201d (\\(X\\) cryptocurrency coins).</li> <li>They are chosen to create the next block with a probability proportional to their stake \\(X\\).</li> <li>They receive fees for each time they create a block.</li> </ul> <p>What is staking good for?</p> <ol> <li>Aligning incentives of miners (i.e. proposers and validators) with cryptocurrency.</li> <li>Preventing Sybil attacks: A participant can make many clones of their computer program, but they can\u2019t change the total amount of money those clones have.</li> <li>Slashing: If miners misbehave, we can \u201cslash\u201d their stake, i.e. give them a fine.</li> <li>Raising money for a blockchain: Staking may be considered equivalent to buying stocks (this is a hot legal question).</li> </ol> <p>On a separate note, \u201cmining\u201d for Proof-of-Stake is computationally cheap, but requires robust servers with a reliable internet connection. If PoS occures mainly on the cloud, is it still \u201cdecentralized\u201d?</p> <p> </p> <p>Alternatively, participants may consider stake delegation: Instead of running their own miner on AWS, they may lend someone their coins and let them mine for them:</p> <ul> <li>Coin holders receive rewards proportional to the number of coins they (the delegee takes a small fee).</li> <li>This is convenient, as this does not require a CS degree from Stanford.</li> <li>This is liquid, since coin holders can stake small amounts and withdraw anytime.</li> </ul> <p>Some questions to ponder include:</p> <ul> <li>What are the incentives of delegee?</li> <li>Do they care if a coin holder get slashed?</li> <li>Do they care if the coin value drops?</li> </ul>"},{"location":"lecture-12/#recap","title":"Recap","text":"<p>Proof-of-Stake Recap</p> <p>Protocol</p> <p>At each iteration:</p> <ol> <li>Sample a random coin.</li> <li>Ask its owner to create the next block.</li> </ol> <p>PoS vs. PoW</p> <ul> <li> <p>Upsides:</p> <ul> <li>PoS miners are directly invested in the success of the blockchain</li> <li>PoW miners\u2019 stake are fragile when mining resources can be repurposed, and PoW miners\u2019 incentives are somewhat misaligned (e.g. probably want higher mining rewards).</li> </ul> </li> <li> <p>Downisdes:</p> <ul> <li>PoW is bad for the environment (e.g. a larger carbon footprint than Argentina).</li> <li>It is much harder to get PoS right (details coming up).</li> </ul> </li> </ul> <p>PoS Attacks</p> <ul> <li>Nothing-at-Stake: Miners are incentivized to try to extend every chain, not only the longest chain. This makes it harder to resolve forks.</li> <li>Predictability Attacks: If we know who the next miner is, it is easier to collude with them/attack them/bribe them/etc.</li> <li>Long Range attackS: The attacker starts mining from an old block where they owned coins. A variantconsists in buying/hacking old cryptographic keys of people who used to own coins.</li> </ul> <p>PoS Techniques</p> <ul> <li>Slashing: Taking away coins from miners who misbehave (Incentivize following the protocol, staying online, etc.).</li> <li>RANDAO: Protocol for generating randomness together (Let\u2019s have a coin flipping party?).</li> <li>Verifiable Delay Function (VDF): Slow even with parallelization (Mitigate manipulating sources of randomness).</li> <li>Byzantine Protocols: \\(\\frac{2}{3}\\) of miners (weighted by stake) agree on the new block (Then we are really sure about this block\u2013we can call it \u201cfinalized\u201d).</li> <li>Finalized blocks / Checkpoints: Mitigate long-range attacks.</li> </ul>"},{"location":"lecture-13/","title":"Security Markets","text":"<p>May 26, 2025</p> <p>Blockchain Recap (So Far)</p> <p>Main goal: Maintain a ledger (i.e. a history of events that we all/mostly agree on) that is:</p> <ul> <li>Decentralized: Messages are delayed/dropped (no uniform time)</li> <li>Permissionless: Users join, leave, come back at any time</li> </ul> <p>Risks of Sybil attacks require alternatives to \u201cmajority\u201d:</p> <ul> <li>Vanilla Proof-of-Stake (PoS) protocol: At each iteration:<ul> <li>(1) Sample a random coin;</li> <li>(2) Ask its owner to create the next block.</li> </ul> </li> <li>Proof-of-Work (PoW): At each iteration:<ul> <li>(1) Miners compete to invert a hash function;</li> <li>(2) First miner to succeed creates new block</li> </ul> </li> </ul> <p>Market Failures Recap</p> <p>A market failure occurs when a market fails to converge to an optimal outcome.</p> <p>We have seen five types of market failures:</p> <ol> <li>Externalities (side effects on other parties)</li> <li>Transaction costs</li> <li>Thinness (flow of buyers/sellers too small)</li> <li>Timing issues</li> <li>Information asymmetry</li> </ol> <p>No-Trade Theorems Recap</p> <p>Public information no-trade theorem: If the market already aggregated all available public information, there is no point in trading.</p> <p>Public+Private information no-trade theorem: Alice will only want to buy at current price if she has private information:</p> <ul> <li>If Alice wants to buy, she probably has private information.</li> <li>Bob doesn\u2019t want to sell Alice.</li> <li>Even if Alice learns private information, she can\u2019t use it.</li> </ul>"},{"location":"lecture-13/#flashboys-10","title":"Flashboys 1.0","text":"<p>Example: The High-Frequency Trading (HFT) Speed Arms Race</p> <p>In 2010, Spread Networks opened new $300M fiber optics from NY to Chicago. Why? Because it is (almost) a straight line, which brings latency down from 16ms to 13ms.</p> <p>Who needs 3ms saving in latency? In comparison, a blink of the eye is less than 100ms. Answer: Low latency High Frequency Traders (HFT) race to beat each other.</p> <p>There was a joke at the time saying that Spread Networks's fiber optics would soon be obsolete, when someone digs a tunnel, thus \u201cavoiding the pesky curvature of the earth.\u201d</p> <p>Ironically, in 2011, latency time down to 10ms using microwaves! It turns out that air refracts slightly better than glass.</p> <p>Is Spread Networks a market failure?</p> <p>A hypothesis may be that investing \\(\\$300\\)M for saving 3ms (for only one year) represents social waste, because someone is paying this \\(\\$300\\)M, yet no one is much happier.</p> <p>Maybe Spread Networks made so much money that they were happy, but they would have been happier to take the same money from HFTs without paying \\(\\$300\\)M for new fiber optics.</p> <p>Here are some externalities to consider:</p> <ul> <li>When the first HFT buys bandwidth from Spread Networks, they\u2019re happy because they expect to make a lot of money by being faster than their competitors. Spread Networks is also happy because they sold bandwidth.</li> <li>However, the other HFTs are sad because they\u2019re now slower than 1st HFT.</li> <li>We reach a market equilibrium when all HFTs buy bandwidth.</li> <li>As a result, none of them are happy.</li> <li>The total negative externality is greater than or equal to \\(\\$300\\)M.</li> </ul> <p>Should we consider an even larger market failure?</p> <p>HFTs make a lot more than $300M a year. Yet, they just buy and sell stocks (securities) and make money out of it! Does anyone else benefit from having them?</p>"},{"location":"lecture-13/#a-stock-market-model","title":"A Stock Market Model","text":"<p>We will slowly build up an increasingly elaborate model. Yet, even at the end of this lecture, we will still abstract out many important aspects.</p> <p>Let's model a market for fungible (i.e. interchangeable) shares of one company.</p> <p>Main model points:</p> <ul> <li>Stock exchange rules: continuous limit order book.</li> <li>Players:<ul> <li>Na\u00efve investors</li> <li>Liquidity providers</li> <li>Snipers</li> </ul> </li> <li>Stock \u201cvalue\u201d (and how it changes)</li> </ul> <p>Continuous Limit Order Book Recap</p> <p>Definitions:</p> <ul> <li>Bid: highest buy order</li> <li>Ask: lowest sell order</li> </ul> <p>Rules:</p> <ul> <li>Buy/sell orders can arrive at any time.</li> <li>Trading happens whenever a new buy order is greater than the ask (or a new sell order less than the bid).</li> </ul> <p>Two kinds of orders:</p> <ul> <li>Marketable orders already have a match in the book.</li> <li>Resting (non-marketable) orders are waiting to be matched.</li> </ul> <p>Definition: Na\u00efve Investors</p> <p>Homer just received his paycheck. He wants to buy 1 stock to save for retirement. Abe is retired. He wants to sell 1 stock to pay for health insurance. Homer can buy 1 stock from Abe!</p> <p>Problem (Market thinness): Homer gets his paycheck on the 10th, while Abe\u2019s insurance fees due on the 1st.</p> <p>we need liquidity providers!</p> <p>Note: No-trade theorems don\u2019t apply with na\u00efve investors (Investing for retirement doesn\u2019t make sense for prediction markets).</p> <p>Liquidity Providers Recap</p> <p>Liquidity providers buy low now and sell high later (or: sell high now and buy low later).</p> <p>Liquidity providers leave bid/ask resting orders on the order book.</p> <p>Na\u00efve investors can buy/sell at any time by trading with liquidity providers. When they buy+sell, liquidity providers earn the spread. That means that the na\u00efve investors lose this spread. Spread is the part of na\u00efve investors transaction costs.</p> <p>Definition: Stock Value</p> <p>The \u201ctrue\u201d value of the stock represents investors\u2019 aggregate belief of how much it would pay in the future, e.g., how much dividends it will ever pay, discounting for risk and time.</p> <p>At equilibrium, the true value is between the bid and the ask. Otherwise, investors could buy/sell more stocks.</p> <p>Various events can change the stock\u2019s value (i.e. change investors\u2019 aggregate belief):</p> <ol> <li>Correlated stocks (securities):<ul> <li>If someone buys gold in Chicago, the price of gold in NYC goes up.</li> <li>ES and SPY are two ways to invest in S&amp;P 500.</li> </ul> </li> <li>The 2013 Fed Robbery: The Federal Reserve frequently announces changes in monetary policy. On 2013, September 18th, 2:00:00 PM, when the Fed made an exciting announcement, more than \\(\\$5\\) billion traded within 100 milliseconds. This was controversial, as it means that markets in NYC and Chicago reacted faster than speed of light.</li> <li>Twitter (now X): The timing of tweets is much less regulated than Fed Reserve's announcements. For instance, the stock market moved when Trump tweeted \"covfefe\" in 2017. Are NLP models robust enough to make this kind of call in a few microseconds?</li> <li>Reddit: The timing of posts is also much less regulated than Fed Reserve's announcements, as suggested by the impact of <code>r/wallstreetbets</code> on the GameStop stock price in 2021.</li> <li>Hacks: In 2024, The @SECGov X account was compromised, and an unauthorized tweet was posted, falsely announing that the SEC had approved the listing and trading of spot Bitcoin exchange-traded products, which moved the price of Bitcoin.</li> </ol> <p>Definition: Snipers</p> <p>Snipers wait for a change in a stock value and rush to trade with liquidity providers\u2019 resting orders.</p> <p>When a stock value changes, liquidity providers rush to cancel (or update) their resting orders before getting sniped.</p> <p>Being slightly faster than a competitor is worth a lot of money. Many races are won by a margin of less than 10 \u03bcs.</p> <p>Luck also helps: sometimes, a slower order is accepted due to randomness in the stock exchange\u2019s computer!</p> <p>Putting it all together:</p> <ul> <li>Na\u00efve investors rely on trading with resting orders by liquidity providers (especially in thin markets).</li> <li>When a stock value changes, snipers and liquidity providers race to update/snipe resting orders. This is risky and costly for liquidity providers.</li> <li>Liquidity providers increase the spread to make up for sniping risk.</li> <li>An increased spread means increased transaction costs for na\u00efve investors.</li> </ul>"},{"location":"lecture-13/#fixes-for-these-market-failures","title":"Fixes for These Market Failures?","text":"<p>Can the following speed bumps resolve speed arm races and reduce risk for liquidity providers?</p> <p>Na\u00efve attempts do not work:</p> <ul> <li>Symmetric speed bumps (i.e. all orders get delayed by the same amount):<ul> <li>Don\u2019t solve speed arm races.</li> <li>Don\u2019t solve sniping risk for liquidity providers.</li> </ul> </li> <li>Random speed bumps (i.e. each order gets delayed by some random amount):<ul> <li>Mostly solve speed arm races (when the random delay is greater than the speed difference).</li> <li>Make sniping risk worse, as one liquidity provider may compete with many snipe attempts.</li> </ul> </li> </ul> <p>Better proposals exist:</p> <ul> <li>Sniper-only speed bumps (i.e. only delay sniping orders, which is technically a speed bonus to cancelling resting orders):<ul> <li>Give liquidity providers time to react to value-changing events.</li> <li>Solve speed arm races.</li> <li>Solve liquidity providers\u2019 sniping risk.<ul> <li>Reduce their expected cost.</li> <li>Liquidity providers can afford a smaller spread.</li> <li>Reduce transaction costs for na\u00efve investors.</li> </ul> </li> </ul> </li> <li>Frequent batch auctions:<ul> <li>We batch orders for some short interval (e.g. 100 ms).</li> <li>For each batch, we find the market clearing price.</li> <li>Advantage: everyone has time to react to value-changing-events.</li> <li>Orders are prioritized by price instead of arrival time.    </li> </ul> </li> </ul>"},{"location":"lecture-13/#flashboys-20","title":"Flashboys 2.0","text":"<p>Things get even wackier on the blockchain.</p> <p>Definition: Smart Contracts</p> <p>So far we talked about blockchains that record payments (\u201ctransactions\u201d).Ethereum and other blockchains (but not Bitcoin) support smart contracts:</p> <ul> <li>Smart: Can run complex computer code (ideally Turing-complete).</li> <li>Contract: Both parties agree to terms (and sign).</li> </ul> <p>Example: Loan</p> <ol> <li>Alice pays Bob \\(\\$1\\).</li> <li>300 blocks from now, Bob will pay Alice \\(\\$1.1\\).</li> </ol> <p>Example: Collateralized Loan</p> <p>This is more realistic:</p> <ol> <li>Alice pays Bob \\(\\$1\\) and Bob sends the contract a collateral.</li> <li>300 blocks from now, if Bob hasn\u2019t paid yet, Alice can take the collateral.</li> </ol> <p>In practice, some concreate instances of smart contracts include:</p> <ul> <li>Non-Fungible Tokens (NFTs).</li> <li>Shareholders votes: Once a month, every stock owner can cast a vote to decide on the next dividend.</li> <li>Gambling.</li> <li>Games (e.g., CryptoKitties).</li> </ul> <p>In the collateralized loan example, it is crucial that \"Alice pays Bob \\(\\$1\\)\" and \"Bob sends the contract a collateral\" happen simultaneously. Otherwise, Bob could abort after receiving the money. These are atomic transactions.</p> <p>Definition: Atomic Transactions</p> <p>Either all steps of the transaction are executed successfully, or all steps are cancelled.</p> <p>Example: Flash Loan</p> <p>No collateral is needed when:</p> <ol> <li>Alice pays Bob \\(\\$1\\).</li> <li>Bob pays Alice \\(\\$1.1\\) back in the same block.</li> </ol> <p>Definition: Decentralized Finance (DeFi):</p> <ul> <li>Supports trading of cryptocurrencies, stocks, contracts, derivatives, etc.</li> <li>Is a popular application of smart contracts</li> </ul> <p>Some variants, such as limit order book (IDEX, Paradex, Etherdelta, ...) and Automated market makers (Uniswap, Bancor) can be automated as decentralized smart contracts.</p> <p>Punchline: Extreme sniping opportunities exist in decentralized finance.</p>"},{"location":"lecture-13/#sniping-on-defi","title":"Sniping++ on DeFi","text":"<p>Definition: Arbitrage:</p> <p>Simultaneously buying low in market L and selling high on market H.</p> <p>In centralized finance (e.g. NYSE), the arbitrageur takes some risk: if they buy low first, the price on market H may drop by the time they sell.</p> <p>In contrast, DeFi allows for risk-free atomic arbitrage by bundling \"buy low on market L\" and \"sell high on market H\" into a single atomic transaction. The transaction will execute only if both transactions go through!</p> <p>Additional arbitrage opportunities in DeFi include:</p> <ul> <li>Cryptocurrency prices are highly volatile (some markets update slower/faster).</li> <li>More information is broadcast to everyone (including arbitrageur): the full code and state of smart contract automated market makers is available, which makes it easier to discover exploitable bugs.</li> <li>It is easy to amplify arbitrage using huge collateral-free flash loans.</li> </ul> <p>(Extreme) Example: bZx attacks:</p> <p>3 times in 2020, hackers/arbitrageurs found bugs in DeFi lending platform bZx. They stole about \\(\\$350K\\), \\(\\$600K\\), and \\(\\$8M\\) (the last sum has allegedly been recovered by bZx).</p> <p></p> <p>In short: bZx used the wrong ETH-sUSD price when computing the collateral, so the borrower was better off just walking away with the borrowed ETH. </p> <p>Notice that this complex chain of trades was all executed in a single block because the borrower had to return the flash loan!</p> <p>Definition: Front-Running:</p> <p>Alice sees that Bob is on his way to buy a lot of shares of Stock XYZ. This transaction will increase the price of XYZ. So, Alice runs in front of Bob to by the available shares before the price increase (or, even better: Alice sells XYZ to Bob at a higher price).</p> <p>Front-running is generally illegal in centralized finance, but in DeFi, everyone sees Bob\u2019s order when it\u2019s broadcast to miners.</p> <p>An extreme form of front-running is when miners are also traders:</p> <ul> <li>Suppose an arbitrageur finds a highly profitable risk-free trade. The arbitrageur broadcasts this transaction to miners. Then a miner can execute same trade from their own account instead.</li> <li>The miner still includes the arbitrageur\u2019s transaction in their block (for a high fee), but after the miner\u2019s transaction. The miner\u2019s transaction uses up the liquidity on the profitable trade.</li> </ul>"},{"location":"lecture-13/#miners-extractable-value-mev","title":"Miners\u2019 Extractable Value (MEV)","text":"<p>In centralized finance, speed arms race to get to the top of the order book. In DeFi, arbitrageurs compete to have the highest transaction fee to get to the top of the next block. We run auctions for \u201cgas\u201d (space in block), because transaction ordering influences MEV.</p> <p>Punchline: Miners can extract a lot of value.</p> <p>Miners\u2019 Rewards Structure:</p> <ul> <li>Miners (PoW or PoS) can receive a flat reward for each block they \u201cmine\u201d.</li> <li>Miners also earn custom tips (\u201ctransaction fees\u201d), or more generally MEV</li> </ul> <p>Originally, block rewards were greater than MEV, but important trends are changing this:</p> <ul> <li>Block rewards are decreasing:<ul> <li>On Bitcoin: built-in block rewards are halving every ~4 years</li> <li>On Ethereum: historically, block rewards have generally been decreasing (more complicated)</li> </ul> </li> <li>MEV is increasing: <ul> <li>There is more competition to be included in limited block space</li> <li>More sophisticated DeFi means more sophisticated front running, etc.</li> <li>Flashbots, etc: special networks that auction space in MEV-optimized blocks</li> </ul> </li> </ul> <p></p> <p>In September 2020, fees surpaased block rewards on Ethereum for the first time. Due to transaction fees and MEV, not all blocks are the same.</p>"},{"location":"lecture-13/#fee-sniping-selfish-tie-breaking-and-undercutting","title":"Fee sniping, selfish tie-breaking, and undercutting","text":"<p>Definition: Fee-Sniping Attack</p> <p>Miners prefer to mine an alternative \\(\\widehat{b_t}\\) that includes the large MEV</p> <p></p> <p>With \\(\\alpha\\)-fraction of hash power, we have:</p> \\[ Pr[2 \\; blocks \\; to \\; make \\; new \\; chain \\; longest] = \\alpha^2. \\] <p>The attack pays off whenever:</p> \\[   \\alpha^2 \\cdot \"largeFees\" &gt; \\alpha \\cdot\"smallFees\" \\] <p>The success probability is even higher if other miners try to attack instead of extending \\(b_t\\).</p> <p>Definition: Selfish Tie-Breaking</p> <p>Given equal-length chains (\\(b_t\\), \\(\\widehat{b_t}\\)):</p> <ul> <li>In the default blockchain protocol, we tie-break in favor of the block we saw first.</li> <li>However, strategic miners favor the block that leaves more MEV on the table. Note: this deviation is strategic even for small miners!</li> </ul> <p></p> <p>Definition: Undercutting</p> <p>Undercutting is essentially fee sniping + selfish tie-breaking:</p> <ul> <li>The \\(\\widehat{b_t}\\) block steals most large MEV, but leaves some for extension \\(\\widehat{b_{t+1}}\\).</li> <li>Miners use selfish tie-breaking to incent other miners to join their fee sniping attack.</li> </ul> <p></p> <p>Some ideas that have been considered to mitigate these issues:</p> <ul> <li>Transaction fees distributed between the miners of the next \\(k\\) blocks (e.g. \\(k=100\\)).</li> <li>Capped transaction fees.</li> <li>\u201cBurn\u201d transaction fees (instead of allocating them to miners).</li> </ul> <p>What is a fundamental problem that all these proposed mitigation have?</p> <p>Users with high-priority transactions can incent miners with side payments (or miners directly extract MEV).</p>"},{"location":"lecture-13/#flashboys-00","title":"\u201cFlashboys 0.0\u201d","text":"<p>Example: 19th Century Speed Arms Race</p> <p></p> <ul> <li>1816: Israel Beer Josaphat born in Kassel (present Germany). </li> <li>1830\u2019s: He moves to Goettingen, hangs out with Gauss who was inventing the telegraph.</li> <li>1845: He converts to Christianity (Protesant), changes name to Paul Julius Reuter.</li> <li>1850: He uses Aachen-Brussels pigeon carrier to communicate stock prices between Berlin and Paris (pigeons were faster than trains).</li> <li>1851: A Aachen-Brussels telegraph line is built, so Reuter moves to London.</li> </ul> <p></p> <ul> <li>1863: Reuter constructs a private telegraph link from Cork to Crookhaven (current population: 59). Why? Because it is the first place to catch news traveling from new world by steam boats.</li> <li>1865: Reuter is the first in Europe to learn about Lincoln\u2019s assassination. His contacts in the stock markets knew about it hours before anyone else!</li> </ul> <p>Considering Reuters vs Spread Networks:</p> <ul> <li>The Reuters News Agency quickly became a leading source for news to the general public (in 1865, there were \u201conly\u201d 12 days of latency to learn about Lincoln\u2019s assassination).</li> <li>Will Spread Networks cables eventually find uses outside financial markets speed race?</li> </ul>"},{"location":"lecture-13/#recap","title":"Recap","text":"<p>High-Frequency Trading Recap</p> <ul> <li>Na\u00efve investors rely on trading with resting orders by liquidity providers (especially in thin markets).</li> <li>When value changes, snipers and liquidity providers race to update/snipe resting orders: this is risky and costly for liquidity providers.</li> <li>Liquidity providers increase spread to make up for sniping risk.</li> <li>Increased spread means increased transaction cost for na\u00efve investors.</li> </ul> <p>High-Frequency Trading Recap</p> <ul> <li>Na\u00efve investors rely on trading with resting orders by liquidity providers (especially in thin markets).</li> <li>When value changes, snipers and liquidity providers race to update/snipe resting orders: this is risky and costly for liquidity providers.</li> <li>Liquidity providers increase spread to make up for sniping risk.</li> <li>Increased spread means increased transaction cost for na\u00efve investors.</li> </ul> <p>Interventions include:</p> <ul> <li>Na\u00efve (symmetric/random) speed bumps don\u2019t resolve sniping cost.</li> <li>Sound proposals<ul> <li>Sniper-only speed bumps (fairly specific to fungible good market design).</li> <li>Frequent batch auctions (batching is a generally useful design principle).</li> </ul> </li> </ul> <p>Smart Contracts &amp; DeFi Recap</p> <ul> <li> <p>Smart Contracts:</p> <ul> <li>Smart: Can run complex computer code (ideally Turing-complete).</li> <li>Contract: Both parties agree to terms (and sign).</li> <li>Example: Implement an automated market maker or limit order book as a smart contract in DeFi.</li> </ul> </li> <li> <p>Atomic transaction:</p> <ul> <li>Either all steps of the transaction are executed successfully, or all steps are cancelled.</li> <li>Example: In a flash loan, Alice pays Bob \\(\\$1\\), Bob will pay Alice \\(\\$1.1\\) in the same block, with no collateral needed.</li> </ul> </li> </ul> <p>Miners\u2019 Opportunities in DeFi Recap</p> <ul> <li>Arbitrage: Simultaneously buying low in market L and selling high on market H.</li> <li>Even more arbitrage opportunities with DeFi:<ul> <li>Risk-free atomic arbitrage</li> <li>Highly volatile cryptocurrencies</li> <li>Explicit information broadcast to arbitrageur</li> <li>Huge collateral-free flash loans available</li> </ul> </li> <li>Front-running: Alice sees that Bob is on his way to buy a lot of shares of Stock XYZ. Alice runs in front of Bob to by the available shares before the price increase.</li> <li>Competition to get to top of the block vs speed arms race. Miners enjoy increasing transaction fees, or steal transactions for themselves.</li> <li>Transaction fees distort miners\u2019 incentives:<ul> <li>Fee-sniping: create alternative block \\(\\widehat{b_t}\\) to steal high-fee transactions from \\(b_t\\).</li> <li>Undercutting: The \\(\\widehat{b_t}\\) block steals most large MEV, but leaves some for extension \\(\\widehat{b_{t+1}}\\). Miners use selfish tie-breaking to incent other miners to join their fee sniping attack.</li> </ul> </li> </ul> <p></p>"},{"location":"lecture-14/","title":"Envy and Fairness","text":"<p>May 28, 2025</p> <p>In week 1, we discussed matching problems modeled with ordinal utilities. In most lectures since, we looked into markets/auctions with money.</p> <p>The basic assumption for mechanism design with money is \"How much I want something = how much I\u2019m willing to pay for it.\" However, an obvious caveat to keep in mind is that \"How much I\u2019m willing to pay also depends on how much money I have.\"</p> <p>Today, we are going to discuss fair allocation without monetary transfers (w/ cardinal utilities).</p>"},{"location":"lecture-14/#i-cut-you-choose-mechanism","title":"I-Cut-You-Choose Mechanism","text":"<p>Alice and Bob want to split a cake. Each has different preferences over strawberries, blueberries, cheese, etc. </p> <p>Definition: I-Cut-You-Choose mechanism</p> <ol> <li>Alice cuts the cake into two equal-for-her pieces.</li> <li>Bob chooses his favorite of the two pieces</li> </ol> <p>This mechanism is a long-time favorite, even used to divide land in the Bible:</p> <p>\u201cAnd Abram said unto Lot, \"Let there be no strife, I pray thee, between me and between thee and between my herdmen and between thy herdmen, for we be brethren.\u00a0Is not the whole land before thee? Separate thyself, I pray thee; if thou wilt take the left hand, then I will go to the right; or if thou depart to the right hand, I will go to the left.\u201c \u2013 Genesis 13:8-9</p> <p>This mechanism is also very simple: Aviad uses it often at home with his kids.</p> <p>Yet, is I-cut-you-choose strategyproof?</p> <p>Claim: I-cut-you-choose is not strategyproof.</p> <p>Proof:</p> <p>Suppose Alice just wants to get as much cake as possible:</p> <ul> <li>If Bob really likes blueberries, Alice could cut a small piece of cake with 51% of blueberries. Bob would choose it, and Alice gets the rest of the cake.</li> <li>If Bob really likes strawberries, small piece with 51% of strawberries\u2026</li> </ul> <p>To cut optimally, Alice wants to know Bob\u2019s preferences\u2014and Bob probably doesn\u2019t want to tell her in advance.</p> <p>In other words, I-cut-you-choose is not strategyproof for the cutter (but it is strategyproof for the chooser).</p> <p>Definition: Envy-Freeness (1/2).</p> <p>An allocation is envy-free if no agent envies another agent\u2019s allocation (i.e. no agent would rather have the other\u2019s allocation over their own).</p> <p>Claim: If Alice cuts the cake into 2 pieces that are equal for her (and Bob picks his favorite), the resulting allocation is envy-free.</p> <p>Note that envy-freeness is a stronger property than stability: for example, giving the entire cake to Alice is stable (because Alice wouldn\u2019t want to switch allocation), but not envy-free (because Bob would).</p> <p>It\u2019s pretty satisfying that I-cut-you-choose is envy-free! However, maybe some envy-free allocations still seem unfair (e.g. think of Bob getting a small piece with 51% of blueberries).</p> <p>How can we analyze mechanisms from a fairness perspective?</p>"},{"location":"lecture-14/#fairness-criteria-and-objectives","title":"Fairness Criteria and Objectives","text":"<p>We\u2019re about to see a lot of definitions. Aviad doesn't expect us to remember all of them by the end of lecture. The point is to give us (a) a sense of the complexity of this question, and (b) a taste of major approaches to tackling it.</p> <p>Formalizing fair allocations:</p> <ul> <li>Our goal is to choose an allocation \\(A = (A_1, ..., A_n)\\) to \\(n\\) agents.</li> <li>Each agent \\(i\\) has a valuation function \\(v_i()\\).</li> </ul> <p>For this part of lecture, we can just think about additive valuations: If \\(x,y\\) are disjoint, then \\(v_i(x \\cup y) = v(x) + v(y)\\).</p> <p>Definition: Envy-Freeness (2/2).</p> <p>We may re-define envy-free allocations as:</p> \\[   \\forall i,j \\; v_i(A_i) \\geq v_i(A_j). \\] <p>There exists a lot of ways to define fair allocations.</p> <p>Definition: Proportional.</p> <p>We may define proportional allocations as:</p> \\[   \\forall i \\; v_i(A_i) \\geq \\frac{v_i(A)}{n}. \\] <p>In other words: my happiness guarantee doesn\u2019t depend on others\u2019 allocation.</p> <p>Definition: Perfect.</p> <p>We may define perfect allocations as:</p> \\[   \\forall i \\; v_i(A_i) = \\frac{v_i(A)}{n}. \\] <p>This is similar to proportional, but also making sure that no one is too happy (we may want to think about why this may be helpful).</p> <p>Definition: Equitable.</p> <p>We may define equitable allocations as:</p> \\[   \\forall i,j \\; v_i(A_i) = v_j(A_j). \\] <p>This a little funny because we have to compare \\(i\\)\u2019s happiness to \\(j\\)\u2019s happiness (we may want to think about how to achieve this).</p> <p>Definition: Competitive equilibrium from equal incomes (CEEI)</p> <ul> <li>Give each agent a budget of 1 (fake money).</li> <li>Find prices (for pieces of cake) and an allocation such that:<ul> <li>The entire cake is allocated (unless there is a zero-priced unwanted leftover piece).</li> <li>Every agent is allocated the favorite piece they can afford given their budget constraint.</li> </ul> </li> </ul> <p>Note: With fake money, agents maximize value subject to budget constraint. This is different from Lecture 6 where they maximized utility based on value-price.</p> <p>With indivisible goods, often none of these can be satisfied! For example, if we want to allocate 1 hat, then both Alice and Bob want it.</p> <p>Notice that for this example we would have a competitive equilibrium with real money: Alice gets the hat and pays for it, Bob gets nothing and keeps his money. But with fake money Bob isn\u2019t satisfied with keeping his fake money.</p> <p>Definition: Maximin Share (MMS)</p> <p>Each agent \\(i\\) proposes (cuts) allocation \\(A^i\\) to \\(n\\) agents.</p> <p>(\\(A\\) still denotes the real allocation.)</p> <p>MMS condition:</p> \\[   v_i(A_i) \\geq \\underset{A^i}{max} \\; \\underset{j}{min} \\; v_i(A_j^i) \\] <p>Some unfairness is inevitable (for instance if we think of the hat example). MMS tries to capture the minimal necessary unfairness. It is a little bit like saying to agent \\(i\\), \u201cEven if you cut the cake any way you wanted to, you wouldn\u2019t guarantee more than you got in this allocation\u201d. This is why we say that MMS is \u201cno worse than worst-case I-cut-you-choose.\"</p> <p>Surprisingly, sometimes, even MMS cannot be satisfied.</p> <p>One idea is to approximately-fairly dividing indivisible items.</p> <p>Definition: Envy-Free-Up-to-1-Item (EF1)</p> \\[   v_i(A_i) \\geq v_i(\\frac{A_j}{\"best\" \\; item}). \\] <p>EF1 in equations:</p> \\[   \\forall i,j \\; \\exists g \\in A_j \\; v_i(A_i) \\geq v_i (\\frac{A_j}{g}). \\] <p>EF1 in words: </p> <ul> <li>\\(v_i(A_j)\\) = the value agent \\(i\\) would have for agent \\(j\\)\u2019s allocation \\((A_j\\).</li> <li>A weaker benchmark is: \\(v_i(\\frac{A_j}{\"best\" \\; item})\\) = the same as \\(v_i(A_j)\\), but we discard \\(i\\)\u2019s favorite item.</li> </ul> <p>EF1 can always be satisfied, e.g., agents taking turns selecting one item at a time (\u201cround robin\u201d). However, EF1 is sometimes not very fair. For example, Alice and Bob are splitting 2 cars and 10 chairs:</p> <ul> <li>Fair allocation: each gets one car + 5 chairs.</li> <li>Also EF1: each gets one car, and Alice gets all the chairs.</li> </ul> <p>Definition: Envy-Free-Up-to-Any-Item (EFX)</p> \\[   v_i(A_i) \\geq v_i(\\frac{A_j}{\"worst\" \\; item}). \\] <p>Can EFX always be satisfied? This is an open problem.</p> <p>Definition: \\(\\alpha\\)-Maximin Share (MMS)</p> <p>\u201cOnly a bit (\\alpha) worse than worst-case I-cut-you-choose.\u201d</p> <p>\\(\\alpha\\)-MMS condition:</p> \\[   v_i(A_i) \\geq \\alpha \\; \\underset{A^i}{max} \\; \\underset{j}{min} \\; v_i(A_j^i) \\] <p>Theorem: A \\(\\frac{3}{4}\\)-MMS allocation always exists.</p> <p>Is more-than-\\(\\frac{3}{4}\\)-MMS always impossible? This is another open problem.</p> <p>Definition: Approximate Competitive equilibrium from equal incomes (A-CEEI)</p> <ul> <li>Give each agent a budget of approximately 1 (fake money).</li> <li>The allocation is only required to approximately clear markets.</li> </ul> <p>An A-CEEI always exists, although with some tradeoff in approximations of equal incomes vs. market clearing.</p> <p>However, this is computationally intractable: the computational complexity of finding an A-CEEI was one of Aviad's first research projects in PhD, and he still works on it.</p> <p>What do we mean by approximately market clearing? We have to allocate non-existing goods.</p> <p>Another approach is to look for an allocation that maximizes fair objective functions.</p> <p>Definition: Utilitarian/Social Welfare</p> <p>\u201cMaximum total happiness\u201d</p> \\[   \\underset{A_1, ..., A_n}{max} \\sum_i v_i(A_i) \\] <p>Definition: Egalitarian/Maximin</p> <p>\u201cMaximum happiness of the saddest person\u201d</p> \\[   \\underset{A_1, ..., A_n}{max} \\; \\underset{j}{min} \\; v_i(A_i) \\] <p>Definition: Nash Social Welfare</p> <p>Middle ground between utilitarian and egalitarian:</p> <ul> <li>Emphasis on increasing lower \\(\ud835\udc63_\ud835\udc56\\) (we can think of the extreme case where one \\(v_i\\) is close to zero).</li> <li>Yet, it doesn\u2019t completely ignore higher \\(v_i\\) like egalitarian.</li> </ul> \\[   \\underset{A_1, ..., A_n}{max} \\; \\prod_i \\; v_i(A_i) \\] <p>The Nash Social Welfare is scale-invariant, which makes it easier to compare different agents without money.</p> <p>Theorem: CEEI allocation maximizes Nash Social Welfare.</p> <p>Examples: Some fair allocations in practice.</p> <ul> <li>Allocating goods:<ul> <li>Spliddit used to maximize social welfare subject to MMS/proportionality/EF. </li> <li>They decided that NSW is better.</li> </ul> </li> <li>Allocating classes:<ul> <li>Approximate-CEEI</li> <li>Combinatorial demand</li> <li>The number of students must significantly exceed the number of classes</li> </ul> </li> <li>Allocating chores:<ul> <li>Spliddit used to assign chores by assuming they\u2019re divisible (we can use randomized rounding to assign fractional chores) and drawing new chores each week to guarantee approximate \u201cex-post\u201d fairness.</li> <li>Then they looked for an equitable solution, and optimized over all equitable solutions using Linear Programming.</li> </ul> </li> <li>Allocating computing (see upcoming example from Databricks).</li> </ul>"},{"location":"lecture-14/#dominant-resource-fairness-drf","title":"Dominant Resource Fairness (DRF)","text":"<p>Definition: Leontieff Utility</p> <p>I want to make as much cake as possible, so I need fixed ratios of \\(flour : oil : sugar\\).</p> <p>Example</p> <p>Consider the following computational resource sharing model:</p> <ul> <li>We have \\(m\\) resources (CPU, memory, I/O, \u2026) with fixed capacities.</li> <li>There are \\(n\\) users:<ul> <li>Each user wants to run as many identical tasks as possible.</li> <li>Each task demands a fixed vector of resources (2 GPU, 3 TB RAM).</li> <li>For now, tasks are infinitesimally small (2\\(\\epsilon\\) GPU, 3\\(\\epsilon\\) TB RAM).</li> </ul> </li> </ul> <p>How can we allocate these resources to these users? In particular, we want to - Ensure fairness, efficiency. - Avoid monetary transfers (e.g. allocation inside a given organization).</p> <p>Incentives are important in compute allocation:</p> <ul> <li>\u201cFor example, one of Yahoo!\u2019s Hadoop MapReduce datacenters has different numbers of slots for map and reduce tasks. A user discovered that the map slots were contended, and therefore launched all his jobs as long reduce phases, which would manually do the work that MapReduce does in its map phase.\u201d</li> <li>\u201cA big search company provided dedicated machines for jobs only if the users could guarantee high utilization. The company soon found that users would sprinkle their code with infinite loops to artificially inflate utilization levels.\u201d</li> </ul> <p>Definition: Dominant Resource Fairness</p> <p>Fix an allocation of resources to users:</p> <ul> <li>User \\(i\\) has some share (fraction) of the total capacity of each resource.</li> <li>User \\(i\\)\u2019s dominant resource is the resource with the largest share.</li> </ul> <p>Dominant resource fairness means equal shares of dominant resources across users.</p> <p>Example: Not Dominant Resource Fairness</p> <p>Consider the following scenario:</p> <ul> <li>Total capacity = 10 GPU, 20 TB</li> <li>Ali received 4 GPU, 16 TB</li> <li>Ali\u2019s GPU share \\(= \\frac{2}{5}\\)</li> <li>Ali\u2019s memory share \\(= \\frac{4}{5}\\)</li> <li>Beth\u2019s GPU share \\(\\leq \\frac{3}{5}\\)</li> <li>Beth\u2019s memory share \\(\\leq \\frac{1}{5}\\)</li> </ul> <p>Ali\u2019s dominant resource is memory: this is not DRF.</p> <p>Example: Dominant Resource Fairness</p> <p>Consider the following scenario:</p> <ul> <li>Total capacity = 10 GPU, 20 TB</li> <li>Ali received 2 GPU, 16 TB</li> <li>Ali\u2019s GPU share \\(= \\frac{1}{5}\\)</li> <li>Ali\u2019s memory share \\(= \\frac{4}{5}\\)</li> <li>Beth\u2019s GPU share \\(= \\frac{4}{5}\\)</li> <li>Beth\u2019s memory share \\(= \\frac{1}{10}\\)</li> <li>\\(\\frac{1}{10}\\) of memory remains unused</li> </ul> <p>Ali\u2019s dominant resource is memory and Beth's dominant resource is GPU: this is DRF.</p> <p>Definition: Dominant Resource Fairness Mechanism</p> <ul> <li>Every user submits their ratio of demands (e.g. 2 GPU : 1 TB).</li> <li>The algorithm computes the (unique) maximal allocation subject to DRF.</li> </ul> <p>Theorem: Every user prefers the DRF allocation to \\(\\frac{1}{n}\\) of each resource.</p> <p>In other words, every user prefers DRF over na\u00efve resource allocation. This property is called sharing incentives and it is related to individual rationality.</p> <p>Proof: Allocation is maximal</p> <p>\\(\\rightarrow\\) Some resource \\(j^\\star\\) is exhausted.</p> <p>\\(\\rightarrow\\) Some user \\(i^\\star\\) gets \\(\\geq \\frac{1}{n}\\) of \\(j^\\star\\).</p> <p>\\(\\rightarrow\\) \\(i^\\star\\) has \\(\\geq \\frac{1}{n}\\) of her dominant resource.</p> <p>\\(\\rightarrow\\) every user has \\(\\geq \\frac{1}{n}\\) of their dominant resource.</p> <p>Theorem: The DRF allocation is envy-free.</p> <p>Proof: Assume by contradiction that Ali envies Beth.</p> <p>\\(\\rightarrow\\) Beth has more of every resource\u2013including Ali\u2019s dominant resource.</p> <p>\\(\\rightarrow\\) This is a contradiction to DRF.</p> <p>Theorem: DRF is Pareto-optimal.</p> <p>Proof: Allocation is maximal</p> <p>\\(\\rightarrow\\) Some resource \\(j^\\star\\) is exhausted.</p> <p>\\(\\rightarrow\\) By simplifying assumption, we can\u2019t make someone happier without taking some \\(j^\\star\\) from someone else.</p> <p>Theorem: The DRF mechanism is strategyproof</p> <p>Proof: Let \\(A_T\\) denote the allocation of DRF with true preferences.</p> <p>Assume by contradiction that Ali can misreport to get a better allocation \\(\\widehat{A}\\).</p> <p>By Pareto-optimality of \\(A_T\\), Beth has a smaller allocation in \\(\\widehat{A}\\) than in \\(A_T\\).</p> <p>\\(\\rightarrow\\) Beth receives less of her dominant resource in \\(\\widehat{A}\\).</p> <p>\\(\\rightarrow\\) Ali receives less of his dominant resource in \\(\\widehat{A}\\)-this is a contradiction.</p>"},{"location":"lecture-14/#beyond-drf-in-practice","title":"(Beyond) DRF in Practice","text":"<p>Let's revisit our example from earlier:</p> <p>Example</p> <p>Recall the following vanilla assumptions:</p> <ul> <li>We have \\(m\\) resources (CPU, memory, I/O, \u2026) with fixed capacities.</li> <li>There are \\(n\\) users:<ul> <li>Each user wants to run as many identical tasks as possible.</li> <li>Each task demands a fixed vector of resources (2 GPU, 3 TB RAM).</li> <li>Simplifying assumption: demand \\(&gt;0\\) for every resource.</li> <li>For now, tasks are infinitesimally small (2\\(\\epsilon\\) GPU, 3\\(\\epsilon\\) TB RAM).</li> </ul> </li> </ul> <p>In practice:</p> <ul> <li>Tasks are not identical, and they arrive and depart over time \\(\\rightarrow\\) This is a hard problem, both in theory and in practice, which requires Practical DRF (While resources are not exhausted, select user \\(i\\) with the minimum dominant share, and add their next feasible task).</li> <li>Some tasks have flexibility in resource use (e.g., map vs. reduce manipulation example).</li> <li>Some tasks have more specific desiderata (e.g., specific hardware, specific location).</li> <li>Tasks are not infinitesimally small \\(\\rightarrow\\) \\(\\rightarrow\\) This is a hard problem, both in theory and in practice, which requires Practical DRF (While resources are not exhausted, select user \\(i\\) with the minimum dominant share, and add their next feasible task).</li> <li>Some users have higher priority \\(\\rightarrow\\) This requires Weighted DRF.</li> <li>Some tasks have zero demand for some resource \\(\\rightarrow\\) This requires Extended DRF (Find the maximal DRF allocation, and then recurse on remaining feasible tasks).</li> </ul>"},{"location":"lecture-14/#recap","title":"Recap","text":"<p>Envy and Fairness Recap</p> <ul> <li> <p>Envy-free Cake Cutting</p> <ul> <li>I-cut-you-choose mechanism: 1. Alice cuts the cake into two equal-for-her pieces, and then 2. Bob chooses his favorite of the two pieces.</li> <li>I-cut-you-choose is not strategyproof for the cutter (but it is strategyproof for the chooser).</li> <li>Definition: An allocation is envy-free if no agent envies another agent\u2019s allocation (i.e. no agent would rather have the other\u2019s allocation over their own).</li> <li>Claim: If Alice cuts the cake into 2 pieces that are equal for her (and Bob picks his favorite), the resulting allocation is envy-free.</li> </ul> </li> <li> <p>A lot of different ways to define fair allocations:</p> <ul> <li>Envy-Free (EF1/EFX): \\(v_i(A_i) \\geq v_i(A_j)\\).</li> <li>Proportional: \\(v_i(A_i) \\geq \\frac{v_i(A)}{n}\\).</li> <li>Equitable: \\(v_i(A_i) = v_j(A_j)\\).</li> <li>Perfect: \\(v_i(A_i) = \\frac{v_i(A)}{n}\\).</li> <li>\\(\\alpha\\)-Maximin Share (MMS): \\(v_i(A_i) \\geq \\alpha \\; \\underset{A^i}{max} \\; \\underset{j}{min} \\; v_i(A_j^i)\\).</li> <li>(Approximate) Competitive equilibrium from equal incomes (A-CEEI).</li> <li>Utilitarian/Social Welfare: \\(\\underset{A_1, ..., A_n}{max} \\sum_i v_i(A_i)\\).</li> <li>Nash Social Welfare: \\(\\underset{A_1, ..., A_n}{max} \\; \\prod_i \\; v_i(A_i)\\).</li> <li>Egalitarian/Maximin: \\(\\underset{A_1, ..., A_n}{max} \\; \\underset{j}{min} \\; v_i(A_i)\\).</li> </ul> </li> <li> <p>Which definition to use depends on the application:</p> <ul> <li>Allocating goods:<ul> <li>Spliddit used to maximize social welfare subject to MMS/proportionality/EF. </li> <li>They decided that NSW is better.</li> </ul> </li> <li>Allocating classes:<ul> <li>Approximate-CEEI</li> <li>Combinatorial demand</li> <li>The number of students must significantly exceed the number of classes</li> </ul> </li> <li>Allocating chores:<ul> <li>Spliddit used to assign chores by assuming they\u2019re divisible (we can use randomized rounding to assign fractional chores) and drawing new chores each week to guarantee approximate \u201cex-post\u201d fairness.</li> <li>Then they looked for an equitable solution, and optimized over all equitable solutions using Linear Programming.</li> </ul> </li> <li>Allocating computing (see example from Databrick).</li> </ul> </li> <li> <p>Dominant Resource Fairness:</p> <ul> <li>Fix an allocation of resources to users:<ul> <li>User \\(i\\) has some share (fraction) of the total capacity of each resource.</li> <li>User \\(i\\)\u2019s dominant resource is the resource with the largest share.</li> </ul> </li> <li>Theorems: DRF satisfies sharing incentives, envy-free, Pareto-optimal, strategyproof.</li> <li>Extended DRF: Find the maximal DRF allocation, and recurse on the remaining feasible tasks.</li> <li>Practical DRF: While resources are not exhausted, select user \\(i\\) with the minimum dominant share, and add their next feasible task.    </li> </ul> </li> </ul>"},{"location":"lecture-15/","title":"Course Allocation + End-of-Quarter Party","text":"<p>June 4, 2025</p>"},{"location":"lecture-15/#nostalgic-review-of-cs269i","title":"Nostalgic Review of CS269i","text":"<p>Course Goals Recap</p> <ul> <li> <p>Learn to think about incentives</p> <ul> <li>Frameworks and language: mechanisms, strategyproof, equilibria, \u2026</li> <li>Practice, practice, practice: see/understand/discuss a lot of examples</li> <li>Practice, practice, practice: ask good questions!</li> </ul> </li> <li> <p>Exposure to great ideas from economic theory: Stable matching, welfare theorem, proper scoring rules, VCG auction, Proof-of-Work</p> </li> <li> <p>Theory-vs-applications case studies</p> <ul> <li>How can theory inform practice? </li> <li>What are the gaps that you should think about before applying theory?   </li> </ul> </li> </ul>"},{"location":"lecture-15/#frameworks-and-language-mechanisms-strategyproof-equilibria","title":"Frameworks and language: mechanisms, strategyproof, equilibria, \u2026","text":"<p>Definition: Mechanism</p> <p>Soliciting inputs, running an algorithm, and taking actions.</p> <p>\"A procedure for making a decision or taking an action, as a function of what people want (i.e., of participants\u2019 preferences).\u201d</p> <p>Key point: What (agents believe) we do with agents\u2019 inputs affects these inputs.</p> <p>Example: Mechanism</p> <ul> <li>Soliciting inputs: Ranking of dorms.</li> <li>Running an algorithm: For each student in random order, find favorite available dorm.</li> <li>Taking actions: Assign rooms.</li> </ul> <p>We\u2019ve seen a LOT of mechanisms since:</p> <ul> <li>Matching mechanisms (Lectures 1-2): The Draw, Deferred Acceptance.</li> <li>Auctions (Lecture 8 + guest lecture!): 1st price, 2nd price, all-pay, GSP, VCG.</li> <li>Crowdsourcing (Lectures 9+10): Proper scoring rules, prediction markets.</li> <li>Fair division (Lecture 14): I-cut-you-choose, Dominant Resource Fairness (DRF).</li> </ul> <p>We analyzed them: the ones in bold above are strategyproof, the others are not.</p> <p>What happens when there\u2019s no dominant strategy?</p> <p>Definition: External Regret</p> <p>External Regret = \\( \\underset{i}{\\max} \\underset{t}{\\sum} r_{i,t} - \\underset{t}{\\sum} r_{ALG(t),t} \\)</p> <p></p> <p>Definition: Swap-Regret (Internal Regret)</p> <p>Swap-Regret = \\( \\underset{\\Phi : [n] \\to [n]}{\\max} \\underset{t}{\\sum} (r_{\\Phi(ALG(t)),t} - r_{ALG(t),t}) \\) where \\(\\Phi\\) maps each action to another action.</p> <p></p> <p>Algorithm: Follow The Regularized Leader (FTRL)</p> <p>On day \\(t\\), choose distribution \\(x\\) maximizing \\(\\underset{t}{\\sum} (r_{x,t} - \\frac{1}{\\eta}\\varphi(x))\\) where:</p> <ul> <li>\\(r_{x,t}\\) provides the historical performance,</li> <li>\\(\\eta\\) balances randomness and history,</li> <li>\\(\\varphi\\) is the regularizer, which penalizes unbalanced distributions.</li> </ul> <p>Definition: Nash Equilibrium</p> <p>Each player samples independently from distribution; nobody has incentive to deviate from distribution.</p> <p>Definition: Correlated Equilibrium</p> <p>A correlated distribution of actions that every player would rather follow.</p> <p>Definition: Stackelberg Equilibrium</p> <ol> <li>Follower\u2019s strategy is optimal given Leader\u2019s strategy</li> <li>Leader\u2019s commitment is optimal</li> </ol> <p>Definition: Bayesian Nash Equilibrium</p> <ul> <li>\\((Mixed = randomized)\\) strategy for each Alice.</li> <li>\\((Mixed = randomized)\\) strategy for each Bob.</li> </ul> <p>Alice and Bob drawn at random.</p> <p>Equilibrium: For each Alice, strategy is optimal in expectation over Bobs + their strategies.</p> <p>Game Theory in repeated games</p> <p>Definition: Tit-for-Tat Strategy</p> <ul> <li>In Stage 1, upload.</li> <li>In Stage \\(i\\), do whatever Marv did in Stage \\(i-1\\).</li> </ul> <p>Definition: Grim Trigger Strategy</p> <ul> <li>If there was ever a time when Marv didn\u2019t upload, don\u2019t upload.</li> <li>Otherwise, upload.</li> </ul>"},{"location":"lecture-15/#stable-matching-welfare-theorem-proper-scoring-rules-vcg-auction-proof-of-work","title":"Stable matching, welfare theorem, proper scoring rules, VCG auction, Proof-of-Work","text":"<p>Definition: Blocking Pair</p> <p>Given a match \\(M\\), the pair (doctor \\(i\\), hospital \\(j\\)) forms a blocking pair if they prefer each other to their current assignments in \\(M\\).</p> <p>Definition: Stable Matching</p> <p>A matching \\(M\\) is stable if there are no blocking pairs. Equivalently, for every unmatched pair \\((i,j)\\), either:</p> <ul> <li>Doctor \\(i\\) prefers Hospital \\(M(i)\\) over Hospital \\(j\\), or;</li> <li>Hospital \\(j\\) prefers Doctor \\(M(j)\\) over Doctor \\(i\\).</li> </ul> <p>Theorem: First Welfare Theorem</p> <p>If \\((p,M)\\) is a competitive equilibrium, then \\(M\\) is a matching that maximizes social welfare:</p> \\[ \\forall M' \\; \\underset{i}{\\sum} v_{i,M(i)} \\geq \\underset{i}{\\sum} v_{i,M'(i)} \\] <p>Definition: Vickrey-Clarke-Groves (VCG) Auction</p> <ol> <li>Solicit bidders\u2019 valuation functions </li> <li>Find welfare-maximizing allocation (assuming bids are truthful): \\(X\\).</li> <li>Same as \\(\\#2\\), but without bidder \\(i\\): \\(X^{-i}\\).</li> <li>\\(i\\) pays \\(i\\)\u2019s externality (negated): \\(\\displaystyle p_i (X) = \\sum_{k \\neq i} b_k (X^{-i}) - b_k(X)\\).</li> </ol> <p>Theorem: The VCG auction is truthful.</p> <p>Proof:</p> <p>Bidder \\(i\\)\u2019s goal is to maximize:</p> <p>\\(\\displaystyle v_i(X) - p_i(X) = v_i(X) + \\sum_{k \\neq i} b_k(X) - \\sum_{k \\neq i}b_k (X^{-i})\\)</p> <p>Note that \\(\\sum_{k \\neq i}b_k (X^{-i})\\) is not in \\(i\\)\u2019s control.</p> <p>The auction chooses the \\(X\\) that maximizes welfare, i.e. \\(\\displaystyle b_i(X) + \\sum_{k \\neq i} b_k(X)\\).</p> <p>Definition: Proof-of-Work (PoW)</p> <p>Proof-of-Work: If you find a valid nonce, it proves that you tried a lot of hashes and got lucky.</p> <p>Miners compete to find the first valid nonce and create (mine) the new block. The winner gets a reward (encoded as a transaction in the new block).</p> <p></p> <p>Miners should try to extend the longest-chain available, i.e. they should prioritize \\(b_{t+1}\\) or \\(\\hat b_t\\).</p> <p>Definition: Vanilla Proof-of-Stake (PoS) protocol</p> <p>At each iteration:</p> <ol> <li>Sample a random coin.</li> <li>Ask its owner to create the next block.</li> </ol> <p>PoS miners are directly invested in success of blockchain:</p> <ul> <li>PoW miners\u2019 stake fragile when mining tech can be repurposed.</li> <li>PoW miners\u2019 incentives somewhat misaligned (e.g. probably want higher mining rewards).</li> </ul> <p>PoW is bad for the environment (larger carbon footprint than Argentina), but it\u2019s much harder to get PoS right.</p>"},{"location":"lecture-15/#exposure-to-great-ideas-from-economic-theory","title":"Exposure to great ideas from economic theory","text":"<p>Econ Nobel prizes we covered in CS269i</p> <ul> <li>Deferred Acceptance algorithm for Stable Matching (Lecture 2).</li> <li>Nash equilibrium + Correlated equilibrium (2 prizes in Lecture 4).</li> <li>Coasian bargaining \u2013 auction off public goods (Lecture 5)\u2014This is one way for coping with Externalities.</li> <li>Market for Lemons / information asymmetry (Lecture 5)<ul> <li>At equilibrium, sellers of good cars may leave the market (only lemons left).</li> <li>Implications for grades, insurance, clickbaits.</li> </ul> </li> <li>Myerson\u2019s revenue maximizing auction (Lecture 8)\u2014Second price + reserve.</li> <li>VCG auction (HW / Bhawalkar\u2019s guest lecture)\u2014Pay your externality.</li> </ul> <p>Note that ideas from bullet points 1, 5, and 6 went into the FCC spectrum auction (Milgrom+Wilson \u201820 Nobel Prize).</p> <p>Definition: Second-Price Auction With Reserve Price</p> <ul> <li>Each bidder submits a bid \\(b_i\\).<ul> <li>Let \\(i^\\star\\) be the bidder with the highest bid \\(b_{i^\\star}\\).</li> <li>Let \\(b^{(-i)} = \\underset{j \\neq i^\\star}{max}\\) be the second-highest bid.</li> </ul> </li> <li>If \\(b_{i^\\star} \\geq p(D)\\), then \\(i^\\star\\) receives the items and pays \\(max{b^{(-i)}, p(D)}\\).</li> <li>If \\(b_{i^\\star} &lt; p(D)\\), then the seller keeps the item and no payment occurs.</li> </ul> <p>This generalizes the optimal auction for single buyers to \\(n\\) buyers. This also generalizes the second-price auction if we set the reserve price, i.e. \\(p(D)\\), to \\(0\\) or negative infinity.</p> <p>This is equivalent to the seller also participating in the auction, with the seller\u2019s bid being the reserve price, i.e. \\(p(D)\\).</p> <p>This means that it is strategyproof for the buyers, but it is not exactly strategyproof for the seller, because they have to figure out the optimal reserve price.</p>"},{"location":"lecture-15/#theory-vs-applications-case-studies","title":"Theory-vs-Applications Case Studies","text":"<p>DA Applications Recap</p> <ul> <li>In theory: DA in theory is Pareto-optimal among all matchings, doctor-optimal and hospital-worst among stable matchings, and doctor-strategyproof but not not hospital-strategyproof.</li> <li>In practice: DA is expensive (collecting preferences is costly, e.g. holistic admissions in US colleges and interview in hospitals) and preferences may not be captured by the model (e.g. matching couples).</li> </ul> <p>Theorem: In DA, \u201csafety choices\u201d are never \u201csafer\u201d.</p> <p>Market equilibria: Theory (Optimal Welfare) vs Practice (Market Failures)</p> <p>The vanilla assumption at the foundation of classical microeconomics is that a free market (\"invisible hand\") naturally converges to an optimal outcome.</p> <p>A market failure occurs when a market fails to converge to an optimal outcome.</p> <p>It is important to understand what can go wrong in market design. We have seen five types of market failures:</p> <ol> <li>Externalities and public goods</li> <li>Transaction costs</li> <li>Market thinness</li> <li>Timing issues</li> <li>Information asymmetry</li> </ol> <p>Mitigation strategies exist for all five types of market failures.</p> <p>A Few More Points on Big Miners in Practice</p> <p>In 2014, the GHash.io mining pool exceeded 51% of the Bitcoin hashrate. Instead of executing a 51% attack, they encouraged miners to leave the pool. Why? In order to prevent (actually to stop) a drop in the value of Bitcoin.</p> <p>For smaller coins (with a lower total hashrate), this is a bigger issue. If there is a lower barrier to reach 51% of the hashrate, and attackers do not care about a drop in value of a small coin, they can divert a lot of mining power for a short time period. Allegedly, 51% attacks happened (multiple times) to \u201cEthereum Classic.\u201d</p> <p>Examples: Some fair allocations in practice</p> <ul> <li>Allocating goods:<ul> <li>Spliddit used to maximize social welfare subject to MMS/proportionality/EF. </li> <li>They decided that NSW is better.</li> </ul> </li> <li>Allocating classes:<ul> <li>Approximate-CEEI</li> <li>Combinatorial demand</li> <li>The number of students must significantly exceed the number of classes</li> </ul> </li> <li>Allocating chores:<ul> <li>Spliddit used to assign chores by assuming they\u2019re divisible (we can use randomized rounding to assign fractional chores) and drawing new chores each week to guarantee approximate \u201cex-post\u201d fairness.</li> <li>Then they looked for an equitable solution, and optimized over all equitable solutions using Linear Programming.</li> </ul> </li> <li>Allocating computing (see upcoming example from Databricks).</li> </ul>"},{"location":"lecture-15/#practice-practice-practice-ask-good-questions","title":"Practice, Practice, Practice: Ask Good Questions!","text":"<p>Thanks to guest speakers Eric, Geoff, Kshipra:</p> <ul> <li>How to train an AI that doesn\u2019t lie (Eric Neyman)</li> </ul> <p></p> <ul> <li>Incentives in Sponsored Search Auctions (Kshipra Bhawalkar)</li> </ul> <p></p> <ul> <li>Incentive (Mis)alignment in Blockchain Exchanges (Geoff Ramseyer)</li> </ul> <p></p>"},{"location":"lecture-15/#course-allocation","title":"Course Allocation","text":"<p>This is a chance to learn a bit about Aviad's own research that connects to a lot of the topics we saw throughout the quarter.</p> <p>The Course Allocation Problem</p> <p>Most discussion/project/sports classes have enrollment caps. This is extremely important (\u201ccareer-trajectory changing\u201d) in MBA, law schools, etc. Note that CS269i, like many lecture-based CS classes, is (essentially) uncapped. How should we allocate limited seats in courses to students?</p> <p>Serial Dictatorship</p> <ul> <li>At Stanford (pre Fall 2023): First-Come-First-Serve, i.e. how quickly students could connect to Axess.</li> <li>When Aviad was an undergrad (at Technion): by seniority</li> <li>At Stanford (now): Mix of FCFS and seniority.</li> </ul> <p>This is strategyproof, and Pareto optimal, but not fair: for room assignment, someone always gets a better room.</p> <p>Contrast with Serial Dictatorship: The top dictator gets all their favorite classes (including ones they\u2019re just shopping for). Can we at least satisfy EF1 (enfy-free-up-to-1-good)?</p> <p>Fake money all-pay auction</p> <ul> <li>Used in Aviad's masters program (Tel-Aviv University)</li> <li>Each student gets a budget of 100 pts, and bids on various courses</li> <li>Why all-pay? (Far from strategyproof):<ul> <li>Losing a real-money auction (I keep the money I don't spend)</li> <li>Losing a fake-money auction (I keep fake money I don't spend)</li> </ul> </li> <li>Difficult to express complex preferences, such as \u201cI want either CS269i or CS256\u201d </li> </ul> <p>Competitive equilibrium from equal incomes</p> <ul> <li>Give each agent a budget of 1 (fake money); </li> <li>Find prices (for courses) + allocation (schedules) such that:<ul> <li>Every course is either exactly full or [0-priced and under-demanded].</li> <li>Every student allocated favorite schedule can afford given budget constraint.</li> </ul> </li> <li>Note: with fake money, agents maximize value subject to budget constraint. This is different from Lecture 6 where they maximized utility = value-price.</li> <li>\u201cStrategyproof in the large\u201d, i.e. approximately strategyproof for large markets. The intuition is:<ul> <li>You get optimal schedule given prices.</li> <li>In a large market, each student a has small effect on prices.</li> </ul> </li> <li>Parero optimal.</li> <li>Fair (\u201cequal budgets\u201d \\(\\rightarrow\\) envy-free).</li> <li>Caveat: such a CEEI often doesn\u2019t exist!</li> </ul> <p>Approximate competitive equilibrium from equal incomes (A-CEEI)</p> <ul> <li>Give each agent budget of approximately 1 (fake money).</li> <li>Allocation only required to approximately clear markets.</li> <li>Always exists! (some tradeoff in approximations of equal incomes vs market clearing).</li> <li>Approximately strategyproof (\u201cSP-L: strategyproof-in-the-large\u201d).</li> <li>Approximately Pareto optimal.</li> <li>Approximately fair (\u201cEF1: envy-free up-to-1 good\u201d).</li> <li>Always exists.</li> <li>Caveat: can we find it algorithmically?<ul> <li>Computation in theory: an intractable problem (in the worst case).</li> <li>Computation in practice: real instances tend to be easier: <ul> <li>Used in several MBA programs (medium size: 500~3000 students).</li> <li>Algorithm may take a few days (with a lot of compute), but only need to do this once every semester.</li> <li>With Ruiquan Gao and Qianfan Zhang (undergrads at the time), Aviad found a much faster heuristic algorithm\u2014See Practical algorithms and experimentally validated incentives for equilibrium-based fair division (A-CEEI) paper.</li> <li>With that fast new heuristic algorithm we can go back and look at incentives:<ul> <li>In theory: Strategyproof \u201cin the large\u201d\u2013need a number of students greater than the number of atoms in universe.</li> <li>In practice: use the fast algorithm to search for strategic manipulations. Aviad, Ruiquan, and Qianfan's algorithm found some surprising manipulations, and they used insights to create a new algorithm that is \u201cmore strategyproof\u201d.</li> </ul> </li> </ul> </li> </ul> </li> </ul> <p>Punchline: It is important to think about incentives and algorithms together.</p>"},{"location":"lecture-15/#parting-words","title":"Parting Words","text":"<p>We are leaving this class empowered with a lot of useful knowledge. Aviad can\u2019t wait to see how we use these ideas 1/3/10 years from now. We should let him know when we do!</p> <p>We should also remember that with power comes great responsibility, and use our knowledge to make our world better.</p>"},{"location":"lecture-2/","title":"Stable Two-Sided Matchings","text":"<p>April 2, 2025</p> <p>After medical school, med students start their internship called a \"residency.\" Each (prospective) doctor has preferences over hospitals, and each hospital has preferences over doctors. How should doctors and hospitals be matched?</p> <p>Key Nuances:</p> <ul> <li>The biggest difference between doctor-residency matching and student-dorm matching is that we now have two-sided preferences (each doctor and hospital have preferences over each other).</li> <li>Matching students to dorms is a centralized process, while matching doctors to hospitals requires incentivizing participants to use a centralized process to prevent side deals.</li> </ul>"},{"location":"lecture-2/#stable-matching","title":"Stable Matching","text":"<p>Definition: Blocking Pair</p> <p>Given a match \\(M\\), the pair (doctor \\(i\\), hospital \\(j\\)) forms a blocking pair if they prefer each other to their current assignments in \\(M\\).</p> <p>For example, if Doctor \\(n\\) prefers Stanford over UCSF and Stanford prefers Doctor \\(n\\) over Doctor 1, who is currently matched, then (Doctor \\(n\\), Stanford) forms a blocking pair. This results in an Unstable Matching.</p> <p>Note: Blocking pairs exist only in two-sided matching markets. In a 1-sided matching, we cannot have blocking pairs, because in a blocking pair, both participants need to prefer each other. In the Stanford Undergraduate Housing Problem, even though students have preferences over dorms, dorms do not have preferences over students.</p> <p>Definition: Stable Matching</p> <p>A matching \\(M\\) is stable if there are no blocking pairs. Equivalently, for every unmatched pair \\((i,j)\\), either:</p> <ul> <li>Doctor \\(i\\) prefers Hospital \\(M(i)\\) over Hospital \\(j\\), or</li> <li>Hospital \\(j\\) prefers Doctor \\(M(j)\\) over Doctor \\(i\\).</li> </ul> <p>Key Point: Stability removes incentives to deviate from the centralized matching. For instance, if you have a stable matching between doctors and hospitals, then neither doctors nor hospitals have any incentive to deviate from the matching and match outside of the process. They might as well stay in the centralized matching, because they cannot get anything better outside of the matching process.</p>"},{"location":"lecture-2/#deferred-acceptance","title":"Deferred Acceptance","text":"<p>The Deferred Acceptance Algorithm is an algorithm that finds stable matchings.</p> <p>Main idea: Each doctor proposes to their favorite hospital that hasn't rejected them yet. Hospitals accept the best available candidate. If we discover a blocking pair, we switch the matching.</p> <p>Mechanism: Deferred Acceptance</p> <p>While there's an unmatched doctor \\(i\\):</p> <ul> <li>Doctor \\(i\\) proposes to their next-favorite hospital \\(j\\).</li> <li>If hospital \\(j\\) has no match, they accept doctor \\(i\\).</li> <li>Else, if hospital \\(j\\) prefers their current match over doctor \\(i\\), doctor \\(i\\) remains unmatched.</li> <li>Else, hospital \\(j\\) matches with doctor \\(i\\), releasing their previous match.</li> </ul> <p>The algorithm stops when everyone is matched.</p> <p>Example</p> <p>Consider the following agents and their respective preferences:</p> <ul> <li>Doctors: \\(Alice (X, Y, Z)\\), \\(Bob (Y, X, Z)\\), \\(Charlie (Y, Z, X)\\).</li> <li>Hospitals: \\(X (Bob, Alice, Charlie)\\), \\(Y (Alice, Bob, Charlie)\\), \\(Z (Bob, Charlie, Alice)\\).</li> </ul> <p>Stable matching result: \\((Alice, X), (Bob, Y), (Charlie, Z)\\).</p> <p>Note: No matter the order chosen to process the doctors, we always find the stable matching.</p> <p>Theorem: Runtime of Deferred Acceptance</p> <p>Deferred Acceptance runs in \\(O(n^2)\\) time.</p> <p>This is important, because when inputs are of non-trivial size, we want to make sure the algorithm is efficient.</p> <p>Proof:</p> <ul> <li>There are at most \\(n^2\\) proposals (each doctor proposes at most to \\(n\\) hospitals).</li> <li>Each proposal is \\(O(1)\\).</li> <li>So, the total runtime is \\(O(n^2)\\).</li> </ul> <p>Why do we have at most \\(n^2\\) iterations? A doctor never proposes to the same hospital twice (we never try to match the same pair of doctor-hospital twice), because we are going down the list of preferences. So, if there are \\(n\\) doctors and \\(n\\) hospitals, then there are \\(n^2\\) possible doctor-hospital pairs. This is why we have at most \\(n^2\\) iterations (there are instances where it really takes \\(n^2\\) time).</p> <p>Why is each proposal \\(O(1)\\)? We assume that doctor \\(i\\) already has a ranked list of preferences (how we get the preferences is non-trivial but outside of this algorithm). On every iteration, all we do is advance to the next hospital in the list of a doctor\u2019s preferences (for instance, in the case of \\(Bob\\) above, we moved from Hospital \\(Y\\) to Hospital \\(X\\)). When a hospital needs to determine whether it prefers its current match \\(i'\\) over \\(i\\) (or not), it can use an array with the preferences of doctors (ranked): this way, it can compare the ranks in \\(O(1)\\) time.</p> <p>Theorem: Deferred Acceptance is Stable.</p> <p>Given \\(n\\) doctors and \\(n\\) hospitals, Deferred Acceptance outputs a complete stable matching.</p> <p>Corollary: A stable matching exists.</p> <p>(This is not obvious!)</p> <p>Proof (outline):</p> <p>We prove that Deferred Acceptance outputs a complete stable matching with three claims:</p> <ol> <li>The current match is stable at each iteration.</li> <li>Once matched, hospitals remain matched.</li> <li>At completion, everyone is matched.</li> </ol> <p>Therefore, no blocking pairs exist.</p> <p>How does this proof work? Claim 1 states that we have already matched some doctors and hospitals, and we assume that the matching so far is stable. Claim 2 states that once a hospital is matched, it may be matched to another doctor, but it never gets unmatched (however, doctors can get unmatched). Claim 3 states that no doctor, and no hospital, is unmatched at the end of the matching. Since we know that the matching is stable after each iteration, and there is no one left unmatched at the end, then the matching at the end is stable.</p> <p>Proof of Claims:</p> <ul> <li>Claim 1: We assume that doctor \\(d\\) and hospital \\(h\\) are currently matched to other matches and they are a blocking pair. This means that \\(d\\) is matched to a worse hospital, so we must have tried to match \\(d\\) to \\(h\\), and we must have gone down the preference list, either because \\(h\\) refused to match to \\(d\\), or because \\(h\\) preferred another doctor. This means that \\(h\\) must have already matched with someone better than \\(d\\). This is a contradiction with the fact that \\((d,h)\\) is a blocking pair.</li> <li>Claim 2: If you look at the pseudo code of the algorithm, there is simply no command to unmatch a hospital.</li> <li>Claim 3: If we have a free doctor, then we need to have a free hospital as well. However, if we reach the end of the algorithm, \\(d\\) already tried to propose to \\(h\\), but after that step, \\(h\\) is matched (either because \\(d\\) matches with \\(h\\), or because \\(h\\) is already matched to another doctor). By Claim 2, we know that a hospital stays matched until the end of the algorithm, so \\(h\\) cannot be free. This is a contradiction to \\(h\\) being free at the end of the algorithm (we cannot have a \\((d,h)\\) pair free).</li> </ul>"},{"location":"lecture-2/#deferred-acceptance-as-a-mechanism","title":"Deferred Acceptance as a Mechanism","text":"<p>We know that Deferred Acceptance always finds a stable matching. Is it optimal? What does optimality mean?</p> <p>Theorem: Efficiency of Stable Matchings.</p> <p>Every stable matching is Pareto-optimal.</p> <p>Reminder: An assignment \\( A \\) is Pareto-optimal if for any other assignment \\( B \\), there is a participant that (strictly) prefers \\( A \\) over \\( B \\).</p> <p>Proof: Any deviation from a stable matching worsens at least one participant\u2019s outcome.</p> <p>Note: If we take the stable matching, and any other matching (whether it is stable or not), there are going to be some doctors and/or some hospitals that prefer the stable matching over that other matching.</p> <p>Theorem: The matching returned by the Deferred Acceptance mechanism is doctor-optimal.</p> <p>In other words, every doctor is matched to their favorite hospital possible in any stable matching.</p> <p>If we have multiple stable matchings, which one is the best one? </p> <p>Doctor-optimality means that every doctor gets the best hospital they can possibly get out of any stable matching. When we talked about the student-dorms 1-sided matching, it was possible that some students were happier, while others were sadder, depending on the matching (we had some trade-offs). However, here, among all stable matchings, the matching that DA outputs is the best for every single doctor simultaneously. An implication of this theorem is that the Deferred Acceptance mechanism is also doctor-strategyproof.</p> <p>Corollary: Deferred Acceptance is doctor-strategyproof.</p> <p>Doctors cannot gain from misreporting their preferences.</p> <p>Theorem: The matching returned by the Deferred Acceptance mechanism is hospital-worst.</p> <p>In other words, every hospital is matched to their least favorite doctor possible in any stable matching.</p> <p>Corollary: Deferred Acceptance is NOT hospital-strategyproof.</p> <p>Hospitals can benefit from misreporting their preferences.</p> <p>Example</p> <p>Consider the following agents and their respective preferences:</p> <ul> <li>Doctors: \\(Alice (X, Y, Z)\\), \\(Bob (Y, X, Z)\\), \\(Charlie (Y, Z, X)\\).</li> <li>Hospitals: \\(X (Bob, Alice, Charlie)\\), \\(Y (Alice, Charlie, Bob)\\), \\(Z (Bob, Charlie, Alice)\\).</li> </ul> <p>Stable matching result: \\((Alice, Y), (Bob, X), (Charlie, Z)\\).</p> <p>By changing their preferences over \\(Bob\\) and \\(Charlie\\) (compared to the previous example setup), Hospital \\(Y\\) gets matched to \\(Alice\\) (their top choice).</p> <p>Question 1: Does a hospital need to know the preferences of the other hospitals over the doctors to game the system?</p> <p>Yes, Hospital \\(Y\\) needs to know the preferences of all other hospitals over all other doctors to game the system, otherwise, it is pretty hard to figure out how to game the system on the hospital side. Aviad does not know of a good strategy for hospitals to game the system.</p> <p>Question 2: If a doctor ranks their safety choices higher, does it help them?</p> <p>No. For instance, if a doctor prefers Stanford, but understands that their second favorite hospital may be less competitive (i.e. rank them higher as a doctor), it is still not in their best interest to rank that second hospital higher than Stanford in their preference list, because the DA is completely doctor-strategyproof.</p> <p>Question 3: Why is the DA mechanism hospital-worst?</p> <p>Because we are processing things in order of doctor preferences, and we are only using hospital preferences for tie-breaking purposes.</p> <p>Question 4: Does Pareto optimality change if we allow participants to have non-strict preferences?</p> <p>In practice, doctors do not rank all possible hospitals, and hospitals do not rank all possible doctors. The theorem statements do not hold exactly in practice (in particular for Pareto optimality if we have non-strict preferences).</p> <p>Question 5: As a doctor, if you know that a hospital is misrepresenting their preferences, is it still doctor-stragegyproof?</p> <p>From the doctor\u2019s perspective, the hospitals submit some ranked lists of doctors, but they don\u2019t know whether it is truthful or not, so this is still doctor-stragegyproof. However, if a doctor can influence how a hospital reports their preferences, then they can game the system.</p>"},{"location":"lecture-2/#deferred-acceptance-in-practice","title":"Deferred Acceptance in Practice","text":"<p>Why don\u2019t we use a DA mechanism for undergrad admissions in the US? </p> <p>In the US, the system is very decentralized, and this is a challenge for schools, because they need to admit a number of students without knowing how many are actually going to enroll\u2014unlike hospitals, which generally have 1 or 2 residency positions available, so if none shows up, they don\u2019t have a doctor, and if 5 show up, they cannot pay everyone\u2014and these are generally large numbers.</p> <p>Also in the US, applications are holistic, which costs money, so application fees limit the number of universities students apply to.</p> <p>In contrast, in other countries where standardized tests scores are used instead of holistic reviews, DA mechanisms can be used (such as in Brazil, China, Hungary).</p> <p>How do doctors rank hospitals?</p> <ul> <li>1950s: DA-like algorithms initially used. At the time, there were very few female doctors (not to mention openly gay doctors).</li> <li>1960s: Couples complicate preferences. More couples want to be near each other, i.e. doctors no longer have a ranked preference over hospitals.</li> <li>1980s: Negative theory results on stable matching with couples. A stable matching may not exist: it is possible that there is no stable matching when you try to match couples to the same hospitals in the same cities. Deciding if a stable matching exists is an NP-complete computational problem (it is, in theory, intractable).</li> <li>1990s: Extended DA for couples adopted.</li> </ul> <p>How do hospitals rank doctors?</p> <ul> <li>Interviews (costly process): hospitals strategize over which doctors to interview (\"safety choices\").</li> <li>Standardized Tests (probably not any more: USMLE switched to pass/fail in January 2022).</li> <li>Letters of recommendation, medical school evaluation, personal statement, CV.</li> </ul> <p>Theorem: In DA, using safety choices is never safer for hospitals.</p> <p>Formally, manipulating true ranks to move \\(i-th\\) doctor (\"safety choice\") to a higher position cannot help a hospital match with a doctor of rank \\(i-or-better\\).</p> <p>Hospitals also use \"safety choices\" after interviews: they may not rank first the candidates that they evaluate as \u201ctoo good to come here.\u201d This is particularly interesting since we have seen in doctor-proposing DA that ranking safety choices higher is never safer for hospitals (this is provable).</p> <p>Proof:</p> <ul> <li>Until doctor \\(i\\) tries to match with a hospital, manipulation has no effect.</li> <li>After doctor \\(i\\) tries to match with a hospital, they can only be replaced by a better doctor.</li> </ul> <p>Yet, hospitals still strategically use \"safety choices\" when ranking doctors post interview. Why?</p> <ul> <li>One possible explanation is ignorance of matching mechanisms and their properties (they did not take CS269I).</li> <li>Another possible explanation is that they consider their reputation/ego. After the matching is complete, the organization that handles the matching publishes the \u201cnumber needed to fill\u201d metric, which is the lowest ranking a hospital had to go to fill their positions. This is helpful for doctors the following year to get a sense of how competitive a program is, which is therefore also a signal of how prestigious a program is. For instance, Stanford probably don\u2019t have to be turned down by many doctors, so their number is low, while less prestigious hospitals may need to go through more doctors and have a higher number. If a hospital ranks higher doctors that they think will join them, they can reduce their \u201cnumber needed to fill\u201d metric and appear more prestigious.</li> </ul> <p>Other DA Applications:</p> <ul> <li>Routing Network Packets: Suppose that instead of doctors and hospitals, you want to match packets to servers on the internet. When you own all the servers, you don't have to worry about them matching outside of your algorithm. This would apply to a company like Akamai, who owns a lot of servers\u2014and it actually works better in practice than in theory (because DA is very fast in practice). Packets typically get one of the top servers, so preferences lists are truncated, and the total running time is closer to \\(O(n)\\). Given the highly distributed nature of packet routing, every packet looks for its own server. </li> <li>Stanford Marriage Pact: Matches are made between Stanford students who want to make a pact, i.e. \"If we don't get married by time X, we will marry each other.\" DA is not used anymore (they use something closer to max weight matching instead) because in the 21st century, the graph is not bipartite, due to people having many different preferences. Stability is not a very important part of the requirement, because in practice, Stanford students spend their time looking for a partner outside of the pact/matching.</li> </ul>"},{"location":"lecture-2/#recap","title":"Recap","text":"<p>Recap:</p> <ul> <li>In theory: DA is Pareto-optimal among all matchings, doctor-optimal and hospital-worst among stable matchings, and doctor-strategyproof but not hospital-strategyproof.</li> <li>In practice: DA is expensive (collecting preferences is costly, e.g. holistic admissions in US colleges and interview in hospitals) and preferences may not be captured by the model (e.g. matching couples).</li> </ul>"},{"location":"lecture-3-2023/","title":"Top Trading Cycles","text":"<p>January 18, 2023</p> <p>Note: This lecture was skipped during the Spring 2025 quarter. Below are notes from the Winter 2023 quarter.</p> <p>Previously, we studied:</p> <ul> <li>Random Serial Dictatorship (one-sided matching)</li> <li>Deferred Acceptance (two-sided matching)</li> </ul> <p>This lecture: What happens when participants are already endowed with goods?</p> <p>Example: Stanford PhD housing renewal\u2014students face tradeoffs between renewal and lottery entry.</p> <p>Problem Setup:</p> <ul> <li>\\(n\\) students, \\(n\\) rooms</li> <li>Each student has a current room and preference over all rooms</li> </ul> <p>Mechanism: Top Trading Cycles (TTC)</p> <p>While there are unmatched students/rooms:</p> <ol> <li>Create a graph with unmatched rooms and students as nodes.</li> <li>Draw edges:</li> <li>from each room to its current owner.</li> <li>from each student to their most preferred available room.</li> <li>Identify cycles, remove them, and execute the trades indicated by cycles.</li> </ol> <p>Theorem: Runtime of TTC</p> <p>Top Trading Cycles runs in \\(O(n^2)\\) time.</p> <p>Proof: Constructing graph and edges is \\(O(n)\\). Finding cycles via directed edges visits at most \\(2n+1\\) nodes (pigeonhole principle), thus \\(O(n)\\). Each iteration removes at least one student; thus, overall complexity is \\(O(n^2)\\).</p> <p>Input size is \\(n^2\\) (preference lists of length \\(n\\)), so TTC is linear relative to input size.</p> <p>Definition: Individual Rationality (IR)</p> <p>A mechanism is individually rational if no agent is worse off after participating.</p> <p>Theorem: TTC is Individually Rational</p> <p>Proof: Students trade rooms only if they prefer the new room.</p> <p>Theorem: Strategyproofness of TTC</p> <p>Top Trading Cycles is strategyproof.</p> <p>This proof was noted as flawed; a corrected proof was expected.</p> <p>Theorem: Efficiency of TTC</p> <p>TTC allocation is Pareto-optimal.</p> <p>Proof: Equivalent to serial dictatorship in order cycles formed. Serial dictatorship is Pareto-optimal, thus TTC is Pareto-optimal.</p>"},{"location":"lecture-3-2023/#top-trading-cycles-and-chains-ttcc","title":"Top Trading Cycles and Chains (TTCC)","text":"<p>Graduating students and new students (without rooms) require modifications:</p> <p>Mechanism: Top Trading Cycles with Chains (TTCC)</p> <ol> <li>Process students in random order.</li> <li> <p>Mark student \\(i\\) visited:</p> </li> <li> <p>If \\(i\\)'s top room is unoccupied, assign room, release previous.</p> </li> <li>If room occupied by unvisited student \\(j\\), move \\(j\\) ahead of \\(i\\).</li> <li>If room occupied by visited student \\(j\\), cycle identified.</li> </ol> <p>TTCC maintains strategyproofness, Pareto-optimality, individual rationality, and efficiency.</p>"},{"location":"lecture-3-2023/#ttcc-in-practice","title":"TTC(C) in Practice","text":"<p>Why isn't TTCC common?</p> <ul> <li>Running TTCC repeatedly can break strategyproofness over multiple years.</li> <li>Increased strategic behavior over \"popular\" rooms.</li> </ul> <p>Application: School choice (New Orleans, 2011-12) used TTC but switched to DA (Deferred Acceptance) due to simplicity in explanation.</p> <p>College Admissions: Universities prefer specific applicants; direct trades don't apply.</p> <p>Kidney Transplants: Kidney exchange involves patient-donor compatibility issues and logistical constraints.</p> <p>Considerations in Kidney Exchange:</p> <ul> <li>Strategyproofness less critical (compatibility-based).</li> <li>Priority considerations (health urgency).</li> <li>Stability and central mechanism incentivization.</li> <li>Logistical challenge: Long cycles require simultaneous transplants; chains manageable.</li> <li>Dynamic arrivals/departures.</li> <li>Strategic hospitals may internally match.</li> <li>High failure probability (93% matches fail).</li> <li>Ethical issues and multi-organ exchange possibilities.</li> </ul>"},{"location":"lecture-3/","title":"Online Learning and Regret Minimization","text":"<p>April 9, 2025</p> <p>So far in this class, agents had dominant strategies, such as doctor reporting their true preferences in DA, so we had a good guess of what they should do. However, what should agents do without dominant strategies or even knowledge of the game's rules? In other words, what should selfish agents do when the mechanism is not strategyproof, and possibly not fully specified? Let's explore this question from a single agent's perspective.</p>"},{"location":"lecture-3/#regret-minimization","title":"Regret Minimization","text":"<p>If we know which stocks are going to go up and down, we should buy the stocks that are going to go up and sell them before they are going to go down. But what do we do when we don\u2019t know that? We want an algorithm that will explain how to invest well in the stock market. First, we will try to use historical performance of stocks to determine what to do tomorrow.</p> <p>Scenario 1</p> <p>Consider investing in stocks without knowledge of future performance:</p> <ul> <li>There are \\(n\\) possible actions (stocks).</li> <li>We run an algorithm over \\(T\\) days.</li> <li>On day \\(t\\), the algorithm picks action \\(ALG(t)\\).</li> <li>Reward for action \\(i\\) on day \\(t\\) is \\(r_{i,t}\\), bounded by \\(-1 \\leq r_{i,t} \\leq 1\\).</li> </ul> <p>Our goal is to maximize the sum of the rewards of the actions the algorithm chooses, namely \\(\\sum r_{ALG(t),t}\\).</p> <p>Key Idea #1: Use historical performance to inform decisions.</p> <p>A pretty natural guess is to try a greedy algorithm, called Follow The Leader, where on every day, we just take the action that has generated the most reward so far.</p> <p>Algorithm: Follow The Leader (FTL)</p> <p>On day \\(t\\), take the action with the highest total reward up to day \\(t-1\\). Break ties arbitrarily.</p> <p>How can we reason about whether this first idea is a good idea or not?</p> <p>We measure the success of online learning algorithms in terms of regret. Our benchmark is a fixed action over time: we are comparing with the best stock overall, not the best stock every single day. The intuition is that, if the algorithm has low regret, then we are doing almost as well as the best stock.</p> <p>Definition: External Regret</p> <p>External Regret = \\( \\underset{i}{\\max} \\underset{t}{\\sum} r_{i,t} - \\underset{t}{\\sum} r_{ALG(t),t} \\).</p> <p>Question: Can regret be negative? Yes! That is a fantastic case, when we are doing better than the best stock.</p> <p>So, is FTL (i.e. the greedy algorithm that takes the best action every time) a good idea?</p> <p>Claim 1: For independently, identically distributed (iid) rewards, FTL's expected regret is \\(O(\\sqrt{T \\log(n)})\\).</p> <p>In other words, if, for each action, the rewards on different days are independently, identically distributed, aka iid (i.e. every day, the rewards are drawn from the same distribution\u2014some stocks tend to do better, some stocks tend to do worse), then FTL has an expected regret in the order of \\(O(\\sqrt{T \\log(n)})\\).</p> <p>What does this mean?</p> <p>It means that, as long as \\(T\\) is much bigger than \\(\\log(n)\\), the whole thing inside the square root is going to be less than \\(T^2\\) (because \\(\\log(n)\\) is less than \\(T\\), so \\(T \\log(n)\\) is less than \\(T \\cdot T\\), so the whole thing is less than \\(T^2\\)).</p> <p>Regret is less than 1 on average, so regret is less than the number of days, and regret per day is diminishing (going to 0).</p> <p>For instance, the NYSE only lists 2,000 companies, and \\(\\log(2000) \\approx 11\\), so it takes about \\(11\\) days to start doing as well as the best stock on the NYSE (we can replace \\(11\\) days with \\(11\\) seconds if we are trading really fast).</p> <p>Claim 2: No algorithm can outperform \\(\\Omega(\\sqrt{T \\log(n)})\\) with iid rewards.</p> <p>Why is this claim true?</p> <p>Let\u2019s assume that the reward for every action, every day, is just a random coin flip (\\(+1\\) or \\(-1\\)), completely independently at random. So, in any algorithm based on history, the next choice is going to be a random coin flip.</p> <p>However, if we have \\(n\\) actions, one of them is going to do a little bit better than average (each one has a \\(50/50\\) expectation, but one of them is going to be a little bit better than average). If we do the math of how much better than average it is going to be, it comes out to roughly this magic number of \\(O(\\sqrt{T \\log(n)})\\).</p> <p>The number \\(O(\\sqrt{T \\log(n)})\\) itself is not super important. However, the intuition is that it is the right bound: \\(O(\\sqrt{T \\log(n)})\\) converges very fast, and as long as \\(T\\) is much bigger than \\(\\log(n)\\), since \\(\\log(n)\\) is a tiny number, the algorithm is doing really well.</p> <p>Key Idea #2: External regret is the difference between the algorithm's total reward and the reward from the single best-in-hindsight action, i.e. </p> <p>External Regret = \\( \\underset{i}{\\max} \\underset{t}{\\sum} r_{i,t} - \\underset{t}{\\sum} r_{ALG(t),t} \\).</p>"},{"location":"lecture-3/#regret-minimization-algorithms","title":"Regret Minimization Algorithms","text":"<p>So, we are doing almost as well as the best stock. The catch is that the stock market is not iid: the stocks may go up one day, two days, three days, but they can only go up so much.</p> <p>In an adversarial context, it is as if someone is trying to make it hard for us. FTL does not do well with adversarial input. In fact, no deterministic algorithm does well with adversarial input.</p> <p>Key Idea #3: To minimize regret in an adversarial context, we introduce randomness in the algorithm.</p> <p>However, choosing actions uniformly at random may also be a bad idea, because some actions are consistently worse than others.</p> <p>Key Idea #4: We need an algorithm with a good balance between having enough randomness and paying attention to historical performance.</p> <p>One idea to minimize regret is to pick a distribution of actions rather than a pure action.</p> <p>Algorithm: Follow The Regularized Leader (FTRL)</p> <p>On day \\(t\\), choose distribution \\(x\\) maximizing \\(\\underset{t}{\\sum} (r_{x,t} - \\frac{1}{\\eta}\\varphi(x))\\) where:</p> <ul> <li>\\(r_{x,t}\\) provides the historical performance,</li> <li>\\(\\eta\\) balances randomness and history,</li> <li>\\(\\varphi\\) is the regularizer, which penalizes unbalanced distributions.</li> </ul> <p>How does \\(\\varphi\\) work?</p> <p>\\(\\varphi\\) is a (usually convex) function that \u201cpenalizes\u201d unbalanced distributions. For instance, if we have a distribution that puts all the weight on one stock, then \\(\\varphi\\) is going to determine that it is a bad distribution and give it a bad score. We can pick different functions that are going to give different performance.</p> <p>Now, let's consider another famous algorithm.</p> <p>Algorithm: Multiplicative Weight Update (MWU)</p> <ul> <li>Initialize weights \\(z_{i,0} = 1\\) for all \\(i\\).</li> <li>At day \\(t\\):<ul> <li>Choose action \\(i\\) with probability \\(\\frac{z_{i,t}}{\\underset{j}{\\sum} z_{j,t}}\\).</li> <li>Update weights: \\(z_{i,t+1} \\leftarrow z_{i,t} e^{\\eta r_{i,t}}\\).</li> </ul> </li> </ul> <p>The idea is that, at the beginning, all the weights are going to be 1. Each day, we are going to choose an action with a probability proportional to the weight. Then, there are updates: we multiply the weight of each action by the reward the action got. If an action got a big positive reward, we are multiplying its weight by a big number, and its weight next time is going to be bigger. If an action got a small or negative reward, we are multiplying its weight by a small number, and its weight next time is going to be smaller.</p> <p>\\(\\eta\\) is called \u201clearning rate\u201d. It is a parameter that balances between randomness and FTRL:</p> <ul> <li>If we have a higher \\(\\eta\\), it is putting a lot of weight into learning really fast (fitting to historical performance).</li> <li>If we have a smaller \\(\\eta\\), we are staying very close for a long time to the original distribution, which is uniform over all actions, so it is more random.</li> </ul> <p>Although we are not going to prove this claim in lecture, it turns out that this Multiplicative Weight Update (MWU) algorithm is the same as FTL when using the entropy regularizer.</p> <p>Claim: MWU = FTRL with entropy regularizer \\(\\varphi(x) = - \\sum x_i \\log(x_i) \\).</p> <p>The point is that these are two ways to look at the same algorithm, trying to balance randomness and historical performance to avoid an adversarial example.</p> <p>Theorem: MWU and FTRL achieve expected regret \\(O(\\sqrt{T \\log(n)})\\).</p> <p>This is optimal, even against adversarial input. As long as the adversary does not know the inside randomness of the algorithm, this algorithm is completely adversary proof.</p>"},{"location":"lecture-3/#swap-regret-minimization-algorithms","title":"Swap-Regret Minimization Algorithms","text":"<p>So far, we have formalized the idea of measuring online algorithms with regret, and we have seen actual optimal algorithms that minimize regret even against very adversarial input. However, what happens if there is an adversary who can look at our algorithm\u2019s choices, and use that to do better than us? This is embarrassing: how can we avoid that?</p> <p>In Judo, winning is not about our own strength, but about using our opponent\u2019s strength to make them fall over. Similarly, in the stock market, winning is not about knowing the market ourselves, but instead using our opponent\u2019s power to beat them.</p> <p>Definition: Swap-Regret (Internal Regret)</p> <p>Swap-Regret = \\( \\underset{\\Phi : [n] \\to [n]}{\\max} \\underset{t}{\\sum} (r_{\\Phi(ALG(t)),t} - r_{ALG(t),t}) \\) where \\(\\Phi\\) maps each action to another action.</p> <p>In the context of external regret, our algorithm was just competing with the best single action (i.e. the best stock). In the context of swap regret, our algorithm has to compete with a friend, who is seeing what our algorithm is recommending, and using that to do something else.</p> <p>How is the swap function determined? We are taking the best of all possible swap functions.</p> <p>Between external regret and swap regret, which one is higher? The swap regret is always at least as high as the external regret.</p> <p>Which kind of regret is it better to minimize? It is better to minimize swap regret.</p> <p>One way to see this is that we can have everything mapping to the same action (i.e. the best action in hindsight) in the swap function. So, if we can minimize the swap regret, it is better, although it is also much harder because the benchmark is more complex.</p> <p>Why don\u2019t we use the best possible scenario to calculate regret? Indeed, another benchmark we can use is the best action each day (rather than the best action overall). The benchmark is just too high. For instance, if everyday we are playing a game where we flip a coin, and we need to determine whether it is going to be head or tail, there is no good algorithm for this, it is helpless.</p> <p>We saw that, for external regret, there is an algorithm that can do as well as the best stock, even with adversarial input. It turns out that with swap regret, there is also a pretty good algorithm.</p> <p>Algorithm: Swap-Regret Minimization</p> <ul> <li>Define meta-actions for each possible swap choice \\(\\Phi\\).</li> <li>Run MWU/FTRL on meta-actions.</li> <li>At each iteration, solve for distribution \\(x_t = \\mathbb{E}[\\Phi(x_t)]\\).</li> </ul> <p>Claim: The Total Swap-Regret is in the order of \\(O(\\sqrt{T n \\log(n)})\\).</p> <p>The total swap regret looks like the external regret, except that instead of \\(n\\) actions, we have to take the \\(\\log\\) of \\(n^n\\) actions. This is a higher regret, so it is going to take more time to achieve vanishing regret.</p> <p>For instance, if we think about the NYSE, with 2,000 actions, instead of \\(\\log(2000) \\approx 11\\), it is going to be \\(\\log(2000^{2000})\\), which is \\(\\log(2000) \\cdot 2000 \\approx 11 \\cdot 2000 \\approx 22000\\).</p> <p>So, for instance, if \\(T\\) is in days, it is going to take \\(60\\) years for the swap to beat the entire stock exchange, but if we are talking in seconds, then \\(22000\\) seconds is not very long (about \\(6\\) hours).</p>"},{"location":"lecture-3/#regret-minimization-with-bandit-feedback","title":"Regret Minimization with Bandit Feedback","text":"<p>Scenario 2</p> <p>Consider a new portal/feed editor:</p> <ul> <li>We run an algorithm over \\(T\\) days.</li> <li>There are \\(n\\) possible \"arms\" (i.e. actions), where an \"arm\" is a news category or a reporter, for instance.</li> <li>On the \\(t\\)-th day, a user comes, and we can show them one news item.</li> <li>The reward \\(r_{i,t}\\) measures how long the user engaged with the news item.</li> </ul> <p>Our goal is to minimize regret, namely \\(\\underset{i}{max} \\underset{t}{\\sum}(r_{i,t} - r_{ALG(t),t})\\).</p> <p>Challenge: With partial (bandit) feedback, we see how long a user engaged with what we showed them, but we don\u2019t know how long the user would have engaged with the content we did not show them. Yet, we still have to compete with all the other actions.</p> <p>Solution: We need to arbitrate how much we want to learn about how each arm is doing (i.e. exploration) and how much we want to extract reward from the best arm (i.e. exploitation).</p> <p>Key Idea #5: To account for bandit/partial feedback, we may apply a similar regret minimization framework, with a trade-off between exploration and exploitation.</p> <p>Mechanism: Multi-Armed Bandit Model Warm-Up Algorithm</p> <ul> <li>Try all possible arms to see which one yields the best reward.</li> <li>Keep pulling on the best arm identified.</li> </ul> <p>This approach works well if rewards are fixed (e.g., one arm consistently rewards highly). However, it struggles with randomness, leading to potential overfitting.</p> <p>How much exploration is needed? By the Law of Large Numbers, the average reward converges to the expectation over time. Concentration Inequalities quantify this convergence rate.</p> <p>Using the Hoeffding Inequality:</p> \\[ Pr\\left[\\text{expectation} &gt; \\text{average of samples} + \\sqrt{\\frac{X}{\\text{number of samples}}}\\right] &lt; e^{-2X} \\] <p>To ensure confidence across \\(T\\) iterations:</p> \\[ Pr\\left[\\text{expectation} &gt; \\text{average of samples} + \\sqrt{\\frac{2\\ln(T)}{\\text{number of samples}}}\\right] &lt; \\frac{1}{T^4} \\] <p>By setting \\(X = 2\\ln(T)\\), we derive an Upper Confidence Bound (UCB). This ensures minimal mistakes over the algorithm's runtime.</p> <p>Mechanism: UCB1 Algorithm</p> <p>Let \\(\\text{UCB}_i = \\text{(average of $i$'s samples)} + \\sqrt{\\frac{2\\ln(T)}{\\text{number of $i$'s samples}}}\\).</p> <p>On day \\(t\\):</p> <ul> <li>Compute UCB for each arm.</li> <li>Pull the arm with the maximum UCB (breaking ties randomly).</li> </ul> <p>This greedy algorithm balances exploration and exploitation:</p> <ul> <li>Under-explored arms (few samples) have a high exploration incentive.</li> <li>High-performing arms (high averages) encourage exploitation.</li> </ul> <p>The expected regret under iid rewards (between -1 and 1) is in the order of \\(O(\\sqrt{nT\\log(T)})\\) as long as \\(T &gt; n\\log(n)\\), which is not as good as in full feedback scenarios, but remains reasonable for bandit feedback.</p> <p>In other words:</p> <ul> <li>Good: Regret per day approaches 0 if \\(T &gt; n\\log(n)\\).</li> <li>Bad: Not as optimal as full feedback.</li> <li>Ugly: Requires iid rewards.</li> </ul> <p>Caveat: Because UCB1 is a greedy, deterministic algorithm (with no randomness), we can construct an adversarial input.</p> <p>Let's explore another algorithm that works for bandit feedback AND against an adversarial input. The idea is to use MWU again. The challenge is that in MWU, every time we want to update the weights of the actions, we need to know the rewards, but we don\u2019t have the rewards. Instead, we are going to feed the algorithm pseudo-rewards, something that is going to replace the reward and hopefully work as well as the reward.</p> <p>Mechanism: Exp3 Algorithm</p> <p>On day \\(t\\):</p> <ul> <li>MWU selects an arm.</li> <li>Define pseudo-rewards:<ul> <li>If arm \\(i\\) is not selected: \\(\\hat{r}_{i,t} = 0\\).</li> <li>If arm \\(i\\) is selected: \\(\\hat{r}_{i,t} = \\frac{r_{i,t}}{Pr[\\text{selecting } i]}\\).</li> </ul> </li> </ul> <p>Pseudo-reward rationale:</p> <ul> <li>Unselected actions yield zero (unknown) rewards.</li> <li>Selected actions are scaled to compensate selection probability, ensuring unbiased estimates (Inverse Propensity Score).</li> </ul> <p>Key Idea #6: The Inverse Propensity Score means that \\(\\hat{r}_{i,t}\\) is an unbiased estimator, i.e. \\(\\mathbb{E}[\\hat{r}_{i,t}] = \\mathbb{E}[{r}_{i,t}]\\).</p> <p>The expected regret for Exp3 is \\(O(\\sqrt{nT\\log(n)})\\), slightly worse than MWU with full feedback (i.e. \\(O(\\sqrt{T\\log(n)})\\), though better algorithms exist to reduce variance.</p>"},{"location":"lecture-3/#recap","title":"Recap","text":"<p>Key Ideas</p> <ul> <li>Key Idea #1: Use historical performance to forecast the future.</li> <li>Key Idea #2: Use regret to measure the success of algorithms.</li> <li>Key Idea #3: Use randomness as safety against adversarial inputs.</li> <li>Key Idea #4: Balance randomness and optimization.</li> <li>Key Idea #5: Balance exploration and exploitation.</li> <li>Key Idea #6: Use unbiased estimates of rewards.</li> </ul>"},{"location":"lecture-4/","title":"Equilibria in Games","text":"<p>April 14, 2025</p> <p>Last time, we explored what selfish agents should do when the mechanism is not strategyproof, and possibly not fully specified, from a single agent's perspective. Now, let's analyze systems with multiple agents. This serves as an introduction to game theory.</p> <p>Example: The Penalty Kick Game</p> <p>Consider the scenario of a penalty kick between Messi (kicker) and Lloris (goalie), where the probability of goal is defined as follows:</p> Kick Left Kick Right Jump Left 0.5 0.8 Jump Right 0.9 0.2 <p>Questions:</p> <ul> <li>Where should Messi kick?</li> <li>Where should Lloris jump?</li> </ul> <p>Lloris does not know which direction Messi is going to kick\u2014and needs to think about it. Messi does not know which direction Lloris is going to jump\u2014and needs to think about it.</p> <p>The answer is not obvious, but Game Theory tells us:</p> <ul> <li>Messi should kick left with probability 0.6 and right with probability 0.4.</li> <li>Lloris should jump left with probability 0.7 and right with probability 0.3.</li> <li>The probability of goal is 0.62.</li> </ul> <p>More specifically:</p> <ul> <li>When Messi kicks left, there is a \\(0.5 \\cdot 0.7 + 0.9 \\cdot 0.3 = 0.62\\)  probability of goal.</li> <li>When Messi kicks right, there is a \\(0.8 \\cdot 0.7 + 0.2 \\cdot 0.3 = 0.62\\) probability of goal, too.</li> <li>When Lloris jumps left, there is a \\(0.5 \\cdot 06 + 0.8 \\cdot 0.4 = 0.62\\) probability of goal.</li> <li>When Lloris jumps right, there is a \\(0.9 \\cdot 0.6 + 0.2 \\cdot 0.4 = 0.62\\) probability of goal, too.</li> </ul> <p>This is called a Nash Equilibrium:</p> <ul> <li>If Lloris jumps with these probabilities, Messi is indifferent when kicking left or kicking right.</li> <li>If Mess kicks with these probabilities, Lloris is indifferent when jumping left or jumping right.</li> <li>When both are kicking/jumping with these probabilities, neither has an incentive to deviate from these probabilities.</li> </ul> <p>Definition: Nash Equilibrium</p> <p>A pair of (possibly randomized) strategies, such that no player has an incentive to deviate. </p> <p>Question: How do we come up with these probabilities?</p> <p>There is an algorithm that comes up with those.</p> <p>Question: Do Messi and Lloris know each other\u2019s probabilities?</p> <p>No, they don\u2019t. They are deciding in the moment.</p> <p>Question: Are both Messi and Lloris\u2019s decisions happening at the same time or is one reacting to the other?</p> <p>Lloris does not have time to see what Messi does, so both are happening at the same time.</p>"},{"location":"lecture-4/#equilibrium-in-2-player-zero-sum-games","title":"Equilibrium in 2-Player Zero-Sum Games","text":"<p>Assumptions:</p> <ul> <li>There are 2 players (Lloris and Messi).</li> <li>This is a Zero-sum game, meaning that their incentives are perfectly misaligned: what Lloris wants is exactly the opposite of what Messi wants.</li> </ul> <p>Messi wants to optimize the probability of goal, i.e. he wants to maximize over all his possible kick strategies, assuming the worst possible jumping strategy for Lloris:</p> \\[   Messi's \\; opt: \\underset{kick-stratregy}{max} \\; \\underset{jump-stratregy}{min} \\; Pr[goal] \\] <p>Lloris has the opposite optimization problem: he wants to figure out how to jump, he knows Messi is going to kick in the direction that makes it as hard as possible for him, and he wants to minimize the probability of goal:</p> \\[   Lloris's \\; opt: \\underset{jump-stratregy}{min} \\; \\underset{kick-stratregy}{max} \\; Pr[goal] \\] <p>Theorem: Messi\u2019s max-min = Nash equilibrium = Lloris\u2019s min-max.</p> <p>In other words: In a 2-player zero-sum game, even with more than 2 strategies per player, Messi\u2019s max-min optimization problem is the same as the Nash Equilibrium (which tells us that the Nash Equilibrium is unique), which is going to be the same as Lloris\u2019 min-max optimization problem.</p> <p>Where did these probabilities come from?</p> <p>One way to compute them is to use Linear Programming (LP), which generalizes the \\(s-t \\; Max-Flow\\), which is the same as the \\(s-t \\; Min-Cut\\), and this duality is the same reason why Messi\u2019s problem is equivalent to Lloris\u2019s problem. This gives us an efficient algorithm to compute these probabilities.</p>"},{"location":"lecture-4/#mini-case-study-poker-algorithms","title":"Mini Case Study: Poker Algorithms","text":"<p>Case Study: \"Heads Up\" (2-Player) Poker</p> <p>Rough rules of Poker:</p> <ul> <li>Every player is dealt 2 cards (private information).</li> <li>Players bet during betting round.</li> <li>There are some open cards on the table (public information).</li> <li>We determine which player's private cards are a better match to the cards on the table.</li> </ul> <p>Can we use Linear Programming to compute the Nash Equilibrium strategy?</p> <p>Linear Programming is fast, but Poker is a very large game. How large? Large enough that someone wrote a research paper about it\u2014actually, about an algorithm for estimating how large it is.</p> <p>In fact, there are 56 trillion possible card combinations. However, this is not the only thing to consider: we also need to take into account how the other player is betting. Then, the number of possible combinations of what could happen becomes \\(10^{160}\\). Efficient Linear Programming algorithms are not fast enough.</p> <p>Poker is an extensive-form game, because there are turns. In game theory, an extensive-form game represents\u00a0a strategic interaction where players make decisions sequentially, and the order of moves is explicitly modeled using a game tree.</p> <p>Definitions: Game Theory Terminology Overload</p> <ul> <li>Game Node: State of the game (i.e. all cards dealt, and all bets made, so far).</li> <li>Game Tree: Graph representing which game nodes are reachable from which game node (i.e. there is an edge if we can go from \\(A\\) to \\(B\\) by calling another bet).</li> <li>Information Set: All the game nodes consistent with a player's information.</li> </ul> <p>The most important part is the Information Set, which represents all the games nodes consistent with a player\u2019s information, i.e. there are different states of the game that correspond to different cards that other players could be holding.</p> <p>Why should we worry about all the possible game states? Why can't we just solve the current state/information set in real time?</p> <p>What a player wants to do at a given state of the the game depends on what they know (what they see), but it also depends on what the other players are going to do, which in turns depends on what they think that original player is going to do, and what they think that player is going to do depends on the cards they have now, but also what they might do with other cards.</p> <p>Algorithmic Insight #1: Use the Blueprint strategy.</p> <ol> <li>Blueprint solves a coarse approximation of Poker (\"only\" \\(10^{13}\\) states, i.e. \\(50TB\\) to store 1 strategy).</li> <li>During live play: we can use Blueprint to solve the actual information set, i.e. to estimate what the other player thinks we would do if we had their informations set.</li> </ol> <p>Algorithmic Insight #2: Use regret-minimizing algorithms, e.g., MWU/FTRL.</p> <p>Theorem: If two players play a zero-sum game, and both use a regret-minimization algorithm, they will converge towards a Nash equilibrium.</p> <p>Case Study: \"Heads Up\" (2-Player) Poker (Cont'd)</p> <p>In 2017, CMU's Poker bot beat top human Poker players for the first time.</p> <p>Recap: 2-Player 0-Sum Game</p> <p>At Nash equilibirum:</p> <ul> <li>Both players choose actions randomly.</li> <li>Neither player can gain from changing distribution.</li> <li>Theorem: max-min = Nash equilibrium = min-max.</li> </ul> <p>Algorithms for computing Nash equilibrium:</p> <ul> <li>Linear programming.</li> <li>Regret minimization.</li> </ul>"},{"location":"lecture-4/#nash-equilibrium-in-non-zero-sum-games","title":"Nash Equilibrium in Non-Zero-Sum Games","text":"<p>The Penalty Kick Game, Revisited</p> <p>Lloris's incentives:</p> <ul> <li>Lloris wants to win the World Cup (he already did in 2018).</li> <li>Lloris also wants to be the goalie who stopped Messi's penalty kick (to earn more fame and sponsorship opportunities).</li> </ul> Pr[goal] Messi kicks Left Messi kicks Right Lloris jumps Left 0.5 0.8 Lloris jumps Right 0.9 0.2 Pr[save] Messi kicks Left Messi kicks Right Lloris jumps Left 0.4 0 Lloris jumps Right 0 0.6 <p>Therefore: \\(Lloris's \\; utility = Pr[save] - Pr[goal]\\).</p> U_{Messi} Messi kicks Left Messi kicks Right Lloris jumps Left 0.5 0.8 Lloris jumps Right 0.9 0.2 U_{Lloris} Messi kicks Left Messi kicks Right Lloris jumps Left -0.1 -0.8 Lloris jumps Right -0.9 0.4 <p>This is what Lloris is really maximizing (while Messi is only optimizing the probability of goal).</p> <p>Where should Messi kick now? Where should Lloris jump now?</p> <p>It is no longer a zero-sum game, because the incentives of Messi and Lloris are neither perfectly aligned not perfectly misaligned. Messi does not care whether he misses or Lloris saves, but Lloris does (he would rather save).</p> <p>How should we model this question?</p> <p>We can use a Nash equilibrium, which always exists in a finite game, even if it is not a zero-sum game.</p> <p>Theorem: Nash's Existence Theorem (1951)</p> <p>Every finite game has at least one Nash equilibrium (possibly mixed strategies). If players play the Nash equilibrium, neither wants to deviate.</p> <p>Caveats: The Nash equilibrium...</p> <ul> <li>... is no longer unique.</li> <li>... is no longer equal to the max-min and the min-max.</li> <li>... is not approached by Regret Minimization.</li> <li>... is intractable to compute, even approximately.</li> <li>... sometimes does not make sense (see below).</li> </ul> <p>Example: The CS269I Grade Game</p> <p>You and your partner submitted a wonderful project, but the instructor is not sure how much each of you contributed to it. So, they will assign your grades using the following game:</p> <ol> <li>You send the instructor \\(x \\in \\{2, ..., 99\\}\\), and your partner sends the instructor \\(y \\in \\{2, ..., 99\\}\\).</li> <li> <p>Then the instructor assigns you grade as follows:</p> <ul> <li>If \\(x = y\\), then your grade is \\(x\\).</li> <li>If \\(x &lt; y\\), then your grade is \\(min\\{x,y\\}+2\\).</li> <li>If \\(x &gt; y\\), then your grade is \\(min\\{x,y\\}-2\\).</li> </ul> </li> </ol> <p>Which grade should you send to the instructor?</p> <p>If you know your partner is going to put 99, then you put 98 so you get 100, and your friend get 96. So, maybe, you are going to put 97 and get 99, but if your friend it doing 97, you would rather put 96 and get 98. But then your friend is doing 96, so you would rather put 95, etc. It turns out that the unique Nash equilibrium in this game is when both players play 2 (x = y = 2), which is not expected in practice with real players.</p> <p>So, we have this theorem that says that a Nash equilibrium always exists, but it has all these caveats. Let's consider alternative solution concepts that have emerged in game theory which, sometimes, may be a better model for this kind of games or strategic situations.</p>"},{"location":"lecture-4/#correlated-equilibrium","title":"Correlated Equilibrium","text":"<p>Example: The Intersection Game</p> <p>This is similar to the \"Chicken\" and the \"Hawk-Dove\" games. Essentially, you arrive at an intersection, where another car arrives as well, and you need to decide what to do:</p> Go Wait Go (-99,-99) (1,0) Wait (0,1) (0,0) <p>What does this mean?</p> <ul> <li>If we both wait, we both get 0, nobody is moving.</li> <li>If we go and the other person waits, we get 1 and they get 0.</li> <li>If we wait and the other person goes, we get 0 and they get 1.</li> <li>If we both go, we are very unhappy, we get a very negative utility, we have an accident.</li> </ul> <p>This implies the following equilibria:</p> <ul> <li>Asymmmetric equilibria: (Go, Wait), (Wait, Go), i.e. if they go, we want to wait, and vice versa.</li> <li>Symmetric equilibria: Go with a probability of \\(1\\%\\) and Wait with a probability of \\(99\\%\\), i.e. we each independently go with a probability of \\(1\\%\\) and wait with a probability of \\(99\\%\\)\u2014which is not great, since there is still a \\(\\frac{1}{10000}\\) probability of colliding.</li> <li>Correlated Equilibrium: (Go, Wait) with a probability of \\(50\\%\\) and (Wait, Go) with a probability of \\(50\\%\\), i.e. we go and they wait with a probability of \\(50\\%\\), and we wait and they go with a probability of \\(50\\%\\). This is a correlated distribution: it is not independent. To implement this, we need a correlating device, i.e. something that is going to help us correlate our choices, such as a traffic light. For instance, the traffic light, with a probability of \\(50\\%\\) is going to show us red and show them green, or vice versa, but it is never going to show us both green (hopefully).</li> </ul> <p>Definition: Correlated Equilibrium</p> <p>A correlating device sends each player a secret recommended action (\"signal\") from a pubicly-known correlated distribution. No player can gain from deviating from the recommended action.</p> <p>In other words, when we have a correlated distribution over actions that gives a signal/recommended action, if the other player follows their recommended action, it is in our best interest to follow our own recommended action.</p> <p>For instance, if the traffic light shows us red, it is probably because someone else has green, so we don\u2019t want to go, but if the traffic light shows us green, then we know everyone must have red, so we might as well go.</p> <p>Note: Every Nash equilibrium is an (un)correlated equilibrium.</p> <p>Good news:</p> <ul> <li>A correlated equilibrium can be computed efficiently (e.g. with Linear Programming).</li> <li>If every player runs a Swap-Regret-minimizing algorithm, then the play converges towards the set of correlated equilibria. It is not really an equilibrium, though, as players will continue to cycle around the set of correlated equilibria forever.</li> <li>If every player runs an External-Regret-minimizing algorithm, then the play converges towards the set of coarse correlated equilibria.</li> </ul>"},{"location":"lecture-4/#coarse-correlated-equilibrium","title":"Coarse Correlated Equilibrium","text":"<p>What is the difference between a correlated equilibrium and a coarse correlated equilibrium?</p> <p>Both types of equilibria rely on a correlating device to send each player a secret recommended action (\"signal\") from a publicly-known correlated distribution. However:</p> <ul> <li>In the correlated equilibrium: Players choose to follow the recommended action after seeing it.</li> <li>In the coarse correlated equilibrium: Players want to follow their average recommended action, but may not like some recommendations.</li> </ul> <p>Question: How do we know which algorithm and equilibrium are appropriate?</p> <p>One way to think about it is to remember that agents are going to do what is good for them. If you think players are going to use the simplest algorithm, it is probably external-regret minimizing, so they will likely end up at the coarse correlated equilibrium. If you expect them to use a more sophisticated swap-regret algorithm (which has other game theory advantages), you expect them to end up at the correlated algorithm.</p> <p>Bad News:</p> <ul> <li>Just like the Nash equilibrium, the correlated equilibrium is not unique, and sometimes, it does not even make sense (cf. the CS269I Grade Game).</li> <li>Without a correlating device, we are not in an equilibrium: players have an incentive to deviate.</li> </ul>"},{"location":"lecture-4/#stackelberg-equilibrium","title":"Stackelberg Equilibrium","text":"<p>Example: The Intersection Game, Revisited</p> <p>As a reminder:</p> Go Wait Go (-99,-99) (1,0) Wait (0,1) (0,0) <p>However, suppose now that you are playing against a \"dog driver\":</p> <ul> <li>You don't trust the dog to be rational.</li> <li>You don't trust the dog to follow the correlating device (the traffic light signal).</li> <li>You wait... so the dog can go.</li> </ul> <p>Conclusion: The dog is better than you at the Intersection Game. More generally, committing to a strategy gives power.</p> <p>This is what we call a Stackelberg equilibrium.</p> <p>Definition: Stackelberg Equilibrium</p> <p>A Stackelberg equilibrium is a pair of strategies (Leader's strategy, Follower's strategy) such that:</p> <ol> <li>The Follower's strategy is optimal given the Leader's strategy.</li> <li>The Leader's commitment is optimal, i.e. the payoff is optimal for the Leader among all pairs satisfying #1.</li> </ol> <p>In other words, the Follower's strategy is a best response to what the strategy chosen by the Leader, and the Leader chooses the optimal strategy for them assuming that the Follower is going to pick a best response.</p> <p>Theorem: The Leader\u2019s utility, in the optimal Stackerlberg equilibrium, is at least what they can get in any possible equilibrium.</p> <p>Note: If we think about the games we talked about so far, we talked about mixed strategies. For instance, Messi did not want to tell Lloris which side he was going to kick. However, in the Stackelberg equilibrium, without loss of generality, the follower is using a deterministic action, because the leader is already committed to a strategy, and the fact that the follower is using a deterministic action gives us an efficient algorithm using Linear Programming agin (it is something that we can compute efficiently).</p> <p>Mini Case Study: Security Games</p> <p>A Stackelberg equilibrium may be used to model how security games where:</p> <ul> <li>The goal for defense forces (i.e. security checks, patrols, etc.) is to choose the optimal strategy.</li> <li>The assumption is that attackers can observe the defense strategy.</li> </ul> <p>In practice, this is deployed in a variety of domains:</p> <ul> <li>Infrastructure (airport security).</li> <li>Nature (wildlife protection).</li> <li>Urban crime (LA Metro).</li> <li>Cybersecurity (honeypots, audits).</li> </ul> <p>Challenges in real-world applications include:</p> <ul> <li>Algorithmic difficulty: How to deal with the combinatorial number of possible actions (e.g. a route in a large road network)?</li> <li>Uncertainty: What are the attacker's payoffs? Is the attacker rational? Will the defender be able to executed their strategy as planned?</li> <li>Collaboration with human defenders: Will patrol teams follow, or feel \"micro-managed\" by, the algorithm?</li> </ul>"},{"location":"lecture-4/#recap","title":"Recap","text":"<p>Three Solution Concepts</p> <ul> <li> <p>Nash Equilibrium: Each player samples independently from a distribution. Nobody has an incentive to deviate from the distribution.</p> <ul> <li>Cleanest game theory assumption.</li> <li>May not be achievable in complex environments.</li> </ul> </li> <li> <p>Correlated Equilibrium: A correlated distribution of actions that every player would rather follow. This may arise when agents independently run ML algorithms.</p> </li> <li> <p>Stackelberg Equilibrium: The Follower's strategy is optimal given the Leader's strategy, and the Leader's commitment is optimal.</p> <ul> <li>Applies when one player has commitment power.</li> <li>It is unclear how to generalize for more than two players.</li> </ul> </li> </ul> <p>Example: The Penalty Kick Game\u2014What Really Happened (Spoiler Alert)</p> <p>Messi kicked left. Lloris jumped left. Messi scored.</p>"},{"location":"lecture-5/","title":"P2P File-Sharing Dilemma","text":"<p>April 14, 2025</p> <p>P2P File-Sharing History:</p> <ul> <li>Napster (1999-2001):<ul> <li>Major file-sharing network at the time (mostly .mp3 music files).</li> <li>Represented an estimated 40%-60% of college dorms Internet traffic.</li> <li>Network shut down in 2001 due to copyright infringement.</li> </ul> </li> <li>Gnutella (2000 onwards):<ul> <li>Decentralized P2P file-sharing network, harder to shut down.</li> <li>2010 court order to shut down popular client LimeWire (other clients remain available, but the popularity of the network is in decline).</li> </ul> </li> </ul> <p>Challenge: Free-Riding</p> <ul> <li>P2P file-sharing networks rely on users uploading content for sharing.</li> <li>Users want to download files for free, but there aren\u2019t really many incentives when uploading files, so some people do not want to contribute back.</li> <li>On Gnutella, a large fraction of users only download (66% in 2000, 85% in 2005).</li> </ul> <p>A Simplified Game Theory Model of File-Sharing</p> <p>Game theorists usually call this the Prisoners' Dilemma.</p> <p>Consider the following game:</p> <ul> <li>There are two players: Harry and Marvin.</li> <li>Each player has two actions: \"upload\" and \"free-ride\".</li> <li>Their payoff is \\(-1\\) for uploading (due to legal risks) and \\(+3\\) for dowloading, which means:</li> </ul> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <p>In other words:</p> <ul> <li>If Harry free-rides and Marvin uploads, their utility is respectively \\(3\\) and \\(-1\\)\u2014and vice versa, if Harry uploads and Marvin free rides, their utility is respectively \\(-1\\) and \\(3\\).</li> <li>If they are both uploading, they both get \\(2\\), i.e. \\(3\\) for downloading and \\(-1\\) for uploading.</li> <li>If they are both free riding, there is nothing happening in the network, so their utility is \\(0\\).</li> </ul> <p>What should they do?</p> <p>For anything Marv would do, Harry would rather free ride (and vice versa), so the optimal thing for them to do is to free ride. However, if everyone free rides, there are no files available for download, so the social welfare is the worst.</p>"},{"location":"lecture-5/#iterated-file-sharing-dilemma","title":"Iterated File-Sharing Dilemma","text":"<p>Let's now iterate the file-sharing dilemma game \\(n\\) times.</p> <p>Harry and Marvin now repeat the file-sharing game \\(n\\) times.</p> <ul> <li>In each iteration, they play the same file-sharing dilemma game. Their strategy may depend on past iterations.</li> <li>Their goal is to maximize their total payoff across all iterations.</li> </ul> <p>What should they do? How do we model this question?</p> <p>One consideration: Can agents commit to a specific strategy or not? If there are two agents sharing files on the internet, it is really hard to enforce commitments towards future rounds of the game.</p> <p>Definition: Subgame Perfect Equilibrium (SPE)</p> <ul> <li>On day \\(n\\), the agents play a Nash equilibrium.</li> <li>On day \\(n-1\\), the agents evaluate their actions assuming that they will play a Nash equilibrium on day \\(n\\). Then, they play a Nash equilibrium for this particular game.</li> <li>On day \\(n-2\\), we assume the agents will play an SPE in the future.</li> <li>Etc.</li> </ul> <p>An SPE means that agents cannot commit to playing suboptimal strategies in the future. For instance, if the optimal action for them is to free-ride tomorrow, they cannot commit to uploading tomorrow.</p> <p>Another way to think about this is that it is like a Nash equilibrium (they are thinking about what happens on the last day), with backward induction (they are thinking back and modeling the future with that thinking).</p> <p>In a way, this inability to commit is almost like the opposite of the Stackelberg equilibrium where the leader cannot commit to a strategy profile even when the strategy profile is suboptimal given what the follower is doing.</p> <p>Note: Anything that Harry does will change what Marvin does, and vice versa, and Harry needs to have some model of what Marvin will do in order to determine what will maximize his utility. However, one assumption we can make is that they are never doing something that is suboptimal.</p> <p>Example: SPE for the File-Sharing Dilemma Game Iterated \\(n\\) Times</p> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <ul> <li>In the \\(n\\)-th iteration, free-riding is the dominant strategy. Note that what happened in the past does not change incentives here.</li> <li>In the \\((n-1)\\)-th iteration, free-riding is still strategic. No matter what a player does now, both will free-ride in the next iteration.</li> <li>By induction, free-riding is strategic in every iteration.</li> </ul>"},{"location":"lecture-5/#repeated-games-with-discounting","title":"Repeated Games with Discounting","text":"<p>Example: \\((1-p)\\)-Discounted File-Sharing Dilemma</p> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <p>Random-stopping assumption: We are playing\u2014possibly forever when \\(p = 0\\). At every iteration, we stop playing with probability \\(p\\) (for instance because another player\u2019s connection defaults, or because there was a lawsuit against the network, etc.).</p> <p>In other words, Harry and Marvin repeat the file-sharing game:</p> <ul> <li>In each iteration, they play the same file-sharing dilemma game. Now, their strategy may depend on past iterations.</li> <li>At each iteration, they stop with probability \\(p\\).</li> <li>Their goal is to maximize their total payoff across all iterations.</li> </ul> <p>Here, it is less obvious what players should do (due to the random stopping rule).</p> <p>Note: This is also a popular model in general (outside of file-sharing) to model interest rates because you would rather have your money today than tomorrow, so every day in the future is worth a little less.</p> <p>One possible strategy that Harry can choose is called the Grim Trigger Strategy.</p> <p>Definition: The Grim Trigger Strategy</p> <ul> <li>If there was ever a time when Marvin did not upload, do not upload ever again.</li> <li>Otherwise, upload.</li> </ul> <p>Essentially, if there is ever a time when Marvin free-rides, then from that point on, Harry free-rides forever, but until then, he uploads. This is all about making a threat: if Marvin ever stops sharing, then Harry will stop sharing with him. Note however that, on the last day, there is no threat, because there is no future.</p> <p>Let's analyze Marvin\u2019s optimal strategy, given that Harry is playing the Grim Trigger Strategy. We are assuming that Marvin believes that Harry will actually play Grim Trigger (we will see later why this is a possible assumption). In this scenario, Marvin\u2019s optimal strategy is either to always upload or to never upload, and which one is optimal depends on \\(p\\).</p> <p>Analysis: Marvin's Strategy When Harry Plays The Grim Trigger</p> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <p>Always Uploading: If Marvin always uploads, Harry also always uploads, so every day they keep playing, Marvin has an expected payoff of \\(2\\). This is why the payoff is basically \\(2 \\cdot the \\; geometric \\; sequence \\; for \\; (1-p)\\), which comes down to \\(\\frac{2}{p}\\). In other words, if Marvin always uploads, his expected utility is \\(\\frac{2}{p}\\).</p> <p>Never Uploading: If Marvin never uploads, in the first iteration, Harry uploads, and then Harry sees that Marvin did not upload, so he is never going to upload again. The payoff for Marvin is \\(3\\) for the first day, and \\(0\\) for the rest of time, meaning that his utility is \\(3\\).</p> <p>Conclusion: To decide whether he should always upload or never upload, Marvin should ask when \\(3\\) is greater than \\(\\frac{2}{p}\\), which means that he should always upload when \\(p \\leq \\frac{2}{3}\\).</p> <p>If Harry is playing the Grim Trigger strategy, then it makes sense for Marvin to always upload whenever \\(p \\leq \\frac{2}{3}\\). So, if the game ends every day with probability \\(\\frac{1}{2}\\), then it makes sense for both Harry and Marvin to always upload, which is why this is an SPE.</p> <p>Question: We saw earlier that for any number of iteration \\(n\\), both players should play the strategy of never uploading. How could it be that it makes sense for Marvin to always upload whenever \\(p \\leq \\frac{2}{3}\\), if for any \\(n\\), the SPE is to never upload?</p> <p>The point is that, in the random-stopping game, the players do not know in advance when the game is going to end, so any day, they are still doing the same calculations that we just did (on day \\(7\\), it is still the same as on day \\(1\\)), which is why there is always an incentive to upload and make their future better, because they don\u2019t know when the future terminates. However, when we analyze the SPE with \\(n\\) days, we know when the last day is, so we can determine when there is no point uploading (because there is no future).</p> <p>Theorem: Folk Theorem (Game Strategy)</p> <p>If the players are patient enough, then repeated interaction can result in virtually any average payoff in an SPE equilibrium.</p> <p>In other words, the Folk theorem gives general conditions on when Harry can incentivize Marvin to play certain actions (e.g., uploading) using threats like the Grim Trigger strategy.</p> <p>One issue with the Grim Trigger strategy is that, if for some reason, Marvin's connection goes down one day and he cannot upload, then Harry will never upload again, and Marvin in turn will not upload again, so they get stuck with a utility of \\(0\\) until the end of the game.</p> <p>A more practical, robust alternative to the Grim Trigger strategy is called the Tit-for-Tat strategy.</p> <p>Definition: The Tit-for-Tat Strategy</p> <ul> <li>In stage \\(1\\), upload.</li> <li>In stage \\(i\\), reproduce the action of the other player on stage \\(i-1\\).</li> </ul> <p>Analysis: Marvin's Choices When Playing Tit-for-Tat</p> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <p>At stage \\(i\\), Marvin can:</p> <ul> <li>Upload on stage \\(i\\) (utility of \\(-1\\)), and then download on stage \\(i+1\\) (utility of \\(+3\\)) with probability \\((1-p)\\), because there is the probability that there is no next day.</li> <li>Not upload on stage \\(i\\) (utility of \\(0\\)), and then not download on stage \\(i+1\\) (utility of \\(0\\)).</li> </ul> <p>Conclusion: To decide whether he should always upload or never upload, Marvin should ask when \\(-1 + 3 \\cdot (1-p) \\geq 0\\), which means that he should always upload whenever \\(p \\leq \\frac{2}{3}\\).</p> <p>Notes:</p> <ul> <li>The conclusion is the same for Tit-for-Tat as for Grim Trigger, but this is a coincidence: in general, Grim Trigger is a stronger threat.</li> <li>When \\(p = \\frac{2}{3}\\), then both \u201calways upload\u201d and \u201cnever upload\u201d are strategic.</li> </ul> <p>Iterated File-Sharing Dilemma Recap</p> <ol> <li>In a one-shot game, free-riding is a strictly dominant strategy, i.e. for any action Marvin takes, Harry is always better off not uploading.</li> <li>With any fixed number of iterations, free-riding is the SPE.</li> <li>With a random number of iterations, a credible threat of future retaliation can lead to cooperation.</li> </ol> <p>In practice, game theory predicts that:</p> <ol> <li>Players do not cooperate in one-round games.</li> <li>Players do not cooperate in \\(n\\)-round games.</li> <li>Players likely cooperate in games with a random number of rounds.</li> </ol> <p>In fact, in the real world, we frequently observe \\((1)\\) and \\((3)\\): in tourist traps, restaurants might as well charge customers as much as they can, because they will only see them once, while in locals\u2019 favorite spots, restaurants expect customers to come over and over, so they have incentives to serve them well.</p> <p>What about \\((2)\\)?</p> <p>Experiment: Axelrod Games</p> <p>Around 1980, Professor Robert Axelrod invited friends to write computer programs for a tournament of iterated file-sharing dilemma with a fixed number of round \\(n = 200\\).</p> <p>Out of 15 submissions, Tit-for-Tat won first place.</p> <p>Notes:</p> <ul> <li>Tit-for-Tat can never win a head-to-head match, but encouraging cooperation leads to a higher score on average.</li> <li>You can always do better than Tit-for-Tat: you can play Tit-for-Tat during \\(199\\) rounds, and not upload in the last round, but in practice, no participant tried this strategy.</li> </ul> <p>Later on, Professor Axelrod invited his friends to play again. Out of 62 submissions, Tit-for-Tat won first place again.</p> <p>Note: Veritasium's What Game Theory Reveals About Life, The Universe, and Everything YouTube video offers an excellent perspective on this topic.</p> <p>So, in practice, \\((2)\\) requires fragile assumptions: Harry assumes in round \\(i\\) that \\(\\rightarrow\\) Marvin assumes in round \\(i+1\\) that \\(\\rightarrow\\) Harry assumes in round \\(i+2\\) that \\(\\rightarrow\\) ... plays optimally in round \\(n\\).</p>"},{"location":"lecture-5/#bittorrent-strategies","title":"BitTorrent Strategies","text":"<p>Definition: BitTorrent Overview</p> <ul> <li>BitTorrent is currently the main P2P protocol for file-sharing.</li> <li>In 2019, BitTorrent accounted for almost 30% of upload traffic (with major spikes after episodes of Game of Thrones).</li> <li>Users are organized into swrams sharing the same file.</li> <li>A decentralized tracker coordinates active users in a given swarm.</li> <li>Each file is broken down into a number of pieces (e.g. 1,000).</li> <li>Since users need many (all) pieces, they play the iterated file-sharing dilemma game.</li> </ul> <p>Definition: The BitTorrent Default Strategy</p> <p>This is the strategy most users play because they just download the default BitTorrent client that uses this strategy by default:</p> <ul> <li>Every 15-30 minutes, a user contacts the tracker, requesting a new random subset of swarm peers. So, their number of known swarm peers grows over time.</li> <li>Each user attempts to download from their peers.</li> <li>Each user has \\(s\\) slots and allocates \\(\\frac{1}{s}\\) of upload bandwidth to each slot.</li> <li>\\(s\\) peers receive a slot (\"unchoked\"), chosen using a variant of the Tit-for-Tat strategy: each user prioritizes peers who sends them the most data. One slot is reserved for \"pity uploads\" to random peers (\"optimistic unchoking\"), i.e. a freebee for users who do not have data yet (to give \"newbies\" a chance).</li> <li>For each fixed peer, the priority is given to uploading rare file pieces.</li> </ul> <p>Definition: The BitThief Strategy</p> <ul> <li>Never upload anything.</li> <li>Ask tracker for peers much more frequently (to grow the number of peers quickly).</li> </ul> <p>The idea is to maximize the chances of optimistic unchoke. In experiments, though, this strategy is \\(5\\) times slower than the default BitTorrent strategy. It still completes downloads in reasonable time without any uploads.</p> <p>Definition: The BitTyrant Strategy</p> <p>In the Tit-for-Tat/BitTorrent strategy, the idea is to upload data to peers who send you the most data.</p> <p>In contrast, in the BitTyrant strategy, the idea is to upload data to peers who will send you the most data:</p> <ul> <li>For each peer \\(j\\), estimate the amount of upload \\(u_j\\) so that \\(j\\) unchokes you.</li> <li>For each peer \\(j\\), estimate the speed of dowload \\(d_j\\) if \\(j\\) unchockes you.</li> <li>Prioritize sending as much data as possible to peer with the maximum \\(\\frac{d_j}{u_j}\\) ratio.</li> </ul> <p>In other words, the rationale is to look for peers who will give you the most data if you give them a little bit of data.</p> <p>In experiments, this provides \\(\\approx 70\\%\\) download speed gains over the standard BitTorrent strategy.</p> <p>BitTorrent Strategies Recap</p> <ul> <li>Tit-for-Tat (default): Prioritize sending data to peers who send you the most data. Reserve one upload slot for new users (\"optimistic unchoke\").</li> <li>BitThief: Don't upload anything. Contact as many peers as possible to maximize chances of optimistic unchoke.</li> <li>BitTyrant: Prioritize sending data to peers who would send you the most data in return.</li> </ul>"},{"location":"lecture-5/#recap","title":"Recap","text":"<p>P2P File-Sharing Recap</p> <ul> <li>Free-riding Users only download, and don't contribute uploads to others.</li> <li>One-shot file-sharing dilemma: Free-riding is a dominant strategy.</li> <li>Iterated file-sharing dilemma: Tit-for-Tat encourages uploads.</li> <li>Subgame Perfect Equilibrium (SPE): Agents choose today's optimal strategy, assuming they will play an SPE in the future.</li> <li>BitTorrent (decentralized Tit-for-Tat): Free-riding is possible on optimistic unchokes. Strategizing over which peers give the best return for uploads is also possible.</li> </ul>"},{"location":"lecture-6/","title":"Market Equilibrium","text":"<p>April 16, 2025</p> <p>Definition: Market</p> <p>\"A means by which the exchage of goods and services takes place as a result of buyers and sellers being in contact with one another, either directly or through mediating agents or situations.\" \u2014 Joan Violet Robinson, Britannica</p> <p>Previously, we examined scenarios without monetary transfers. Issues with non-monetary markets include:</p> <ol> <li>Limited to ordinal rather than cardinal preferences.</li> <li>Emergence of underground markets.</li> <li>Potential exploitation (bots manipulating donor lists).</li> </ol> <p>Starting today, we want to measure utility with money:</p> <ul> <li>Basic assumption: How much I want something is equal to how much I am willing to pay for it.</li> <li>Obvious caveat: How much I am willing to pay depends on how much money I have.</li> </ul>"},{"location":"lecture-6/#modeling-buyers","title":"Modeling Buyers","text":"<p>Definition: Cardinal Utilities vs Ordinal Utilities</p> <ul> <li>Cardinal utilities: Assign numeric values to preferences.</li> <li>Ordinal utilities: Only rank preferences by order.</li> </ul> <p>Is one type of utilities better than the other?</p> <p>Cardinal utilities convey richer information (ordinal can be derived from cardinal), are intuitive in economic analysis, and typically monetarily measurable.</p> <p>For example, in the kidney exchange program, we may not know whether we prefer one kidney over another, but we can compare two risky matches with one perfect match (one perfect match is definitely better than one risky match) with numbers by adding the happiness that the two risky patients may get and comparing it with the happiness of the one perfect match patient, which is harder to determine with only ordinal utilities.</p> <p>In addition, we can think about probabilistic events, because when we have probabilities and numbers, we can check the expectation. If we can have a good kidney with \\(50\\%\\) or an okay match with \\(70\\%\\) (more likely), we can use expectation to compare those options, while with cardinal utilitie alone, comparison is more challenging.</p> <p>Finally, let's note that we typically quantify cardinal utilities with money, which is good in some applications, but problematic in some other applications. There are alternatives, such as \u201cquality-adjusted life years\u201d in health economics, which measure how many additional years a patient may live with a kidney. It is not perfect either, but it is some way to help us make these decisions.</p> <p>In other applications, where money is not a good idea, it is not clear what the right measure is. For instance, how do we quantify utility in education (e.g. how much utility do we get from taking CS269I?\u2014a lot, for sure, but how much?) or in dating apps (e.g. how do we quantify how we are making better matches?)</p> <p>In contrast, one disadvantage of cardinal utilities is that humans are better at comparing outcomes.</p> <p>Introducing money changes market dynamics substantially:</p> <ul> <li>Stanford housing would differ if rooms were auctioned.</li> <li>Buying organs is illegal in most countries.</li> <li>Hospitals-student monetary matches face legal restrictions.</li> </ul> <p>However, there are also some challenges in non-monetary transfers:</p> <ul> <li>Without monetary transfers, we are limited to ordinal preferences.</li> <li>When we force non-monetary transfers, people try to get their way under the table, which comes with legal issues (admissions scandal) and risks (illegal organ transplants, for both people who buy and sell organs)</li> <li>We need a permissions systems (as opposed to a permission-less system, such as the Internet or Bitcoin), otherwise:<ul> <li>For instance in the Stanford housing lottery, you need a SUNet ID, so that applicants do not enter the draw with a bot and get a million lottery tickets.</li> <li>With online reviews, we really have a problem with restaurants writing really good reviews for themselves and bad reviews for competing restaurants.</li> </ul> </li> </ul> <p>Definition: Fungible Goods vs Idiosyncratic Goods</p> <ul> <li>Fungible goods: Interchangeable units.</li> <li>Idiosyncratic goods: Unique goods.</li> </ul> <p>In other words, goods are fungible when there lots of interchangeable copies of an item, such as copies of a new book. In contrast, goods are idiosyncratic when they have intrinsic features. If you try to buy a house, every house is very different from the house down the street (problems with the roof, nicer garden, better location, etc.), so we need to understand the value of each house. In dating apps, it is similar: each person is very different. NFTs (Non-Fungible Tokens) by definition are idiosyncratic.</p> <p>In practice, many things are a mix of fungible and idiosyncratic. For instance, in a ride-sharing app, we don\u2019t really care who the driver is (the driver is fungible), but we really care where we go (the destination is idiosyncratic).</p>"},{"location":"lecture-6/#supply-and-demand-of-fungible-goods","title":"Supply and Demand of Fungible Goods","text":"<p>Definition: Demand Curve</p> <p>Consider a perfectly fungible good, e.g., copies of a book:</p> <ul> <li>1 buyer is willing to pay $10 for the book.</li> <li>2 additional buyers are willing to pay $9 for the book.</li> <li>1 addition buyer is willing ti pay $8 for the book.</li> <li>Etc.</li> </ul> <p>We can plot this aggregate demand curve, which is typically decreasing, as more buyers are interested when the price is lower:</p> <p></p> <p>Note: The demand curve represents how many buyers are willing to buy at each price. We plot the Price as the y-axis and the Demand as the x-axis, even though it is really the demand as a function of the price.</p> <p>Definition: Supply Curve</p> <p>Let's specifically talk about used (but in identical conditions) books, which means that the books are still fungible:</p> <ul> <li>2 sellers are willing to give their copies for $1.</li> <li>1 addition seller is willing to give their copy for $2.</li> <li>3 additional sellers are willing to give their copies for $3.</li> <li>Etc.</li> </ul> <p>We can plot this aggregate supply curve, which is typically increasing, at least for used books.</p> <p></p> <p>Note: The supply curve represents how many sellers are willing to sell at each price. We plot the Price as the y-axis and the Supply as the x-axis, even though it is really the supply as a function of the price.</p> <p>Side Note: Law of Supply vs. Digital Goods</p> <p>If the above applies to used books, then what about new books?</p> <p>The Law of Supply says that the supply curve should be increasing, but sometimes, this does not hold.</p> <p>For instance, for new books:</p> <ul> <li>There is a monopoly (the publisher is selling all the copies): increasing the price does not mean that they are willing to sell it more.</li> <li>Marginal cost of printing another book is negligible (most of the price is editorial work, promotion, etc.).</li> </ul> <p>If we think of digital goods, the marginal cost of making another copy is zero, so the Law of Supply does not hold anymore:</p> <ul> <li>For ordinary physical goods, the supply curve slopes upward because each additional unit costs a little more to make. Producers only expand output when price at least covers that rising marginal cost.</li> <li>For a pure digital good\u2014software, an e\u2011book, a music file\u2014the marginal cost of copying and distributing one more unit is essentially zero, so a seller is already willing to supply any quantity at any price just above zero.\u00a0Raising the price therefore doesn\u2019t unlock extra output the way it does for physical goods. Instead, the constraint is usually legal (who owns the copyright) or strategic (how the seller wants to segment the market), not technological cost.</li> </ul> <p>In the short run, the supply curve is therefore flat (perfectly elastic) at roughly zero cost, and in the long run, depends on covering the fixed cost of creating the first copy rather than on the per\u2011unit price, so the classic Law of Supply tied to marginal production cost doesn\u2019t bite.</p> <p>Definition: Market Clearing Price</p> <p>The price where demand and supply curves meet is called the market clearing price or market equilibrium.</p> <p></p> <p>The vanilla assumption in classical economics is that buyers and sellers should naturally converge to the special price \\(p^\\star\\), where the supply meets the demand. At market clearing price, the numbers of interested buyers and sellers are equal, so they can all transact (i.e. \"clear the market\").</p> <p>This is not complicated but super important. The reason we call this an equilibrium is because:</p> <ul> <li>At that price, all the buyers who want to buy for that price can buy exactly from all the sellers who want to sell at that price (the supply is exactly meeting the demand).</li> <li>There are some sellers who are willing to sell at a higher price, but they stay out of it.</li> <li>There are some buyers who are willing to buy at a lower price, but they stay out of it.</li> </ul> <p>We are trying to think of a single price that works for the entire market. We will formalize it later, but this price \\(p^\\star\\) maximizes the total happiness.</p>"},{"location":"lecture-6/#supply-and-demand-of-idiosyncratic-goods","title":"Supply and Demand of Idiosyncratic Goods","text":"<p>The Airbnb Market Model</p> <p>Consider:</p> <ul> <li>\\(m\\) different (idiosyncratic) rooms for rent.</li> <li>\\(n\\) guests, each willing to stay in at most one room (unit-demand).</li> <li>Guest \\(i\\) has value \\(v_{i,j}\\) for staying in room \\(j\\).</li> <li>If guest \\(i\\) pays \\(p_j\\) to stay in room \\(j\\), their happiness is given by \\(U_{i,j} = v_{i,j} - p_j\\).</li> <li>If guest \\(i\\) doesn't stay in any room (e.g., camps outside), their happiness is given by \\(U_{i,\\emptyset} = 0\\).</li> </ul> <p>The Airbnb Market Model (in Economics Jargon)</p> <p>Consider:</p> <ul> <li>\\(m\\) different (idiosyncratic) goods for rent.</li> <li>\\(n\\) buyers, each willing to buy at most one good (unit-demand).</li> <li>Buyer \\(i\\) has value \\(v_{i,j}\\) for buying good \\(j\\).</li> <li>If buyer \\(i\\) pays \\(p_j\\) for good \\(j\\), their utility is given by \\(U_{i,j} = v_{i,j} - p_j\\).</li> <li>If buyer \\(i\\) doesn't buy any good (e.g., camps outside), their utility is given by \\(U_{i,\\emptyset} = 0\\).</li> </ul> <p>Definition: Competitive Equilibrium</p> <p>A competitive equilibrium is the combination of a price vector \\(\\overrightarrow{p} = (p_1, ..., p_n)\\) and a matching \\(M\\) of buyers to goods, such that:</p> <ul> <li>Each buyer is matched to their favorite good (given prices): </li> </ul> \\[ \\forall i,j' \\; v_{i,M(i)} - p_{M(i)} \\geq v_{i,j'} - p_{j'}. \\] <ul> <li>If no buyer is matched to good \\(j\\), then \\(p_j = 0\\).</li> </ul> <p>There is one more condition for a competitive equilibrium. If a buyer is not matched, that means that they don\u2019t want any good. This means that the utility for every good is negative (the value minus what it would cost them is negative). So, they stay unmatched (they have the option to remain unmatched).</p> <p>This is an important concept called individual rationality: no agent is strictly less happy after participating in the mechanism. In other words, every buyer is no worse after transacting than they were before. If they cannot find anything in their price range, they don\u2019t get anything: they don\u2019t lose money by just going to the site and checking all the options.</p> <p>Question 1: How is it possible that every buyer is matched to their favorite good? What if everyone wants the same good?</p> <p>The intuition to manage this is through prices, because if a good is over-demanded, we increase the price, until buyers leave it.</p> <p>Question 2: What if there are two buyers with the exact same utility?</p> <p>Because we have a weak inequality (rather than a strict inequality), it means that every buyer is matched until they do not have another good that they prefer, so if two buyers have the same utility, we can increase the price until they are indifferent between two rooms, and then we can give either room to either buyer and they will both be happy.</p> <p>Question 3: What if there are two people who are willing to pay $6B for the same room?</p> <p>We increase the price to \\(\\$6\\)B, and at that point, they are indifferent between getting the room for \\(\\$6\\)B and not getting any room (or getting another cheap room), so we can tie-break arbitrarily and give the room to one of them.</p> <p>Question 4: Does buying the room provide additional utility, such as the satisfaction of winning the auction and showing off? We are not modeling this here. It is just that every buyer has some value for each good, but they do not care about what everyone else is getting.</p> <p>Why is a competitive equilibrium an equilibrium?</p> <p>Reminder: In a competitive equilibrium:</p> <ol> <li>Each buyer is matched to their favorite good (given prices), or no good if they don't want any.</li> <li>If no buyer is matched to good \\(j\\), then \\(p_j = 0\\).</li> </ol> <p>Therefore:</p> <ul> <li>By \\((1)\\), no buyer wants to deviate to a different room, they are given their favorite room given the prices.</li> <li>No good is over-demanded (we are matching each buyer to a room, but we are not matching two buyers to the same room), so there is no pressure to increase prices.</li> <li>By \\((2)\\), under-demanded goods are priced at \\(0\\), so we can't decrease their price lower any more.</li> </ul> <p>Important Note</p> <p>Aviad pointed out a notion that is always confusing for students: A competitive equilibrium is the combination of prices and the assignment of guests to rooms together (all of that constitute the equilibrium, not just one or the other).</p> <p>The last condition essentially says that there is always an empty room that is priced at \\(0\\). Buyers compare all available rooms at their respective prices, but they also compare those with the option of not getting a room and not paying any price (staying in a tent for instance).</p> <p>Question: What about seller\u2019s utility?</p> <p>This already captures seller\u2019s utility. What this does not capture is production cost. This definition makes sense if we assume that sellers have \\(0\\) cost, which we will pretend for now, which in practice is not true but we will hide it to keep things simpler.</p> <p>Question: Why if no buyer is matched to a good, then the price is 0?</p> <p>If a good is unpopular and no one wants it, we don\u2019t know if it is because it is priced too high, or because it is bad (e.g. a room in a bad neighborhood). So, we lower the price, and we lower the price, until it reaches \\(0\\). If nobody wants it anyway, we know that the price is not too high.</p>"},{"location":"lecture-6/#competitive-equilibrium-properties","title":"Competitive Equilibrium Properties","text":"<p>Anywhere in the world, we can go on Airbnb (from Brazil on a tablet, from India on a phone), and look up rooms in Paris. Once we have equilibrium prices, Airbnb can show all the rooms, with all the prices, to all the guests around the world, and there are not going to be conflicts (unlike when everyone is trying to \u201cenroll now\u201d in some classes with limited seats). The equilibrium prices coordinate things in such a way that no two people want the same room.</p> <p>In practice, it is an approximate equilibrium, so there might be conflicts, but the main thing that makes Airbnb different from Stanford classes or Burning Man tickets (where prices are fixed and good are over-demanded) is that prices are an approximate equilibrium.</p> <p>From a pure computer science system design perspective (leaving aside economics), equilibrium prices are great because the system is not going to crash (since people want different rooms).</p> <p>Definition: Social Welfare</p> <p>The social welfare of an allocation \\(M\\) is the total buyers' value: \\(\\underset{i}{\\sum} v_{i,M(i)}\\).</p> <p>Question: Why don't we include prices in the definition of social welfare?</p> <p>We did not include prices in the definition of social welfare, even though happiness is equal to the value minus the price, because we count the total utility of everyone, including sellers, so prices cancel out.</p> <p>The price goes from the buyers to the sellers, so it is zero-sum, it does not change the total happiness, while what changes the total happiness is the quality of the matching.</p> <p>In contrast, if we wanted to calculate the total happiness of only the buyers, then we would subtract the prices, and if we wanted to calculate the total happiness of only the sellers, we would only care about the prices (this is called revenue maximization).</p> <p>We are also abstracting a bunch of things, such as production costs (including cleaning fees), and taxes (which may make the government happy).</p> <p>Theorem: First Welfare Theorem</p> <p>If \\((p,M)\\) is a competitive equilibrium, then \\(M\\) is a matching that maximizes social welfare:</p> \\[ \\forall M' \\; \\underset{i}{\\sum} v_{i,M(i)} \\geq \\underset{i}{\\sum} v_{i,M'(i)} \\] <p>Another amazing aspect of competitive equilibrium prices is that they maximize the total happiness (social welfare). In any competitive equilibrium, the matching maximizes the total sum of values. If we compare the matching \\(M\\) to any other matching \\(M'\\), the welfare in \\(M\\) at least the welfare in \\(M'\\). Competitive equilibrium prices are making Airbnb happy (they are making a lot of money), the hosts happy (they rent their houses), and guests happy (they find rooms to stay in).</p> <p>Proof: A competitive equilibrium is optimal.</p> <p>Assume by contradiction that \\(\\exists M' \\; \\underset{i}{\\sum} v_{i,M(i)} &lt; \\underset{i}{\\sum} v_{i,M'(i)}\\).</p> <p>By \\((1)\\), \\(\\underset{i}{\\sum} (v_{i,M(i)} - p_{M(i)}) \\geq \\underset{i}{\\sum} (v_{i,M'(i)} - p_{M'(i)})\\).</p> <p>Therefore, \\(\\underset{i}{\\sum} p_{M(i)} &lt; \\underset{i}{\\sum} p_{M'(i)}\\).</p> <p>But, by \\((2)\\), \\(\\underset{i}{\\sum} p_{M(i)} = \\underset{j}{\\sum} p_j \\geq \\underset{i}{\\sum} p_{M'(i)}\\).</p> <p>We have reached a contradiction, so \\(\\forall M' \\; \\underset{i}{\\sum} v_{i,M(i)} \\geq \\underset{i}{\\sum} v_{i,M'(i)}\\).</p> <p>As a reminder, the First Welfare Theorem tells us that the total sum of values is maximized at the competitive equilibrium matching \\(M\\). It is helpful to remember that the statement about total happiness does not include prices anywhere.</p> <p>We assume that there is a matching \\(M'\\) that violates the inequality in the theorem statement, i.e. \\(M'\\) has some strictly higher total happiness. Every \\(i\\) maximizes their happiness (value minus price) at the equilibrium matching \\(M\\).</p> <p>When we subtract both inequalities, it results that sum of prices in \\(M\\) is less than the sum of prices in \\(M'\\): these are the same prices, but summed over different instances (on the left, over goods that are assigned in \\(M\\), and on the right over the goods that are assigned in \\(M'\\)).</p> <p>However, condition \\((2)\\) in the definition of the competitive equilibrium tells us that any good that is not assigned in \\(M\\) has a price of \\(0\\), so the sum of anything that is assigned in \\(M\\) is the same as the sum of all the prices in general. Thus, the sum of all the prices has to be at least the sum of everything that is assigned in \\(M'\\).</p> <p>On one hand, we have an inequality that says that the sum of the prices of what is assigned in \\(M\\) is strictly less than in \\(M'\\), and on the other hand, we have the opposite inequality, so together, we get a contradiction.</p> <p>What is interesting in this proof is how it uses the second condition of the definition of competitive equilibrium, which says that goods that are not assigned have a price of \\(0\\).</p> <p>Question: Why do we have this inequality where the prices of all the goods is greater than the prices of the goods matched in M\u2019?</p> <p>There are more rooms than guests, so we are not necessarily matching all the rooms, but in \\(M\\) we are matching all the rooms that have a non-zero price. It is possible that something that is matched in \\(M\\) (with a positive price) is not matched in \\(M'\\) (so the price is \\(0\\)), because \\(M'\\) does not necessarily match all the goods, since it is not a competitive equilibrium).</p> <p>Theorem: In the \"Airbnb market model\", a competitive equilibrium always exists.</p> <p>This is non-trivial: We will see later a simple example where each buyer wants two goods and a competitive equilibrium does not exist.</p>"},{"location":"lecture-6/#deferred-acceptance-with-prices","title":"Deferred Acceptance With Prices","text":"<p>Let's consider the following setting:</p> <ul> <li>Assumption: Prices are always in some finite range (e.g., \\(\\$0-\\$1,000\\)), with finite increments (e.g., \\(\\$1\\)).</li> <li>Ordinal Preferences: For each buyer, we construct a list of all the \\((good,price)\\) options, ordered by utility. We can truncate the list a the \\((receive \\;nothing, pay \\; nothing)\\) option.</li> <li>At each iteration of the algorithm: The unmatched buyer whose next-favorite option is \\((j,p)\\) proposes price \\(p\\) to good \\(j\\). Good \\(j\\) tentatively accepts if price \\(p\\) is higher than the prices it was offered so far.</li> </ul> <p>This is a twist on the DA we saw before because it is DA with prices. Here, ordinal preferences are not only established (like in the past) based on ordinarily over rooms, but over combinations of rooms and prices. We can truncate the list at the \\((receive \\;nothing, pay \\; nothing)\\) option, which is like an outside option (e.g. sleep in a tent instead). Note that the room preferences (over buyers) are only based on prices (i.e. rooms are indifferent over buyers).</p> <p>Example</p> <p>Let's consider the following scenario, when Mario, Luigi, and Yoshi are going to Paris and are looking for a place to stay near famous monuments:</p> <ul> <li>Mario is willing to pay \\(18\\) for the room near the Eiffel Tower, and \\(16\\) for the room near the Pasteur Institute.</li> <li>Luigi is willing to pay \\(14\\) for the room near the Eiffel Tower, and \\(6\\) for the room near the Pasteur Institute.</li> <li>Mario is willing to pay \\(12\\) for the room near the Eiffel Tower, and \\(8\\) for the room near the Pasteur Institute.</li> </ul> <p>Here is the result:</p> <p></p> <p>This is an overview of what happened:</p> <ul> <li>When the prices are \\(2\\) for Eiffel Tower or \\(0\\) for Pasteur, Mario is indifferent between both rooms, so Mario proposes \\(0\\) to Pasteur.</li> <li>But Luigi and Yoshi still competing for Eiffel Tower, so they keep raising their prices, until the price for Eiffel Tower is \\(4\\), which means that Yoshi is now indifferent between the two rooms.</li> <li>Then, Yoshi is going to switch back and forth between Paster and Eiffel Tower as Mario and Luigi raise the price for Eiffel Tower (every time one place is over-demanded and Mario and Luigi raise the price, Yoshi switches to the other one)\u2026</li> <li>\u2026 until Yoshi gets priced out.</li> </ul> <p>Note: For more details, refer to slides 37-61.</p> <p>Question: What causes Mario to become indifferent to the Eiffel Tower in the first place?</p> <p>When the Eiffel Tower costs \\(2\\) and Pasteur costs \\(0\\), the difference between both rooms is \\(2\\) more in favor of the Eiffel Tower. Mario\u2019s preference for the Eiffel Tower over Pasteur is \\(2\\) more, which exactly balances the difference of \\(2\\).</p> <p>Here is how to interpret the results:</p> <ul> <li>The price that Mario wants to pay for Pasteur (\\(9\\)) is higher than Yoshi\u2019s (\\(8\\)), and similarly, the price Luigi wants to pay for Eiffel Tower (\\(12\\)) is equal to Yoshi\u2019s (\\(12\\)).</li> <li>Mario could have also paid \\(8\\), but this is just due to how the tie-breaking worked out.</li> <li>This is the matching that maximizes the total values on the edges.</li> <li>This is not only a competitive equilibrium, it is also the unique buyer-optimal competitive equilibrium.</li> </ul> <p>Question: Does the order matter?</p> <p>Just like in DA, order does not really matter, except for when it comes to tie-breaking, which may only matter up to \\(1\\) (increment unit).</p> <p>Question: What is the total social welfare 16 +14?</p> <p>Yes. \\(12+9\\) are the prices that Mario and Luigi pay. The welfare is the value that Mario see in Pasteur (\\(16\\)) and Luigi sees in Eiffel Tower (\\(14\\)). In other words, it is not about how much buyers actually pay for the rooms, but how much they are willing to pay for the room.</p> <p>Question: Are Luigi and Mario determining the prices based on the maximum prices that Luigi is willing to pay?</p> <p>Yes, because these are the prices it takes to price Yoshi out (since there is one more buyer than room, we need to price out one buyer).</p> <p>Claim: DA-with-prices always terminates.</p> <p>The algorithm always terminates because every buyer is either matched to a good or they reach a point where they prefer to pay nothing and buy nothing.</p> <p>The running time of DA with prices is in the order of \\(O(n \\cdot m \\cdot i)\\), where \\(i\\) is the number of increments.</p> <p>The running time is at most the number of buyers (\\(n\\)) times the number of rooms (\\(m\\)) times the number of increments (\\(i\\)), i.e 1,000 increments of $1 in our example. This is really fast when (\\(i\\)) is small (and there is a way to make it faster when (\\(i\\)) is not too small), so it is a really fast algorithm with nice economic properties.</p> <p>The DA with prices algorithm finds a competitive equilibrium:</p> <ol> <li>Each buyer is matched to their favorite \\((good,price)\\). If there were any more preferred \\((good,price)\\), the buyer would have already proposed and get kicked out, so the other goods raise the price higher.</li> <li>If a good is unmatched, its price is zero. Once a buyer proposes to a good, a good is always matched, even if to another buyer (and its price is above \\(0\\)), so the only way a good is unmatched is if it is never proposed to (and its price is \\(0\\)).</li> </ol> <p>Bonus Features (inherited from DA):</p> <ul> <li>Buyer-optimal among competitive equilibria.</li> <li>Buyer-strategyproof.</li> </ul> <p>Note: This is not seller-strategy proof though, since they can set a reserve price, i.e. a minimum price that needs to be met.</p> <p>DA with Prices Recap</p> <p>At each iteration of the algorithm, the unmatched buyer whose next-favorite option is \\((j,p)\\) proposes price \\(p\\) to good \\(j\\). Good \\(j\\) tentatively accepts if price \\(p\\) is higher than the prices it was offered so far.</p> <ul> <li>Proves that a competitive equilibrium exists.</li> <li>Fast running time (for discretized prices).</li> <li>Buyer-optimal and buyer-strategyproof.</li> </ul> <p>Competitive Equilibrium Recap</p> <p>By definition:</p> <ul> <li>Each buyer is matched to their favorite good (given prices).</li> <li>If no buyer is matched to good \\(j\\), then \\(p_j = 0\\).</li> </ul> <p>Properties:</p> <ul> <li>At equilibrium prices, buyers can independently choose their favorite goods without conflicts.</li> <li>A competitive equilibrium maximizes social welfare.</li> <li>A competitive equilibrium always exists and can be found efficiently (DA with prices).</li> </ul>"},{"location":"lecture-6/#recap","title":"Recap","text":"<p>Market Equilibrium Recap</p> <p>Modeling buyers</p> <ul> <li>Ordinal vs. cardinal utilities (expressivity, fairness, combining outcomes).</li> <li>Idiosyncratic vs. fungible goods (and unit-demand vs. combinatorial demand).</li> </ul> <p>Competitive equilibrium (supply meets demand)</p> <ul> <li>Prices coordinate the allocation (i.e. each guest can book their favorite room without conflicts).</li> <li>Proof of existence with DA with prices algorithm (it is also a buyer-strategyproof mechanism).</li> <li>First Welfare theorem (social welfare maximized at equilibrium).</li> </ul>"},{"location":"lecture-7/","title":"Market Failures","text":"<p>April 21, 2025</p> <p>The vanilla assumption at the foundation of classical microeconomics is that a free market (\"invisible hand\") naturally converges to an optimal outcome.</p> <p>Definition: Market Failure</p> <p>When the market fails to converge to an optimal outcome.</p> <p>It is important to understand what can go wrong in market design and which strategies exist for mitigating these issues. In this lecture, we focus on five types of market failures:</p> <ol> <li>Externalities and public goods.</li> <li>Transaction costs.</li> <li>Market thinness/monopolies.</li> <li>Timing issues.</li> <li>Information asymmetry.</li> </ol>"},{"location":"lecture-7/#externalities-and-public-goods","title":"Externalities and Public Goods","text":"<p>This is probably the most important market failure.</p> <p>Definition: Externality</p> <p>A side-effect on someone other than the seller/buyer.</p> <p>In other words, an externality is the net effect a transaction has on everyone else. For instance, if you are watching a lecture online, as a side-effect, your roommate's Wi-Fi is congested.</p> <p>An externality is a market failure when market participants don't have incentives to reduce negative externalities and increase positive externalities.</p> <p>Definition: Public Good</p> <p>Something that belongs to everybody but is owned by nobody.</p> <p>A market failure occurs when market participants are under-incentivized to invest in public goods.</p> <p>Ecological damage is considered as \"the biggest market failure of all time.\" It is a side-effect of many economic activities, from hunting, to farming, and Bitcoin mining.</p> <p>The environment is a public good. Whose it it? Who is vested enough to protect it? When we pollute and hurt nature, it hurts everyone (it has a lot of externality on other people, with a large total effect), and we cannot just let the free market take care of the environment.</p> <p>What can we do about it?</p> <p>Ecological Damage Mitigation Strategies</p> <ul> <li>Pigouvian Tax: Named after British economist Arthur Cecil Pigou, the idea is to tax proportionally to the externality. An example of this is a carbon tax: if you pollute, you have to pay for how much you pollute, which incentivizes you to pollute less. Similarly, if you smoke, there are externalities for other people (such as second-hand smoking and more expensive health care), so it makes sense to tax you.</li> <li>Coasian bargaining: Named after British economist Ronald Coase, the idea is to auction off public goods. Everyone can throw their pollution into the air because no one owns it, but if someone owned all the air, they would charge you for polluting it. However, is it a great idea that someone owns all the air? </li> </ul> <p>Question: Is a library an example of a public good?</p> <p>Yes, it is a great example of a public good.</p> <p>Question: Can the government be a participant in the market, given that libraries and national parks are public goods, but on government land?</p> <p>We usually think of those as public goods and we think of the government not as a participant in the market but like an organization that is formed to take care of those public goods.</p> <p>Key point: Environmental impact is a huge example of market failure, even though it is a fairly simple one.</p>"},{"location":"lecture-7/#transaction-costs","title":"Transaction Costs","text":"<p>Note: Computer science really kicks in to address this market failure.</p> <p>Definition: Transaction Costs</p> <p>Transaction costs are costs associated with making transactions that prevent beneficial trades.</p> <p>Example</p> <p>I would be willing to pay \\(\\$10,000\\) for a boat. You are happy at home, so you would rather have \\(\\$9,900\\) than your boat. Should you sell me your boat?</p> <ul> <li>In theory: Yes! Regardless of price, we would be \\(\\$100\\) happier in total.</li> <li>In practice: Probably not, considering a \\(9\\%\\) sales tax.</li> </ul> <p>What can we do about it? Legally, not much:</p> <p>\"nothing is certain except death and taxes.\"</p> <p>Example</p> <p>I am hungry and willing to pay \\(\\$10\\) for an apple. Someone out there would rather have \\(\\$0.99\\) than their apple. Should I buy the apple from them?</p> <ul> <li>In theory: Yes!</li> <li>In practice: How would I find them? How would I send them the money?</li> </ul> <p>Finding someone with an apple might cost more than \\(\\$10\\), even though they are out there. There is another form of transaction cost (distinct from taxes), associated with search, finding matches, and payment.</p> <p>Transaction Costs Mitigation Strategies</p> <p>Facilitating transactions matters because, even if in theory I would rather have an apple, and someone would rather have \\(\\$0.99\\), finding them and paying them is hard.</p> <p>Lots of companies facilitate transactions: dating apps, Couchsurfing, Airbnb, ride-hailing apps, Craigslist, eBay, Amazon, TaskRabbit, StubHub, Mechanical Turk, etc.</p>"},{"location":"lecture-7/#market-thickness","title":"Market Thickness","text":"<p>Market thinness is actually one reason for high transaction costs.</p> <p>Definition: Thick Market</p> <p>We say that a market is thick if there are many buyers and many sellers.</p> <p>In a thick market:</p> <ul> <li>Sellers and buyers have a lot of options.</li> <li>Prices are at (or close to) market equilibrium.</li> <li>The outcome is welfare-maximizing.</li> </ul> <p>Example of a Thick Market: A market in Lagos, Nigeria. There are lots of people with lots of options to go from, and a lot of competition, so we expect the price to be at or close to market equilibrium.</p> <p>Definition: Thin Market</p> <p>We say that a market is thin if there are few buyers and not many sellers.</p> <p>In a thin market:</p> <ul> <li>There are no buyers in sight.</li> <li>If somes buyers do come, sellers monopolize.</li> <li>The outcome is sub-optimal.</li> </ul> <p>Example of a Thin Market: One motorcycle selling ice cream in the Faysoum Desert, Egypt. There is one seller, and zero buyer. If a buyer shows up in the middle of the desert and really wants ice cream, the seller will hike up the price and take advantage of the fact that there is only one ice cream seller in the desert.</p> <p>Why are monopolies a problem?</p> <p>When we consider sellers' incentives, the First Welfare Theorem extends to goods with reserve prices to cover costs. For example, an Airbnb host should never rent a room below the total of their cleaning cost, insurance, and hotel tax.</p> <p>However, sellers setting reserve prices is not strategyproof (like hospitals in DA). Moreover, sellers have simple, obvious manipulations (unlike hospitals, who had to know what other hospitals and doctors were bidding, sellers do not need that information).</p> <p>Example: Monopoly </p> <p>Let's consider a scenario with 1 seller (\\(true \\; cost = 0\\)) and 1 buyer (\\(value = 100\\)):</p> <ul> <li>Any price between \\(0\\) and \\(100\\) would be a competitive equilibrium.</li> <li>DA-with-prices would suggest \\(p = 0\\) (buyer-optimal).</li> <li>The seller wants to set the reserve price to \\(100\\).</li> </ul> <p>On Airbnb, hosts set prices: nobody is actually running DA-with-prices. Let's recall that DA-with-prices is buyer-strategyproof but super not seller-strategyproof anyway.</p> <p>This is why monopolies are a real problem, especially in a thin market.</p> <p>However, there are ways to get around it:</p> <ul> <li>Thick(er) market: As a market becomes very thick (it is far from a monopoly), then it approaches seller-strategyproofness. We say that it is strategyproof-in-the-large.</li> <li>Information limitation: Sellers still need to know the buyer\u2019s value to set a good reserve price, so if they don\u2019t really have a lot of information, even with a monopoly, then they cannot set a very high reserve price, so information limitation helps.</li> </ul> <p>What can we do about it?</p> <p>Market Thinness Mitigation Strategies</p> <p>If monopolies and thin markets are a problem, then one solution is to build a thick market (or make a market thicker).</p> <p>Solutions include:</p> <ul> <li>Spend a lot of resources on recruiting a lot of early adopters to get a thick market, and then retaining them.</li> <li>Scale markets, for instance by merging existing markets (see example of kidney exchange programs).</li> <li>Batch transactions:<ul> <li>Every once in a while, there is a new patient and a kidney that are available for a match, and we may wonder when we should try to clear the market. In many countries, they wait 3-4 months to have a batch and try to find matches, while in the US, the frequency is much higher, i.e. around 1 week. One reason for this is that there is competition between markets, so if one market is matching every few months, and another is matching every few weeks, then you can match quicker in the latter, but it may not be an optimal match, so there is a trade-off.</li> <li>In the context of ride-sharing/ride-hailing, when you order an Uber ride, it might wait for a couple of minutes to pool you with other people who are ordering a car to find the best matches between drivers and riders.</li> </ul> </li> </ul>"},{"location":"lecture-7/#timing-issues","title":"Timing Issues","text":"<p>Timing Issue #1: Committing to Contracts Too Early (Market Unravelling)</p> <p>In the medical residency (doctor-hospitals) job market pre-NRMP:</p> <ul> <li>1900's-1940's: The market was decentralized, and hospitals were racing to make offers earlier.</li> <li>If other hospitals were making offers in December, then you had to make offers in November (slightly more uncertainty, but much less competition).</li> <li>If other hospitals were making offers in Novenber, ...</li> <li>... by 1945, hospitals were making job offers to first-year students.</li> </ul> <p>This is a market failure because commitments were made too early: first-year students don\u2019t know which speciality they want and how good they really are. There was a lot of uncertainty, it was hard to tell whether doctors were a good match, yet because of uncertainty, hospitals were sending offers too early.</p> <p>However, now, with the centralized system, offers are sent during the final year of study, so doctors know better what they want.</p> <p>Timing Issue #2: Exploding Offers</p> <p>An exploding offer is like a job offer that requires a very short response.</p> <p>One reason why one wants to give exploding offers is that, if a job offer is turned down, an employer needs to scramble to find someone else to fill the position.</p> <p>This type of constraint is amplified when:</p> <ul> <li>There is a limited time window for making offers, since you don\u2019t have more time to turn around.</li> <li>An employer is constrained to only hire exactly one person. If you hire 10 people, you can make 20 offers, but if you hire 1 person, you need to know very fast if they are going to accept the offer or not, so you put the pressure on the candidate.</li> </ul> <p>The de Gea transfer</p> <ul> <li>24 hours is a short time window to make a goalie transfer decision worth millions of euros.</li> <li>The reason of this is that FIFA has a limited time window for when you are allowed to transfer soccer players.</li> <li>Each team has exactly one starting goalie, so they cannot make offers to three goalies (they don\u2019t want to have 2 and they don\u2019t want to have 0).</li> </ul> <p>Note: David de Gea's transfer to Real Madrid from Manchester United in 2015 failed due to administrative issues, specifically the late submission of paperwork. The deal was agreed upon, including a part-exchange deal with Real Madrid's Keylor Navas. However, the paperwork was not submitted in time for the La Liga deadline, leading to the transfer collapse.</p> <p>An Extreme Case</p> <p>According to a 2005 applicant for federal judicial clerkships:</p> <p>\"I received the offer via voicemail while I was in flight to my second interview. The judge actually left 3 messages: the first to make the offer, the second to tell me that I should respond soon, and the third to rescind the offer. It was a 35-minute flight.\"</p> <p>Timing Issue #3: Not Waiting For The Market To Clear</p> <p>This is related to hiring psychologists:</p> <ul> <li>Starting at 9am, employers can call psychologists to offer them a position.</li> <li>They may say yes, and then call back and decline because they got a better offer.</li> <li>Then employers had to go back to the next psychologist on their preference list.</li> </ul> <p>You would hope that this was an employer-optimal DA mechanism.</p> <p>In practice:</p> <ul> <li>Employers were worried that psychologists would reject them at 3:59pm, right before the 4:00pm deadline, so they asked that candidates commit to an accepted offer and do not switch.</li> <li>Candidates started accepting offers that may have been their safety choices rather than their favorite choice.</li> <li>Then, employers started making commitments earlier and earlier.</li> </ul> <p>This resulted in a suboptimal matching and it was far from being strategyproof because psychologists started to strategize and think about whether they should commit to an employer that was a safety choice even though they may like another employer better, and therefore, it was not stable.</p> <p>This is another example of market unraveling.</p> <p>What can we do about it?</p> <p>Timing Issues Mitigation Strategies</p> <p>Solutions to deal with timing issues include:</p> <ul> <li>Use centralized matching systems to prevent this type of market unraveling, like we have seen for doctors and hospitals.</li> <li>Set rules that do not allow agents to make offers before certain dates, but in practice, these rules tend to fail, because there are incentives to bend the rules and still make early offers and ask for early commitments</li> <li>In practice, what does seem to work better is to have rules that allow candidates to accept and then cancel exploding offers, because if that is the norm, then it takes the kick out of exploding offers (what is the point of exploding offers if candidates can say yes immediately and then back out?).</li> </ul> <p>Question: Why does eliminating exploding offers solve this issue?</p> <p>For example, in the US, PhD candidates must accept or decline offers by April 15, and everyone know this is the rule. If Stanford says that candidate need to respond by April 1st, then the market unravels.</p> <p>But if the norm is that everyone knows that students can accept Stanford by April 1st, but then still change their mind and go to Berkeley by April 14th, then there is no point for Stanford to even ask candidates to accept by April 1st.</p> <p>By eliminating the ability for candidates to commit to exploding offers, it takes away the ability from universities to make exploding offers in the first place, and prevents the market from unraveling.</p>"},{"location":"lecture-7/#information-asymmetries","title":"Information Asymmetries","text":"<p>Cr/NC vs. Letter Grade Game (In Class)</p> <p>At LovesFun University, students can choose between Cr/NC (Pass/Fail) and a letter grade after seeing their final grade for a class. It always counts for their major.</p> <p>In this in-class game, groups were asked to pick a random mock grade and discuss whether they wanted to opt for Cr/NC or a letter grade.</p> <p>Then, the following discussion questions were asked:</p> <ol> <li> <p>When you see an internship applicant with a Cr/NC on their transcript, what do you think their letter grade must have been? Answer: Probably a B or a C.</p> </li> <li> <p>What is the equilibrium of this game? Answer: The equilibrium of this game is when only people who got a C- take the credit instead of the letter grade. One caveat though is that employers do not really read transcripts, they care about the GPA, so a credit can be more favorable than a grade that lowers the GPA.</p> </li> </ol> <p>Question: What if a team that has a C- wants to show a letter grade?</p> <p>In practice, it does not make sense, because they would prefer to be mixed with people who got a B or a C on average rather than show a worse grade.</p> <p>Example: Market for Lemons</p> <p>Suppose that you want to buy a used car. You are willing to payer more for a car in good condition. Sellers are willing to sell bad cars for less (they are eager to replace their car).</p> Car Condition Buyer's Value Seller's Value Good 12 10 Bad 6 4 <p>Regardless of whether a car is good or bad, there is a positive gain from trade, i.e. the seller and the buyer are happier if they transact. This is why in an optimal market, you buy some car.</p> <p>Here, we say that there is information asymmetry, because the seller knows the car condition, but the buyer doesn't.</p> <p>Let \\(g \\in [0,1]\\) be the fraction of cars in the market that are in good condition. Let \\(h \\leq g\\) be the fraction of cars in good condition also available for sale.</p> <p>What do we expect at market equilibrium?</p> <p>Possible bad equilibrium:</p> <ul> <li>\\(h = 0\\), i.e. only bad cars are for sale.</li> <li>\\(Price \\leq 6\\) since buyers assume that cars for sale are in bad condition (i. the price has to be between \\(4\\)\u2014how much sellers want for their bad cars\u2014and \\(6\\)\u2014how much sellers are willing to pay for bad cars).</li> <li>Good sellers don't want to sell their cars (which is why \\(h = 0\\) is indeed an equilibrium).</li> </ul> <p>Possible good equilibrium:</p> <ul> <li>\\(h = g\\), i.e. all sellers are trying to sell their cars.</li> <li>The buyers' willingness to pay is: \\(12g + 6 \\cdot (1-g) = 6 + 6g\\) (i.e. \\(12\\) times the fraction of good cars, plus \\(6\\) times the fraction of bad cars).</li> <li>For good sellers to stay in the market, we need to have \\(6 + 6g \\geq 10\\), i.e. \\(g \\geq \\frac{2}{3}\\) (the value that buyers are willing to pay for a car in an unknown condition must at least be the value that sellers of good cars are expecting to create an expectation of a good deal.)</li> </ul> <p>This is not great, but it can get worse.</p> <p>Example: Market for Lemons (Even Worse)</p> <p>Let's assume that the market is split \\(\\frac{1}{3}\\), \\(\\frac{1}{3}\\), \\(\\frac{1}{3}\\) between good cars, bad cars, and lemons:</p> Car Condition Buyer's Value Seller's Value Good 12 10 Bad 6 4 Lemons 0 0 <p>If everyone sells, the buyer's value is at most \\(\\frac{1}{3} \\cdot (12 + 6 + 0) = 6\\). So, good sellers exit the market.</p> <p>This means that the buyer's value becomes \\(\\frac{1}{2} \\cdot (6 + 0) = 3\\). So, bad sellers also exit the market.</p> <p>Example: Market for Health Insurance</p> <p>The good equilibrium is that everyone buys a reasonably priced health insurance and everyone who has health insurance is taken car of.</p> <p>The problem is that insurance companies have an incentive to sell only/mostly to healthy people. For instance, selling health insurance to college students is probably a good deal (they are younger and in better health than the average person).</p> <p>If companies can sell health insurance targeted at healthier individuals/demographics, then competitors are left with less healthy people who may be more expensive to insure, which drives the prices up.</p> <p>However, healthy people do not want to buy health insurance when the prices are high, only people who have higher health costs are going to buy when prices are high.</p> <p>That means that health insurance companies have to raise their prices even more. In the extreme, perhaps no one has health insurance, which is a very bad equilibrium, because only unhealthy people buy very expensive health insurance and no one else is insured because it\u2019s too expensive.</p> <p>Example: Clickbait Content</p> <p>Content creators know whether their content is actually good content or not.</p> <p>In the good equilibrium, we mostly have interesting content and a lot of audience.</p> <p>However, there is the something called moral hazard, which is that it is easier and cheaper to create bad content with a clickbait-y title.</p> <p>So, if the number of clicks remains the same, then creators are incentivized to resort to clickbait. As a consequence, perhaps the good creators who do not do this go bankrupt because they cannot meet the market prices (given all the clickbait creators who flood the market).</p> <p>This results in a bad equilibrium, where we have mostly clickbait and users are not interested in it, so they don\u2019t click it.</p> <p>Definition: Adverse Selection</p> <p>Only lemons stay in the market due to information asymmetry.</p> <p>What can we do about it?</p> <p>Adverse Selection Mitigation Strategies</p> <ul> <li>Provide more information to both sides of the market<ul> <li>For example, if you sell your car, there are mandatory disclosures (you are required by law in many states to say if you have problems with your car).</li> <li>Before Obamacare, by law, you had to declare pre-existing health conditions, and that could hike up health insurance prices.</li> <li>You can invest in reputation systems (rating on Yelp, Amazon), which comes with lots of great incentives in CS questions.</li> </ul> </li> <li>Disallow the use of information on both sides of the market<ul> <li>For instance, with universal healthcare, you have to give health insurance to everyone, which does not allow health insurance companies to use information to pick and choose college students.</li> <li>In the stock market, there are regulations about insider trading, where if you have insider information about whether a stock is going to go up or down, you cannot use this information to invest.</li> </ul> </li> <li>Disincentivize lemons<ul> <li>For example, if a dealer has to give a guarantee over a car, they do not have an incentive to sell lemon cars (this is why many states disallow selling cars \u201cas is\u201d).</li> <li>In online ads, ad platforms may not only consider the price of the ads, but also the perceived quality of the ads, for instance if it is for a better product, which incentivizes advertisers to create higher quality content (ads).</li> </ul> </li> </ul> <p>Question: Why do we prefer providing more information for cars and less information for health insurance?</p> <p>One perspective is that, when people buy used cars, the side with the least information (the buyer) is an individual, while when people buy health insurance, the side with the most information (the buyer) is an individual, and maybe we want to side with and protect individuals, rather than larger companies.</p> <p>There may also be a notion of luxury (for a car) vs. necessity (for healthy insurance), all the more since, with a car, some things will break based on how you drive (so you have incentives to drive well), while you cannot prevent yourself from getting sick (even though you still have incentives to eat well and exercise).</p>"},{"location":"lecture-7/#recap","title":"Recap","text":"<p>Market Failures Recap</p> <p>A market failure occurs when a market fails to converge to an optimal outcome.</p> <p>We have seen five types of market failures:</p> <ol> <li>Externalities and public goods.</li> <li>Transaction costs.</li> <li>Market thinness.</li> <li>Timing issues.</li> <li>Information asymmetry.</li> </ol> <p>Mitigation strategies exist for all five types of market failures.</p>"},{"location":"lecture-8/","title":"Single-Unit Auctions","text":"<p>April 23, 2025</p> <p>We have seen that, when supply meets demand, we reach a market clearing price. Equilibrium prices are amazing, because each guest independently chooses their favorite room, with no conflicts, and by the First Welfare Theorem, this allocation is optimal (it maximizes total happiness).</p> <p>Now, we are switching from markets to auctions. We will mostly focus on auctions for selling goods, although procurement auctions to buy goods are also very important.</p> <p>Why do we need auctions?</p> <p>When we don\u2019t know how to price what we sell:</p> <ul> <li>Non-example: If we are selling a generic product, with lots of other sellers, and lots of buyers, and there is price discovery (the market figures out the right price), we don\u2019t need an auction, since we already know how much our good should cost.</li> <li>Governments around the world, especially the US government, have a monopoly on the wireless spectrum, and they auction licenses to broadcast certain frequencies. At these auctions, the government buys some frequencies back from TV stations that don\u2019t need as much any more, and sells them to cell phone companies. Auctions help set the price.</li> <li>If you have a nice product, such as a family heirloom, we may have no idea how much it should cost. An auction (such as eBay) helps discover the price.</li> <li>Products with specific attributes, such as ads. There is a search engine, and a user searching for pizza, at a specific time, in a specific town, so it is a very specific case, and we don\u2019t know exactly how much it is worth to pizzerias next to this user. Ad auctions help determine the price.</li> </ul> <p>When the buyers have the computational power and the patience to bid:</p> <ul> <li>When we we are selling something very expensive, auctions may be a good idea. For instance, in the wireless spectrum auctions, in 2021, the government auctioned $80 billion worth of spectrum, and the whole auction process took 3 months. The TV stations and cell phone companies were willing to spend this time to figure out exactly how to bid and participate in this long process, which makes sense since it is a very expensive product.</li> <li>Ad auctions are also a popular mechanism. These are super fast (split of a second): when you search something, by the time you get the result, there is already an auction happening behind the scenes. These are automated bids: it is all happening algorithmically, in a very short amount of time.</li> <li>Non-example: If a child has some fever, their parents are not going to wait a week to win an auction on eBay (they just go to CVS and pay the price).</li> </ul> <p>Why do auctions matter for computer scientists?</p> <ul> <li>Ad auctions fund many CS initiatives (research, product monetization, etc.).</li> <li>Fast auctions require algorithmic bidders (e.g., ad auctions).</li> <li>Complex auctions require algorithmic auctioneers (e.g., the 2017 FCC wireless spectrum auctions required solving 75,000 large-scale NP-hard problems).</li> </ul>"},{"location":"lecture-8/#auctions-to-only-one-buyer","title":"Auctions To Only One Buyer","text":"<p>Motivation #1: Digital Goods</p> <p>Let's assume that we have a pool (or an \"urn\") of users subscribed to a free online product. We want to know how much to charge for the premium version of the product.</p> <p>Digital goods are goods with unlimited supply and zero/negligible marginal cost to create another copy. We can estimate the demand curve based on historical data or based on a survey, for instance.</p> <p>We want to determine how much to charge for this premium product, with the objective of maximizing revenue, which is the price we charge times the number of buyers. Revenue is represented by a rectangle under the curve.</p> <p> </p> <p>At a price of \\(\\$8\\), we lost one buyer (\\(4\\) vs. \\(5\\)), but the revenue is higher (\\(\\$32\\) vs. \\(\\$30\\)).</p> <p>Myerson's Theorem</p> <p>Given a demand curve \\(D\\), Myerson's theorem provides a formula to determine the price \\(p(D)\\) that maximizes revenue, i.e. the rectangle under the curve.</p> <p>This is the revenue-optimal way to sell one item to one buyer: we fix the price and we charge the price from this one buyer.</p> <p>Note: This is non-trivial. There are many extensions of this simple setting where the theorem does not hold.</p> <p>Motivation #2: Ad Auctions</p> <p>In ad auctions, we tend to have a lot of data and very good machine learning, which enables very accurate targeting.</p> <p>Let's consider the case of a user looking for pizza in a given town: there is only one possible buyer, and in the extreme case, we have only one advertiser (because there is only one pizza place in town).</p> <p>To figure out how much to charge this advertiser, we can take a Bayesian approach: we don\u2019t know the user\u2019s value (otherwise we would just charge exactly the user\u2019s value and make as much money as we can), but we can generate a model that is like a distribution of the possible values the user is likely to have.</p> <p>If we have that distribution, it is like drawing a random buyer from that distribution, and for the seller, the cumulative distribution over the possible buyer values is going to be equivalent to the demand curve.</p> <p>This works similarly as the previous example of a digital good auction. Instead of having the Price vs. the Demand (like we had in market demand curves), here we have the Price vs. the Probability that the buyer will buy:</p> <p></p> <p>From here, we apply Myerson\u2019s theorem again, where the formula is going to give the best rectangle under the curve to maximize the expected revenue.</p>"},{"location":"lecture-8/#auctions-with-many-buyers","title":"Auctions With Many Buyers","text":"<p>In this lecture, we focus on the simplest kind of auction: single-item auction.</p> <p>Basic Model: Single-Item Auction</p> <ul> <li>Each bidder \\(i\\) has a value (i.e. their willingness to pay\") \\(v_i\\) for receiving the item. Bidder \\(i\\) know \\(v_i\\).</li> <li>The seller doesn't know \\(v_i\\). This is why we need an auction mechanism.</li> <li>The utility of a bidder who receives the item and pays \\(p_i\\) for it is: \\(u = v_i - p_i\\).</li> <li>In contrast, the utility of a bidder who doesn't receive the item and still pays \\(p_i\\) is: \\(u = -p_i\\).</li> </ul> <p>Note: We are still auctioning only one item. Auctioning multiple items is super complicated: this is where Myerson\u2019s theorem completely breaks, a,d there is a lot of complicated math involved.</p> <p>Definitions: Types of Auctions</p> <p>Open-Cry Auctions</p> <ul> <li>Multi-round iterations.</li> <li>Ascending-price auctions (English/Japanese) vs. descending-price auctions (Dutch).</li> <li>The FCC spectrum was a very complicated auction: it was a mix of ascending-price and descending-price auctions, because they were both buying and selling frequencies.</li> <li>In this lecture, we are not discussing open-cry auctions: instead, we are focusing on...</li> </ul> <p>Sealed-Bid Auctions</p> <ol> <li>Each bidder sends a secret bid \\(b_i\\) to the auctioneer (in a \"sealed envelope\").</li> <li>The auctioneer decides how to allocate the item and takes the payment from the bidder.</li> </ol> <p>Definition: First-Price Auction</p> <ul> <li>Each bidder submits a bid \\(b_i\\).</li> <li>The highest bidder \\(i\\) receives the item.</li> <li>The highest bidder pays \\(b_i\\).</li> </ul> <p>Note: This definition is not complicated but it is important.</p> <p>Do we expect bidders to bid higher or lower than their true value \\(v_i\\)?</p> <p>We expect bidders to bid lower than their true value: if there is just one bidder, they are just going to bid \\(0\\) and still get the item, so everyone wants to bid as low as they can.</p> <p>Do we expect bids to go up or down as competition (i.e. the number of bidders) increases?</p> <p>We expect bids to go up when competition increases: in the extreme case, if there is just one bidder, they are just going to bid \\(0\\) and still get the item, but as there is more competition, the bids will approach each bidder\u2019s true value (they cannot expect to get the item for much less than their true value).</p> <p>Definition: All-Pay Auction</p> <ul> <li>Each bidder submits a bid \\(b_i\\).</li> <li>The highest bidder \\(i\\) receives the item.</li> <li>Every bidder \\(j\\) pays \\(b_j\\).</li> </ul> <p>Do we expect bidders to bid higher or lower than their true value?</p> <p>We expect bidders to bid lower than their true value.</p> <p>Do we expect bids to go up or down as competition (i.e. the number of bidders) increases?</p> <p>It is not obvious whether bidders should increase or decrease their bid when competition increases: on the one hand, if there is more competition, you want to bid higher to win, but on the other hand, you don\u2019t want to bid too high and lose to a competitor.</p> <p>Do we expect the price to be higher or lower than wit first-price auctions?</p> <p>Intuitively, bids should be lower than with first-price auctions.</p> <p>What are some applications of all-pay auction in practice?</p> <p>In practice, all-pay auctions are rarely used. They are useful for modeling sunk-cost of losers in situations like political lobbying, where players are putting a lot of money to get people to vote yes/no on a ballot, and regardless of the outcome, the money for campaigning is gone. Another example will be covered in the last lecture of the quarter.</p> <p>Note: In general, all-pay auctions are very hard for bidders to determine how to bid. They are very not truthful.</p> <p>Definition: Second-Price Auction</p> <ul> <li>Each bidder submits a bid \\(b_i\\).</li> <li>The highest bidder \\(i^\\star\\) receives the item.</li> <li>The highest bidder pays the second-highest bid.</li> </ul> <p>Note: This is an important type of auction.</p> <p>Do we expect bidders to bid higher or lower than their true value?</p> <p>We expect bidders to bid exactly their true value: this is strategy-proof.</p> <p>Theorem: Second-price auction is strategyproof.</p> <p>Formally, truthful bidding is a dominant strategy, i.e. for any fixed bids \\(b_j\\), for all \\(j \\neq i\\), bidding \\(b_i = v_i\\) is optimal for bidder \\(i\\).</p> <p>Proof</p> <p>Let \\(b^{(-i)} = \\underset{j \\neq i}{max} \\; b_j\\).</p> <ul> <li>If \\(v_i &lt; b^{(-i)}\\):<ul> <li>Bidder \\(i\\) prefers to miss out on the item than to pay \\(b^{(-i)}\\).</li> <li>So, any \\(b_i &lt; b^{(-i)}\\) is optimal. In particular, \\(b_i = v_i\\) is an optimal bid.</li> </ul> </li> <li>If \\(v_i &gt; b^{(-i)}\\):<ul> <li>Bidder \\(i\\) prefers to pay \\(b^{(-i)}\\) and receive the item.</li> <li>So, any \\(b_i &gt; b^{(-i)}\\) is optimal. In particular, \\(b_i = v_i\\) is an optimal bid.</li> </ul> </li> </ul> <p>Truthful bidding is a dominant strategy: we don\u2019t care what other bidders do (they can bid whatever they want), bidding our true value is always optimal.</p> <ul> <li>If our value is lower than the max of everyone else\u2019s bid, we would rather pay nothing and get nothing, than pay over our value, so we might as well bid our true value.</li> <li>If our value is higher than the max of everyone else\u2019s bid, we would rather pay the max and get the item, and again, we might as well bid our true value.</li> <li>If our value is exactly equal to the max of everyone else\u2019s bid, then we are indifferent between getting the item or not, and we might as well bid my true value.</li> </ul> <p>Extreme case: If there is no one else in the auction, the max of everyone else\u2019s bid is \\(0\\), we could bid our true value, pay our true value, and have \\(0\\) utility, or we could shade our bid, pay \\(0\\), and still get the item.</p> <p>Theorem: Second-price auction is individually rational.</p> <p>As long as bidder \\(i\\) bids \\(b_i = v_i\\), the utility \\((u = v_i - p)\\) is non-negative.</p> <p>In other words, as long as we bid our true value, we are never going to pay more than our true value. The utility is non-negative, unlike with all-pay auction where we can have negative utility.</p> <p>Proof</p> <p>Let \\(b^{(-i)} = \\underset{j \\neq i}{max} \\; b_j\\).</p> <ul> <li>If \\(v_i &lt; b^{(-i)}\\): \\(b_i = v_i\\) isn't maximal. No payment occurs, no item is received, so \\(u = 0\\).</li> <li>If \\(v_i &gt; b^{(-i)}\\): \\(u = v_i - b^{(-i)} \\geq 0\\).</li> </ul> <p>Theorem: Second-price auction maximizes the social welfare (where social welfare is the total utility of all bidders and the seller).</p> <p>When everyone bids their true value (which we expect, since it is a truthful auction), then second-price auction maximizes social welfare, i.e. the total happiness of the bidders and the seller, because the bidder with the bidder with the highest value gets the item.</p> <p>This type of auction sounds fantastic: why don't we use it all the time?</p> <p>Second-price auction it is not optimal for the seller, and usually, the seller sets the rules for the auction.</p>"},{"location":"lecture-8/#sellers-revenue","title":"Seller's Revenue","text":"<p>Which type of auction maximizes the seller's revenue: first-price auction, second-price auction, or all-pay auction?</p> <ul> <li>Maybe first-price auction because we charge the full bid?</li> <li>Maybe all-pay auction because we charge everyone?</li> <li>Maybe second-price auction because there is no incentives the shade bids?</li> </ul> <p>Example 1: Full Information</p> <p>Consider the case of a second-price auction, which is strategyproof, with two bidders:</p> <ul> <li>Alice: \\(b_A = v_A = 1\\).</li> <li>Bob: \\(b_B = v_B = 2\\).</li> </ul> <p>Let\u2019s pretend that everyone knows everyone else\u2019s value. Alice bids \\(1\\) and Bob bids \\(2\\). Bob wins the auction: he pays Alice\u2019s bid, so the revenue is \\(1\\).</p> <p>Let's now look at the same example, but with first-price auction instead of second-price auction.</p> <p>Example 1: Full Information</p> <p>Consider the case of a first-price auction, with the same two bidders:</p> <ul> <li>Alice: \\(b_A = 0.99\\) and \\(v_A = 1\\).</li> <li>Bob: \\(b_B = 1\\) and \\(v_B = 2\\).</li> </ul> <p>Assume bids in increments of \\(0.01\\), and random tie-breaking.</p> <p>Alice bids \\(0.99\\), and Bob bids \\(1\\). Alice does not have incentives to increase her bid, because she does not want to pay more than \\(1\\). Bob neither wants to increase his bid (he would have to pay more) nor decrease his bid (otherwise he would not get the item). This is the unique\\(^\\star\\) Nash equilibrium, and the revenue is \\(1\\).</p> <p>\\(^\\star\\)A bid of \\(1\\) from Alice and a bid of \\(1.01\\) from Bob would also be a Nash equilibrium, and the revenue would be \\(1.01\\) instead of \\(1\\). The above solution is essentially the unique Nash equilibrium if we ignore this type of increments and tie-breaking (if both Alice and Bob bid \\(0.99\\), this is also an equilibrium).</p> <p>Example 2: Bayesian</p> <p>Consider the case of a second-price auction, which is strategyproof, with two bidders:</p> <ul> <li>Alice: \\(v_A \\approx Uni[0,1]\\).</li> <li>Bob: \\(v_B \\approx Uni[0,1]\\).</li> </ul> <p>\\(b_A = v_A\\) and \\(b_B = v_B\\) are dominant strategies.</p> <p>So, in expectation, \\(Revenue = E[min \\; \\{v_A,v_B\\}] = \\frac{1}{3} \\)</p> <p>This is a Bayesian example: Alice does not know Bob\u2019s value, Bob does not know Alice\u2019s value, and the auctioneer does not know any of their values.</p> <p>We assume that we draw both Alice and Bob's values, respectively (independently), uniformly at random between 0 and 1. Because this is strategyproof, regardless of what Alice bids, Bob wants to bid their true value, and by symmetry, the same is true for Alice: we expect their bids to be their true values.</p> <p>So, in expectation, the revenue is the expectation over the minimum of their true values, since the higher bid is going to win and pay the lower bid.</p> <p>Note: The expectation of the minimum of two independent random variables drawn uniformly between zero and one is \\(\\frac{1}{3}\\), hence the result.</p> <p>Example 2: Bayesian</p> <p>Consider the case of a first-price auction, with the same two bidders:</p> <ul> <li>Alice: \\(v_A \\approx Uni[0,1]\\).</li> <li>Bob: \\(v_B \\approx Uni[0,1]\\).</li> </ul> <p>The challenge is that Alice doesn't know \"which\" Bob she is bidding against. How should we model this situation?</p> <p>Definition: Bayesian Nash Equilibrium</p> <ul> <li>\\((Mixed = randomized)\\) is the strategy for each Alice.</li> <li>\\((Mixed = randomized)\\) is the strategy for each Bob.</li> </ul> <p>Alice and Bob are drawn at random from their distributions.</p> <p>For each Alice, her strategy is optimal in expectation over the Bobs and their strategies.</p> <p>Every possible Alice has some possibly mixed strategy and every possible Bob has some possibly mixed strategy.</p> <p>We draw one Alice and one Bob from their distributions, and the Alice and the Bob that we draw play their respective strategies.</p> <p>Why is this an equilibrium? Because no possible Alice that we can draw, given the strategy of all the possible Bobs, wants to deviate from their strategy.</p> <p>Example 2: Bayesian (Cont'd)</p> <p>Claim: In equilibrium, \\(b_A = \\frac{v_A}{2}\\) and \\(b_B = \\frac{v_B}{2}\\).</p> <p>So, in expectation, \\(Revenue = E[max \\; \\{b_A,b_B\\}] = \\frac{E[max \\; \\{v_A,v_B\\}]}{2} =  \\frac{1}{3}\\).</p> <p>The max of two random variable drawn uniformly between \\(0\\) and \\(1\\) is \\(\\frac{2}{3}\\). Indeed, if the expectation of the min is \\(\\frac{1}{3}\\), we know that the expectation must be \\(1\\), so we see that expectation of the max is \\(\\frac{2}{3}\\). Therefore, we have \\(\\frac{2}{3}\\) divided by \\(2\\), which is equal to \\(\\frac{1}{3}\\).</p> <p>Interestingly:</p> <ul> <li>In Example 1: Full Information, where \\(v_A = 1\\) and \\(v_B = 2\\), first-price auction and second-price auction both had \\(Revenue = 1\\).</li> <li>In Example 2: Bayesian, where \\(v_A = Uni[0,1]\\) and \\(v_B = Uni[0,1]\\), first-price auction and second-price auction (as well as all-pay auction) had \\(Revenue = \\frac{1}{3}\\).</li> </ul> <p>This is not a coincidence.</p> <p>Theorem: Revenue Equivalence Theorem</p> <p>At equilibrium, expected payments are fully determined by the auction's allocation rule.</p> <p>In other words, in any auction that always gives the good to the max-value buyer, in expectation every buyer pays the same, and in particular, revenue is the same.</p> <p>In all auction types seen so far, the allocation rule was that the bidder with the highest true true always got the item, which is why they all have the same revenue.</p> <p>However, this is not always satisfied in real life, for example in the following cases:</p> <ul> <li>When a bidder overbids in a second-price auction: if Alice bids \\(100\\) and Bib bids \\(0\\), then Alice pays \\(0\\) and gets the item. This is an equilibrium, although not a stable one.</li> <li>If we do the math for the all-pay auction, Alice and Bob want to use randomized strategies at equilibrium, and it is not always true that Bob gets the item, even though Bob\u2019s true value is higher.</li> <li>A seller may get more revenue by deciding to not allocate the item, so they set a reserve price accordingly.</li> </ul>"},{"location":"lecture-8/#bayesian-revenue-maximization-with-n-buyers","title":"Bayesian Revenue Maximization: With \\(n\\) Buyers","text":"<p>Theorem: Myerson's Theorem</p> <p>Given a distribution \\(D\\), where \\(p(D)\\) is given by the formula that maximizes the area under the curve and gives the revenue-optimal way to sell one item to one buyer, with \\(n\\) buyers, each drawing value from \\(D\\) independently (iid), the revenue-optimal auction is a second-price auction where the reserve price is \\(p(D)\\).</p> <p>Definition: Second-Price Auction With Reserve Price</p> <ul> <li>Each bidder submits a bid \\(b_i\\).<ul> <li>Let \\(i^\\star\\) be the bidder with the highest bid \\(b_{i^\\star}\\).</li> <li>Let \\(b^{(-i)} = \\underset{j \\neq i^\\star}{max}\\) be the second-highest bid.</li> </ul> </li> <li>If \\(b_{i^\\star} \\geq p(D)\\), then \\(i^\\star\\) receives the items and pays \\(max{b^{(-i)}, p(D)}\\).</li> <li>If \\(b_{i^\\star} &lt; p(D)\\), then the seller keeps the item and no payment occurs.</li> </ul> <p>What is a second-price auction with reserve price?</p> <ul> <li>Every bidder is still submitting a sealed bid (this is a sealed-bid auction).</li> <li>We start from the highest bid, and the second highest bid receives the item, if they beat the reserve price: what the bidder is paying is the max of the second bid and the reserve price.</li> <li>If the second highest bid does not beat the reserve price, then the seller keeps the item.</li> </ul> <p>This generalizes the optimal auction for single buyers for n buyers. This also generalizes the second-price auction if we set the reserve price, i.e. \\(p(D)\\), to \\(0\\) or negative infinity. This is equivalent to the seller also participating in the auction, with the seller\u2019s bid being the reserve price, i.e. \\(p(D)\\).</p> <p>This means that it is strategyproof for the buyers, but it is not exactly strategyproof for the seller, because they have to figure out the optimal reserve price.</p> <p>Question: Do buyers know the reserve price?</p> <p>Since we usually analyze this in equilibrium, it is always a dominant strategy for the buyers to bid their true values.</p> <p>How does the seller determines the optimal reserve price?</p> <p>It is the same reserve price as with \\(1\\) buyer: even when there are \\(n\\) buyers, we still want to look at the area under the curve to set the reserve price and maximize revenue.</p> <p>Interestingly, as the competition increases, the chance that the reserve price is relevant goes down, because the chance that there are two buyers with bids over the reserve price increases.</p> <p>Theorem: Bulow-Klemperer Theorem</p> <p>\\(Revenue(Second-Price \\; Auction \\; with \\; n+1 \\; buyers) \\geq Revenue(Second-Price \\; Auction \\; with \\; Reserve \\; Price \\; and \\; n \\; buyers)\\) </p> <p>Another way of formalizing this intuition (that as competition increases, we don\u2019t need a reserve price) is this: the revenue with vanilla second-price auction (without a reserve price) but with one more bidder (i.e. \\(n+1\\) bidders) is at least the revenue with the optimal reserve price with \\(n\\) buyers.</p> <p>In other words, with \\(4\\) buyers, we can maximize revenue if we set the right reserve price rather than without a reserve price at all. However, with \\(5\\) buyers without a reserve price, we will have at least as much revenue as if we set the right reserve price with \\(4\\) buyers.</p> <p>In practice, we can either invest in the Machine Learning department to set the right reserve price, or invest in the recruiting department to get one more buyer, and the revenue from the latter will be at least the revenue from the former.</p>"},{"location":"lecture-8/#recap","title":"Recap","text":"<p>Auctions Recap</p> <p>First-Price Auction</p> <ul> <li>Each bidder submits a bid \\(b_i\\).</li> <li>The highest bidder \\(i\\) receives the item.</li> <li>The highest bidder pays \\(b_i\\).</li> </ul> <p>Second-Price Auction</p> <ul> <li>Each bidder submits a bid \\(b_i\\).</li> <li>The highest bidder \\(i^\\star\\) receives the item.</li> <li>The highest bidder pays the second-highest bid.</li> </ul> <p>Secont-price auction is strategyproof, individually rational, and maximizes social welfare.</p> <p>Revenue Equivalence Theorem</p> <p>At equilibrium, expected payments are fully determined by the auction's allocation rule. In other words, in any auction that always gives the good to the max-value buyer, in expectation every buyer pays the same, and in particular, revenue is the same.</p> <p>Revenue Maximizing Auctions Recap</p> <p>Theorem: Myerson's Theorem</p> <p>Given a distribution \\(D\\), where \\(p(D)\\) is given by the formula that maximizes the area under the curve and gives the revenue-optimal way to sell one item to one buyer, with \\(n\\) buyers, each drawing value from \\(D\\) independently (iid), the revenue-optimal auction is a second-price auction where the reserve price is \\(p(D)\\).</p> <p>Theorem: Bulow-Klemperer Theorem</p> <p>\\(Revenue(Second-Price \\; Auction \\; with \\; n+1 \\; buyers) \\geq Revenue(Second-Price \\; Auction \\; with \\; Reserve \\; Price \\; and \\; n \\; buyers)\\) </p>"},{"location":"lecture-9.5/","title":"Mid-Quarter Review Party","text":"<p>April 30, 2025</p> <p>An important meta question in CS269I is: \"How do we allocate stuff?\" One answer is to let the market figure it out.</p>"},{"location":"lecture-9.5/#letting-the-market-allocate-goods","title":"Letting The Market Allocate Goods","text":"<p>Definition: Competitive Equilibrium</p> <p>A competitive equilibrium is the combination of a price vector \\(\\overrightarrow{p} = (p_1, ..., p_n)\\) and a matching \\(M\\) of buyers to goods, such that:</p> <ul> <li>Each buyer is matched to their favorite good (given prices).</li> <li>If no buyer is matched to good \\(j\\), then \\(p_j = 0\\).</li> </ul> <p>A competitive equilibrium:</p> <ul> <li>Maximizes social welfare \\(\\underset{i}{\\sum} v_{i,M(i)}\\).</li> <li>Always exists, and can be found efficiently with DA-with-prices.</li> </ul> <p>Deferred Acceptance With Prices Recap</p> <ul> <li>Assumption: Prices are always in some finite range (e.g., \\(\\$0-\\$1,000\\)), with finite increments (e.g., \\(\\$1\\)).</li> <li>Ordinal Preferences: For each buyer, we construct a list of all the \\((good,price)\\) options, ordered by utility. We can truncate the list a the \\((receive \\;nothing, pay \\; nothing)\\) option.</li> <li> <p>At each iteration of the algorithm:</p> <ul> <li>The unmatched buyer whose next-favorite option is \\((j,p)\\) proposes price \\(p\\) to good \\(j\\).</li> <li>Good \\(j\\) tentatively accepts if price \\(p\\) is higher than the prices it was offered so far.</li> </ul> </li> <li> <p>The running time of DA with prices is in the order of \\(O(n \\cdot m \\cdot i)\\), where \\(i\\) is the number of increments.</p> </li> <li> <p>DA-with-prices always terminates, and finds a competitive equilibrium, because every buyer is either matched to a good or they reach a point where they prefer to pay nothing and buy nothing.</p> </li> </ul> <p>Sometimes, letting the market allocate goods is a great answers. Somestimes, not so much. For instance:</p> <ul> <li>When we don't want to use money, such as in the case of a kidney exchange program or college dorm assignment. There is indeed a fundamental caveat when measuring welfare with money: one's value for something (i.e. how much they are willing to pay for it) also depends on how much money they have.</li> <li>When the market fails to converge to an optimal equilibrium.</li> </ul>"},{"location":"lecture-9.5/#market-failures","title":"Market Failures","text":"<p>Market Failures Recap</p> <p>A market failure occurs when a market fails to converge to an optimal outcome.</p> <p>We have seen five types of market failures:</p> <ol> <li> <p>Externalities and public goods</p> <ul> <li>An externality is a side-effect on someone other than the seller/buyer.</li> <li>A public good is something that belongs to everybody but owned by nobody.</li> <li>Examples: ecological damage, traffic congestion, underinvestment in public goods.</li> <li>What can we do about it? Pigouvian Tax (tax proportionally to the externality) and Coasian bargaining (auction off public goods).</li> </ul> </li> <li> <p>Transaction costs</p> <ul> <li>Transaction costs are market frictions.</li> <li>Examples: taxes, search costs.</li> <li>What can we do about it? Use algorithms for matching buyers and sellers.</li> </ul> </li> <li> <p>Market thinness</p> <ul> <li>The market is thin when the flow of buyers and sellers is too small.</li> <li>Examples: \"failed\" social networks, P2P platforms, marketsplaces.</li> <li>Monopolies create incentive issues.</li> <li>What can we do about it? Encourage more participants to join, merge markets, and batch transactions.</li> </ul> </li> <li> <p>Timing issues</p> <ul> <li>Examples: committing too early, exploding offers.</li> <li>What can we do about it? Use a centralized matching mechanism, allow to accept and then renege exploding offers.</li> </ul> </li> <li> <p>Information asymmetry</p> <ul> <li>Example: adverse selection for lemons.</li> <li>What can we do about it? Provide more information to both sides of the market or disallow the use of asymmetric information.</li> </ul> </li> </ol> <p>Given that the market may fail to converge to an optimal equilibrium, we want to know how to design mechanisms to allocate goods.</p>"},{"location":"lecture-9.5/#designing-mechanisms-to-allocate-goods","title":"Designing Mechanisms To Allocate Goods","text":"<p>Definition: Mechanism</p> <p>A mechanism consists of three things:</p> <ol> <li>A method of collecting inputs from agents,</li> <li>An algorithm that acts on the inputs,</li> <li>An action taken based on the output of the algorithm.</li> </ol> <p>Definition: Strategyproofness/Truthfulness</p> <p>A mechanism is strategyproof if it's in every agent's best interest to act truthfully, i.e., to report true preferences. In other words, an agent can never be worse off reporting their true preferences, regardless of what every other agent is doing.</p> <p>Mechanism: Serial Dictatorship</p> <ol> <li>Sort students in some fixed order (random, seniority, alphabetically, etc.).</li> <li>Go through the list in that order and allow each student (the \"dictator\") to select their most preferred available dorm.</li> </ol> <p>Mechanism: Deferred Acceptance</p> <p>While there's an unmatched doctor \\(i\\):</p> <ul> <li>Doctor \\(i\\) proposes to their next-favorite hospital \\(j\\).</li> <li>If hospital \\(j\\) has no match, they accept doctor \\(i\\).</li> <li>Else, if hospital \\(j\\) prefers their current match over doctor \\(i\\), doctor \\(i\\) remains unmatched.</li> <li>Else, hospital \\(j\\) matches with doctor \\(i\\), releasing their previous match.</li> </ul> <p>The algorithm stops when everyone is matched: it is strategyproof for buyers, but not for hospitals.</p> <p>Some mechanisms are strategyproof, but some aren't (e.g., first-price auctions, prediction markets). What should we expect strategic agents to do in non-strategyproof mechanisms?</p> <p>So, what should agents do when the mechanism isn't strategyproof, and possibly not even fully specified? From a single agent perspective, agents can learn good strategies by trial and error. Regret minimization is a good framework for doing that.</p> <p>Key Ideas</p> <ul> <li>Key Idea #1: Use historical performance to forecast the future.</li> <li>Key Idea #2: Use regret to measure the success of algorithms.</li> </ul> <p>Regret Recap</p> <ul> <li> <p>External Regret is the difference between the algorithm's total reward and the reward from the single best-in-hindsight action, i.e. Regret = \\( \\underset{i}{\\max} \\underset{t}{\\sum} r_{i,t} - \\underset{t}{\\sum} r_{ALG(t),t} \\).</p> </li> <li> <p>Swap-Regret is the difference between the algorithm's total reward and the reward and the best-in-hindsight \"swap function \\Phi of the algorithm's choice, i.e. Regret = \\( \\underset{\\Phi : [n] \\to [n]}{\\max} \\underset{t}{\\sum} (r_{\\Phi(ALG(t)),t} - r_{ALG(t),t}) \\).</p> </li> </ul> <p>How can we minimize regret?</p> <p>Key Ideas</p> <ul> <li>Key Idea #3: Use randomness as safety against adversarial inputs.</li> <li>Key Idea #4: Balance randomness and optimization.</li> </ul> <p>Algorithm: Follow The Regularized Leader (FTRL)</p> <p>On day \\(t\\), choose distribution \\(x\\) maximizing \\(\\underset{t}{\\sum} (r_{x,t} - \\frac{1}{\\eta}\\varphi(x))\\) where:</p> <ul> <li>\\(r_{x,t}\\) provides the historical performance,</li> <li>\\(\\eta\\) balances randomness and history,</li> <li>\\(\\varphi\\) is the regularizer, which penalizes unbalanced distributions.</li> </ul> <p>Algorithm: Multiplicative Weight Update (MWU)</p> <ul> <li>Initialize weights \\(z_{i,0} = 1\\) for all \\(i\\).</li> <li>At day \\(t\\):<ul> <li>Choose action \\(i\\) with probability \\(\\frac{z_{i,t}}{\\underset{j}{\\sum} z_{j,t}}\\).</li> <li>Update weights: \\(z_{i,t+1} \\leftarrow z_{i,t} e^{\\eta r_{i,t}}\\).</li> </ul> </li> </ul> <p>Claim: MWU = FTRL with entropy regularizer \\(\\varphi(x) = - \\sum x_i \\log(x_i) \\).</p> <p>How can we minimize regret when we can't see the rewards?</p> <p>Key Ideas</p> <ul> <li>Key Idea #5: Balance exploration and exploitation.</li> <li>Key Idea #6: Use unbiased estimates of rewards.</li> </ul> <p>Mechanism: Exp3 Algorithm</p> <p>On day \\(t\\):</p> <ul> <li>MWU selects an arm.</li> <li>Define pseudo-rewards:<ul> <li>If arm \\(i\\) is not selected: \\(\\hat{r}_{i,t} = 0\\).</li> <li>If arm \\(i\\) is selected: \\(\\hat{r}_{i,t} = \\frac{r_{i,t}}{Pr[\\text{selecting } i]}\\).</li> </ul> </li> </ul> <p>What should agents do when the mechanism isn't strategyproof, and possibly not even fully specified? From a game theoretic perspective, agents can leverage equilibrium solution concepts, such as Nash and Stackelberg equilibria to reason about other players, and (coarse) correlated equilibria to minimize regret.</p> <p>Nash Equilibrium Recap</p> <p>Each player samples independently from distribution, and nobody has an incentive to deviate from the distribution.</p> <p>The Good:</p> <ul> <li>Theorem: A Nash equilibrium exists in every finite game.</li> <li>If the players play the Nash equilibrium, none of them want to deviate.</li> </ul> <p>The Bad:</p> <p>The Nash equilibrium...</p> <ul> <li>... is not unique.</li> <li>... is not equal to the max-min and the min-max.</li> <li>... is not approached by Regret Minimization.</li> <li>... is intractable to compute, even approximately.</li> <li>... sometimes does not make sense (see below).</li> </ul> <p>The Ugly:</p> <p>Aviad wrote a 300+ page thesis on this topic, so we may get some long answers if we ask questions ;)</p> <p>Correlated Equilibrium Recap</p> <p>A correlated equilibrium is a distributuon of actions that every player would rather follow.</p> <p>Good News:</p> <ul> <li>A correlated equilibrium can be computed efficiently (via both Linear Programming and Regret Minimization*).</li> <li>If every player runs a Regret Minimization aglorithm*, the play converges to the set of correlated equilibria. Note: The game cannot converge to a correlated equilibrium without a correlating device.</li> </ul> <p>*Depdending on the details of the Regret Minimization algorithm, agents may only converge to a Coarse correlated equilibrium.\" We will ignore this distinction in lecture.</p> <p>Bad News:</p> <ul> <li>Just like a Nash equilibrium, a correlated equilibrium is not unique, and sometimes does not make sense (e.g., the CS269I grade game played in class).</li> <li>Without a correlating device, this is not an equilibrium, and players have an incentive to deviate.</li> </ul> <p>Observation: Every Nash equilibrium is an (un)correlated equilibrium.</p> <p>Stackelberg Equilibrium Recap</p> <p>In a Stackelberg equilibrium, the Follower's strategy is optimal given the Leader's strategy, and the Leader's commitment is optimal.</p> <p>Good:</p> <ul> <li>The Leader's utility is no worse than in any correlated equilibrium.</li> <li>Without loss of generality, the Follower's strategy is deterministic. This implies an efficient algorithm using Linear Programming.</li> </ul> <p>Catch:</p> <p>The Leader's strategy may be a sub-optimal response to the Follower's strategy. The Leader needs to be able to commit to this sub-optimal stragegy.</p> <p>Pessimistic Rock\u2013Paper\u2013Scissors (PRPS)</p> R P S R (0,0) (-2,1) (1,-2) P (1,-2) (0,0) (-2,1) S (-2,1) (1,-2) (0,0) <p>Coarse Correlated Equilibrium (CCE)</p> <p>The signal is approximately uniform on the diagonal.</p> <p>The expected utility is:</p> <ul> <li>\\(0\\) for following the signal.</li> <li>\\(-\\frac{1}{3}\\) for ignoring the signal.</li> <li>\\(1\\) for playing \\(P\\) whenever the signal is \\(R\\).</li> </ul> <p>Following the signal is better than following it: this is why this is a CCE. However, this is not a correlated equilibrium (due to the last bullet point).</p> <p>What Is The Stackelberg Equilibrium In The Pessimistic Rock-Paper-Scissors (PRPS) Game?</p> <p>We want to compute the Stackelberg equilibrium in the Pessimistic Rock-Paper-Scissors (PRPS) game, which means that the Leader is going to commit to some strategy (some probability of Rock, Paper, Scissors, respectively), and then the Follower will have some best response.</p> <p>Without loss of generality, we can think of the Follower\u2019s best response as deterministic, i.e. one action. Since the game is symmetric, without loss of generality, we want the Follower to play Rock (\\(R\\)).</p> <p>So, if we want the follower to play \\(R\\), we need to make sure that they don\u2019t want to play Paper (\\(P\\)) instead, for example. Let \\(L_R\\), \\(L_P\\), \\(L_S\\) be the probabilities that the leader plays \\(R\\), \\(P\\), \\(S\\). With these probabilities, what is the Follower\u2019s utility from playing \\(R\\)?</p> <ul> <li>If the follower plays \\(R\\), they get: \\(0 \\cdot L_R - 2 \\cdot L_P + 1 \\cdot L_S\\) \\((1)\\)</li> <li>If the follower plays \\(P\\), they get: \\(1 \\cdot L_R + 0 \\cdot L_P - 2 \\cdot L_S\\) \\((2)\\)</li> <li>If the follower plays \\(S\\), they get: \\(-2 \\cdot L_R + 1 \\cdot L_P + 0 \\cdot L_S\\) \\((3)\\)</li> </ul> <p>Since we want to force the follower to play \\(R\\), we want:</p> <ul> <li>\\((1) \\geq (2)\\), i.e. \\(- 2 \\cdot L_P + 1 \\cdot L_S \\geq 1 \\cdot L_R - 2 \\cdot L_S\\) (this is the first constraint that we have).</li> <li>\\((3) \\leq (1)\\), i.e. \\(-2 \\cdot L_R + 1 \\cdot L_P  \\leq - 2 \\cdot L_P + 1 \\cdot L_S\\) (this is the second constraint that we have).</li> </ul> <p>With these constraints together, we want to make sure that the follower is going to play \\(R\\), i.e. that the probability that the follower is going to play \\(R\\) is higher than anything else.</p> <p>Now, given that the Follower is going to play \\(R\\), what is the optimal/best strategy for the Leader (to get the Follower to play \\(R\\))?</p> <p>Well, given that the follower is playing \\(R\\), the Leader wants to maximize their utility. So, what is the Leader\u2019s utility if the Follower is playing \\(R\\)?</p> <p>The leader is getting \\(0 \\cdot L_R + 1 \\cdot L_P - 2 \\cdot LS\\) , which we want to maximize, i.e. \\(max (0 \\cdot L_R + 1 \\cdot L_P - 2 \\cdot LS)\\).</p> <p>In other words, we want to find \\(max (0 \\cdto L_R + 1 \\cdot L_P - 2 \\cdot LS)\\) given:</p> <ul> <li>\\((1) \\geq (2)\\), i.e. \\(- 2 \\cdot L_P + 1 \\cdot L_S \\geq 1 \\cdot L_R - 2 \\cdot L_S\\) (first constraint),</li> <li>\\((3) \\leq (1)\\), i.e. \\(-2 \\cdot L_R + 1 \\cdot L_P  \\leq - 2 \\cdot L_P + 1 \\cdot L_S\\) (second constraint), and</li> <li>\\(L_R + L_P + L_S = 1\\) (by definition).</li> </ul> <p>Given \\(L_R + L_P + L_S = 1\\), let\u2019s say that \\(L_S = 1 - L_P - L_R\\). So:</p> <ul> <li>\\(max (0 \\cdot L_R + 1 \\cdot L_P - 2 \\cdot LS) = max(L_P - 2 \\cdot (1 - L_P - L_R))\\) </li> <li>\\(-2 \\cdot L_P + 1 \\cdot (1 - L_P - L_R) \\geq 1 \\cdot L_R - 2 \\cdot (1 - L_P - L_R)\\)</li> <li>\\(-2 \\cdot L_R + 1 \\cdot L_P \\leq -2 \\cdot L_P + (1 - L_P - L_R)\\)</li> </ul> <p>Let\u2019s simplify things to solve this, and we get:</p> <ul> <li>\\(max(L_P - 2(1 - L_P - L_R) = max 3 \\cdot L_P + 2 \\cdot L_R - 2\\) (this is going to be the Leader\u2019s utility, and since we want to find the best one, we can ignore the \u201c\\(-2\\)\u201d, because there is nothing the Leader can do about this \u201c\\(-2\\)\u201d).</li> <li>\\(-2 \\cdot L_P + 1 \\cdot (1 - L_P - L_R) \\geq 1 \\cdot L_R - 2 \\cdot (1 - L_P - L_R) \\Leftrightarrow -5 \\cdot L_P -4 \\cdot L_R \\geq -3\\)</li> <li>\\(-2 \\cdot L_R + 1 \\cdot L_P \\leq -2 \\cdot L_P + (1 - L_P - L_R) \\Leftrightarrow L_R - 4 \\cdot L_P \\geq -1\\).</li> </ul> <p>Assuming we have simplified everything correctly, we want to maximize the \\(3 \\cdot L_P + 2 \\cdot L_R - 2\\) subject to the \\(-5 \\cdot L_P -4 \\cdot L_R \\geq -3\\) and \\(L_R - 4 \\cdot L_P \\geq -1\\) constraints.</p> <p>Since this is a problem with two variables, we can plot it:</p> <p></p> <p>We also have the constraint that all the probabilities are non-negative, so the feasible region is the green-shaded quadrilateral. And in this feasible region we want the feasible point that maximizes the objective function (Leader's utility), i.e. as far as possible in the direction of the purple arrow. </p> <p>This optimum for any linear program is always at some vertex of the polygon, and in this case in 2D we can eyeball and see that it's the \\((\\frac{1}{3},\\frac{1}{3})\\) vertex where the blue and brown constraints intersect.</p> <p>So the Stackelberg equilibrium is basically the same as the Nash equilibrium: the Leader plays uniformly over all Rock, Paper, and Scissors, and the Follower could play Rock. But more generally any strategy of the Follower is best response, and the Leader is indifferent between them.</p> <p>Crazy Rock\u2013Paper\u2013Scissors (CRPS)</p> R P S R (0,0) (2,1) (1,2) P (1,2) (0,0) (2,1) S (2,1) (1,2) (0,0) <p>Correlated Equilibrium (CE)</p> <p>The signal is approximately uniform off the diagonal.</p> <p>The expected utility is:</p> <ul> <li>\\(\\frac{3}{2}\\) for following the signal.</li> <li>\\(1\\) for playing \\(P\\) instead of \\(R\\).</li> <li>\\(\\frac{1}{2}\\) for playing \\(S\\) instead of \\(R\\).</li> </ul> <p>Solution concepts can be even more complicated, such as the Subgame Perfect Equilibrium (SPE) and the Bayesian Nash Equilibrium.</p> <p>Definition: Subgame Perfect Equilibrium (SPE)</p> <ul> <li>On day \\(n\\), the agents play a Nash equilibrium.</li> <li>On day \\(n-1\\), the agents evaluate their actions assuming that they will play a Nash equilibrium on day \\(n\\). Then, they play a Nash equilibrium for this particular game.</li> <li>On day \\(n-2\\), we assume the agents will play an SPE in the future.</li> <li>Etc.</li> </ul> <p>Definition: Bayesian Nash Equilibrium</p> <ul> <li>\\((Mixed = randomized)\\) is the strategy for each Alice.</li> <li>\\((Mixed = randomized)\\) is the strategy for each Bob.</li> </ul> <p>Alice and Bob are drawn at random from their distributions.</p> <p>For each Alice, her strategy is optimal in expectation over the Bobs and their strategies.</p> <p>We can use these solution concepts to analyze strategic agents.</p> <p>Theorem: Revenue Equivalence Theorem</p> <p>At equilibrium, expected payments are fully determined by the auction's allocation rule.</p> <p>In other words, in any auction that always gives the good to the max-value buyer, in expectation every buyer pays the same, and in particular, revenue is the same.</p> <p>However, beware: sometimes, equilibria are not great predictors.</p> <p>Example: The CS269I Grade Game</p> <p>You and your partner submitted a wonderful project, but the instructor is not sure how much each of you contributed to it. So, they will assign your grades using the following game:</p> <ol> <li>You send the instructor \\(x \\in \\{2, ..., 99\\}\\), and your partner sends the instructor \\(y \\in \\{2, ..., 99\\}\\).</li> <li> <p>Then the instructor assigns you grade as follows:</p> <ul> <li>If \\(x = y\\), then your grade is \\(x\\).</li> <li>If \\(x &lt; y\\), then your grade is \\(min\\{x,y\\}+2\\).</li> <li>If \\(x &gt; y\\), then your grade is \\(min\\{x,y\\}-2\\).</li> </ul> </li> </ol> <p>Which grade should you send to the instructor?</p> <p>The unique Nash equilibrium in this game is when both players play 2 (x = y = 2), which is not expected in practice with real players.</p> <p>Experiment: Axelrod Games</p> <p>Around 1980, Professor Robert Axelrod invited friends to write computer programs for a tournament of iterated file-sharing dilemma with a fixed number of round \\(n = 200\\).</p> <p>Out of 15 submissions, Tit-for-Tat won first place.</p> <p>Notes:</p> <ul> <li>Tit-for-Tat can never win a head-to-head match, but encouraging cooperating leads to a higher score on average.</li> <li>You can always do better than Tit-for-Tat: you can play Tit-for-Tat during \\(199\\) rounds, and not upload in the last round, but in practice, no participant tried this strategy.</li> </ul> <p>Later on, Professor Axelrod invited his friends to play again. Out of 62 submissions, Tit-for-Tat won first place again.</p> <p>Top Strategy in CS269I Iterated File-Sharing Tournament: The \"Forgive, Don't Forget\" Strategy</p> <p>Overview: We will normally respond Tit-for-Tat, but add in a \"forgiveness mechanism\" to make sure we aren't killing our scores if we are playing a Tit-for-Tat agent. On the first run, we cooperate, to prevent the Grim Trigger and \"defection detection\" strategies from lowering our average score. We take advantage of agents with \"always\" types of actions.</p> <pre><code>If this is the last turn: defect.\n\nIf the user NEVER cooperates: defect.\nIf the user ALWAYS cooperates: defect.\n\nIf the user cooperated with us last time:\n  If hasCooperated != TRUE, set hasCooperated = TRUE.\n  Reset forgiveness counter to 3.\n\nIf the user defected last time:\n  Is hasCooperated:\n    If forgiveness counter &gt; 0:\n      Subtract one from the forgiveness counter\n    If forgiveness counter == 0:\n      Forgive them, and cooperate this once. Reset counter to 3.\n\nOtherwise, play Tit-for-Tat!\n</code></pre>"},{"location":"lecture-9.5/#recap","title":"Recap","text":"<p>Mid-Quarter Review Recap</p> <p>How Do We Allocate Goods?</p> <ul> <li>Let the market figure it out.</li> <li>Design mechanisms.</li> </ul> <p>How do we evaluate these answers?</p> <ul> <li>Social welfare: The \"sum of happiness.\"</li> <li>Revenue: The \"mechanism designer's happiness.\"</li> <li>Pareto Optimality: \"No other allocation can make everyone happier.\"</li> <li>Individual Rationality: \"Nobody is sadder than they were before participating in the mechanism.\"</li> <li>Stability: \"No pair/subset could be Pareto-better by matching outside.\" Note: This generalizes individual rationality and Pareto optimality.</li> </ul>"},{"location":"lecture-9.5/#coming-up-next","title":"Coming Up Next","text":"<p>We will address questions such as \"Who knows what and when?\" and \"How does that impact incentives?\".</p> <p>We have already started talking about this in Lecture 9, when we discussed things that, empirically, \"the crowd\" seems to know well, including public opinion, progress on big projects, and the weight of an ox.</p> <p>We also saw that we needed to exercise caution and set incentives properly, via the No-trade theorems, and examples of information cascades, including the 2010 stock market flash crash.</p> <p>In upcoming lectures, we will welcome guest speakers, and discuss topics such as:</p> <ul> <li>Proper scoring rules.</li> <li>Blockchains: Proof-of-Work and Proof-of-Stake.</li> <li>Stock exchanges (centralized and decentralized).</li> <li>Fair allocation.</li> </ul>"},{"location":"lecture-9.5/#questions","title":"Questions","text":"<p>Below are questions asked by some of the students who attended the lecture in person.</p> <p>How does regret converge to coarse correlated equilibrium?</p> <p>Reminder: In a coarse correlated equilibrium, I would rather follow the signal than ignore the signal. The regret is the sum, over time, of the difference between the reward of the overall best action and the reward from my algorithm.</p> <p>If we use a regret-minimization algorithm, we expect this regret to approach or approximately approach zero. In a candidate coarse correlated equilibrium CCE:</p> <ul> <li>Draw \\(t\\) at random.</li> <li>The signal is \\(ALG(t)\\).</li> </ul> <p>So, each player is getting the action they took on day \\(t\\). Therefore, the expected utility from following the signal is the sum of the rewards the actual algorithm got. The utility from ignoring the signal is the sum of the rewards the best action got. The regret going to zero means that following the signal is almost as good as ignoring the signal and playing the overall best action.</p> <p>How does swap-regret converge to correlated equilibrium?</p> <p>Reminder: the definition of correlated equilibrium is that I am looking at the signal, and still after seeing the signal, I cannot do better than the signal. Swap-regret is harder to minimize, but if you get to minimize, it is better.</p> <p>Can we talk about Myerson\u2019s theorem?</p> <p>Here is what Myerson\u2019s theorem is saying:</p> <ul> <li>The first part is just that we have an auction selling one item to one buyer. In this case, how should we sell one item to one buyer? We can set the reserve price and ask the buyer if they want to buy the item for the price or not. The Myerson\u2019s theorem also gives us a formula to calculate the best price, by looking at the area under the curve.</li> <li>The second part says that this is the best way to sell one item. It is not trivial but it is true: you can do a lot of complicated things, but none of this helps, you might as well just set one price.</li> <li>The third part says that if we have some fixed number of buyers \\(n\\), and we don\u2019t know their values, but we assume that they are iid, they come from the same distribution \\(D\\). It turns out that the revenue-optimal auction to sell one item to these \\(n\\) buyers it to do a second-price with reserve price \\(p(D)\\) auction (where the reserve price is the same whether we have \\(1\\) buyer or \\(100\\) buyers).</li> </ul> <p>Reminder: in a second-price auction, everyone submits their sealed bid, but to win the auction, you don\u2019t need to only beat everyone else, but also the reserve price, and then you pay the maximum of everyone else and the reserve price.</p> <p>As the competition increases, it becomes more likely that we have two buyers whose value exceed the reserve price, and as soon as that happens, we don\u2019t need the reserve price anymore, because to win the auction, we don\u2019t need to beat the reserve price, we need to beat the second highest bid.</p> <p>If the reserve price becomes irrelevant as the number of bidders increases, and the revenue equivalence holds, why is the second price auction revenue optimal and not also the first price auction?</p> <p>As competition increases, it does not matter whether we are doing second-price or first-price, because we are reaching this point where the market is setting the price. The advantage of second-price auction is that it is strategyproof, so it is easier to analyze, while first price auction is not.</p> <p>The revenue equivalence theorem says that the different types of auctions are all going to get the same revenue, but it is assuming that we are always giving the good to the buyer with the maximum value and there are two complications here:</p> <ul> <li>When we have a reserve price, maybe we do not give the item at all.</li> <li>First-price auction is not truthful bidding, so buyers are going to shade their bid in a possibly-mixed strategy way, which makes it harder to keep track of who is going to get the item, but (Aviad thinks that) it is still true that if we run first-price with reserve price and the distribution of values is continuous, first-price and second-price are going to make the same allocation and they will get the same revenue.</li> </ul> <p>Why does the Bulow-Klemperer theorem hold?</p> <p>This theorem says that the revenue of second price with n+1 buyers is at least as great as the revenue of second price with reserve price with n buyers. We can imagine two cases:</p> <ul> <li>One case when it is n+1 real buyers: the seller always makes money.</li> <li>Another case when it is n buyers and the seller that set the reserve price: sometimes, the seller does not make money (because the reserve price is not beat, and the seller keeps the item).</li> </ul> <p>Why is it a dangerous practice to learn the reserve price from historical bids?</p> <p>Let\u2019s say that you are bidding to display ads on my website, and you know that next year I am going to set the reserve price based on what you do this year: what would you this year? You are going to start shading your bids this year, and I have no idea about what your real value is. Instead, what it better is to find another bidder with similar characteristics and try to use their bid to set the reserve price.</p>"},{"location":"lecture-9/","title":"Prediction Markets and Information Cascades","text":"<p>April 28, 2025</p> <p>In the past few lectures, we discussed markets and auctions. Now, we are going to start talking about learning from the wisdom of the crowd and decentralization systems, as well as how to reason about who knows what and when. Today, in particular, we are discussing prediction markets and wisdom of the crowd.</p>"},{"location":"lecture-9/#wisdom-of-the-crowd","title":"Wisdom of The Crowd","text":"<p>Example: Wisdom of The Crowd</p> <p>What is the weight of this ox?</p> <p> Note: This is not the original ox from the 1903 county fair. This is an Arctic Musk ox.</p> <p>Imagine that we are at a county fair, and everyone is taking a guess:</p> <ul> <li>1903: The average of the guesses by the crowd fair fell only 1 Lb away from the true weight.</li> <li>1907: Sir Galton publishes a paper about it. He actually advocated using the median instead of the mean to aggregate information from the crowd. The idea remains that crowds sometimes have better information that each individual person.</li> <li>2004: Surowiecki publishes popular The Wisdom of Crowds: Why the Many Are Smarter Than the Few and How Collective Wisdom Shapes Business, Economies, Societies and Nations book on this topic.</li> <li>2014: Galton's calculations are proven wrong: the average of the crowd was not 1 Lb away from the weight, but was in fact exactly right.</li> </ul> <p>If we have an ox, we can just ask the crowd what they think the weight of the ox is\u2014or we can just weigh the ox. However, often, we want to know what will happen in the future.</p> <p>There are a lot of questions we do not have an answer for. The nice advantage about using the wisdom of the crowd to predict the future is that, eventually, we will learn the answer.</p> <p>This helps us set incentives. If we ask people about specific things, but have no way of paying them, they can say whatever they want. However, if we want to know what will happen in the future, and we pay them about how good their guess was, they have an incentive to guess as right as possible.</p> <p>Examples: Applications of The Wisdom of The Crowd</p> <p>Which questions is the crowd likely to have good answers for?</p> <ul> <li>Elections: We are actually trying to predict the crowd's behavior.</li> <li>The state of a big corporate project: We are aggregating distributed information.</li> </ul> <p>Setting incentives matters</p> <p>In 1982, Tom Bradley was the Mayor of Los Angeles, and lost the race to become the California Governor even though he was ahead in the polls. The theory is that people said they would vote for him because they did not want to appear racist, but they actually voted for the other candidate.</p> <p>This is called the Bradley Effect, the Shy Torry Effect, or the Social Desirability Bias.</p>"},{"location":"lecture-9/#prediction-markets","title":"Prediction Markets","text":"<p>\"If only HP knew what HP knows, we would be three times more productive.\" \u2014 Lew Platt</p> <p>Prediction markets are a very popular way to aggregate information, for instance in big corporations such as HP, Ford, Firm X, Google, Ely Lily. Prediction markets help circumbent hierarchical organization structure, and it tends to outperform experts despite optimism bias (prediction markets tend to be overly optimistic).</p> <p>Example of Prediction Market: Iowa Electronic Market (IEM)</p> <p>IEM is an academic research prediction market hosted by the University of Iowa. It is sometimes used to predict things outside of academia, such as elections.</p> <p>Consider a winner-takes-all presidential elections market, with the following rules:</p> <ul> <li>D-contract pays \\(\\$1\\) if the Democratic candidate wins.</li> <li>R-contract pays \\(\\$1\\) if the Republican candidate wins.</li> <li>Zero-sum trade with the house: buy/sell a contract bundle (1 contract of each type) for \\(\\$1\\). You pay \\(\\$1\\) and you eventually get \\(\\$1\\). Since you have a D-contract and an R-contract, you can trade them with other predictors. So, if you think that the Democratic is more likely to win, you can buy more D-contract and sell some R-contract.</li> </ul> <p>Once a lot of predictors have contracts, they can buy and sell them, so we have a market, and we can see what happens to the prices. The way this is operated in the IEM is using something called the continuous limit order book.</p> <p>Definitions: Continuous Limit Order Book</p> <p>This is continuous because time is continuous and orders can arrive over time.</p> <p>For example: </p> <ul> <li>A buyer wants to buy a contract, and they are willing to pay \\(\\$1\\) for it. This is called the bid: the bid is the highest buy order currently on the book.</li> <li>A seller wants to sell that contract, and they are willing to sell it for \\(\\$3\\). This is called the ask: the ask is the lowest sell order currently on the book.</li> <li>Since the ask is higher than the bid, the buyer and the seller do not want to transact.</li> <li>However, buy and sell orders can arrive at any time: as soon as a trade is feasible, orders trade and they leave the book.</li> <li>A new buyer now wants to buy a contract for \\(\\$4\\). So, the bid goes up, to \\(\\$4\\), since the bid is the highest buy order.</li> <li>Given that the bid is higher than the ask, this new buyer and the seller are happy to trade with each other. We match them, they leave the marker, and the bid is again \\(\\$1\\).</li> <li>Perhaps, later, a new seller wants to sell the contract for \\(\\$2\\). So, the ask becomes \\(\\$2\\).</li> <li>The buyer who previous bought the contract for \\(\\$3\\) would have been better off trading with this new seller, but when they placed their buy order for \\(\\$4\\), the seller with the ask order of \\(\\$2\\) was not there, so they traded with the seller with the ask order of \\(\\$3\\).</li> <li>After we clear out all the orders that can trade with each other, the ask is higher than the bid, and the difference is called the spread.</li> </ul> <p>There are two kinds of orders in this book:</p> <ul> <li>Marketable orders: We already have a match in the book, so they can transact immediately, e.g., a buyer places a buy order for \\(\\$4\\) when there is already a sell order for \\(\\$3\\) on the market.</li> <li>Resting (non-marketable) orders: We are waiting to match , e.g., a buyer places a buy order for \\(\\$1\\), but there is no sell order at all in the book, so it is resting in the book.</li> </ul> <p>Note: This is not very complicated, but these are a very useful definitions (which will come back later in the quarter).</p> <p>Example of Prediction Market: Iowa Electronic Market (IEM) (Cont'd)</p> <p>All predictors have D-contracts and R-contracts. They look at the order book, and they see that R-contracts are trading for \\(\\$0.60\\). If they think that there is less than a 60% probability that the Republican candidate is going to win, they should sell their R-contracts or buy more D-contracts.</p> <p>There are some legal issues with prediction markets in the US (and beyond):</p> <ul> <li>Trading in prediction markets is essentially gambling, which is mostly illegal in the US, except in Las Vegas, Nevada.</li> <li>IEM was granted an exception (\"no action letter\") by the CFTC, but it is limited to 1,000 traders and \\(\\$500\\) per trader (i.e. no one person can dominate the market.)</li> <li>Intrade operated from Ireland for a while but was shut down in 2012 by the CFTC.</li> <li>PredictIt, which is also operated by a university since 2014, namely Victoria University of Wellington, and has a similar status to IEM. The CFTC ordered it to shut down in 2022/2023, but PredictIt sued and won, so it is still operating.</li> <li>Some decentralized prediction markets exist, such as AUgur and Polymarket, but they are probably illegal to use for US residents.</li> </ul> <p>Interestingly, meanwhile on Wall St, we can legally buy \"naked shorts\" for billions of dollars:</p> <ul> <li>A short is when someone believes a stock is going to go down, so they borrow some shares of it, and sell them immediately, with the intention of buying them back after the price has gone down.</li> <li>A \"naked short\" is when someone sells some shares of stock without first borrowing them (or without ensuring that they can be borrowed). This is done in the hopes that the price of the stock will fall, allowing the seller to buy back the shares at a lower price and profit from the difference. Important Note: It is a type of securities fraud.</li> </ul> <p>Example of Prediction Market: Iowa Electronic Market (IEM) (Cont'd)</p> <p>Given the rules of the market, we can interpret the price as a probability, aggregating the market's beliefs:</p> <p></p> <p>In theory (assuming the \\(\\$500\\)-per-trader restriction), at equilibrium prices \\((p_D, p_R)\\): \\(Number \\; of \\; D-contracts = Number \\; of \\; D-contracts\\).</p> <p>In other words: The total budget of the predictors who think that the Democratic candidate has a probability of winning greater than \\(p_D\\), divided by \\(p_D\\), is equal to the total budget of the predictors who think that the Republican candidate has a probability of winning greater than \\(p_R\\), divided by \\(p_R\\).</p> <p>Although this has a sense of aggregate probability, it tends to be skewed towards making the seem a little more balanced than it really is, because each time we are dividing the total budget by the price per contract. So, if the price per contract is small (e.g., this is the contract for the underdog, i.e. the candidate least likely to win), then we are dividing by a smaller number, so we are giving it a higher weight. </p>"},{"location":"lecture-9/#liquidity-and-no-trade-theorems","title":"Liquidity and No-Trade Theorems","text":"<p>In prediction markets, if there is a large spread, it reveals poor information aggregation.</p> <p>Example of Large Spread: Will Lebron James Sign With The Chicago Bulls?</p> <p>The bid for Yes was \\(5\\%\\), while the ask ask for No was almost \\(40\\%\\). This means that there was a large gap between the market aggregate belief in the Yes and the No. This was not very helpful to use this prediction market to predict what is going to happen, because it does not provide a very precise prediction.</p> <p>Why was the spread so large?</p> <p>Definition: Liquidity Providers</p> <p>Liquidity providers buy low now and sell high later (or sell high now and buy low later). In other words, liquidity providers leave bid/ask resting orders on the book.</p> <p>Other investors can buy/sell at any time by trading with liquity providers. When an investor buys from the resting order, and another investor sells from the resting order, the liquidity providers earn the spread, i.e. the gap between the sell order and the buyer order left in the book.</p> <p>As competition between liquidity providers increases, the spread decreases. As liquidity providers have a great business model (they buy low and sell high), they tend to outbid each other, so more competition brings the ask and the bid closer to each other.</p> <p>Providing liquidity sounds like a great deal, but what is the catch? Three is some risk involved.</p> <p>Theorem: Public Information No-Trade Theorem</p> <p>If the market already aggregated all available public information, there is no point in trading.</p> <p>If we already thought of all the possible information, and we all agree that the probability that the Republican candidate will win is 60%, which means that it is worth \\(\\$0.60\\) for everyone, why should anybody trade?</p> <p>How would this be different if some investors have some private information?</p> <p>Examples of Private Information </p> <ul> <li>Alice know that the delivery of component X for product Y is delayed.</li> <li>Alice used the largest, deepest neural network and discovered a trend towards candidate Z.</li> </ul> <p>If we believe that the Republican candidate has a very high probability of winning, we can buy a lot of R-contracts and make a lot of money. Is there any catch?</p> <ul> <li>Earning the profits actually depends on how reliable the private information is. If it is just a rumor, maybe there is some risk, but if it is correct most of the time, then on average, you will be better off.</li> <li>On a prediction market, even if a predictor has private information, they may only be able to sway the price by a \\(\\$500\\) difference. In contrast, on the stock market, traders have more leverage options to trade stocks with more capital than they really have.</li> <li>When someone reveals their intention to make a trade for a different price, they are revealing that they may have some information, and that may change the price. This is why, on the stock market, when large companies want to sell a big amount of stock, they have a lot of mechanisms to hide that fact to prevent the price from going down.</li> </ul> <p>Yet, there is something even more surprising.</p> <p>Theorem: Public and Private Information No-Trade Theorem</p> <p>Alice will only want to buy at the current price if she has private information.</p> <p>In other words, we know that if there is no private information, no one wants to buy or sale. So, if Alice wants to buy at the current price, she must know something that Bob doesn't know. However, if Alice wants to buy (from Bob), then Bob does not want to sell (to Alice). Thus, even if Alice has some private information, she cannot use it, because Bob (i.e. no one) wants to trade with he\u2014because she is too excited to trade based on the public information alone.</p> <p>In prediction markets, if everyone is rational:</p> <ul> <li>No one should be trading. If no one is trading, then we don't know what the aggregate belief is.</li> <li>If a liquidity provider has some resting order on the book, and Alice wants to buy from them, they should be wary. There is a big risk for liquidity providers.</li> <li>If there is more risk for liquidity providers, there are fewer liquidity providers on the market, so we expect the spread to be large, and we cannot make a good prediction based on the prediction market.</li> </ul> <p>Example of Large Spread: Will Lebron James Sign With The Chicago Bulls? (Cont'd)</p> <p>This is the reason why the spread was so large to predict whether Lebron James would sign with the Chicago Bulls.</p> <p>In practice, not everyone is rational, and not everyone thinks that everyone else is rational. This is why prediction markets work pretty well.</p> <p>Example of Prediction Market: Iowa Electronic Market (IEM) (Cont'd)</p> <p>Findings from research (2004) on IEM logs suggest that IEM predicted elections outcomes better than polls.</p> <p>\"Traders are not a representative sample of likely voters. They are overwhelmingly male, well-educated, high-income, and young.</p> <p>Women are also less likely to be liquidity providers.</p> <p>So, most traders are highly biased, and they make irrational choices, but prices are mostly set by a core group of sophisticated liquidity providers, who on average have more capital, are more active, and generate more profits.</p> <p>Prediction Markets Recap</p> <p>Prediction markets:</p> <ul> <li>Allow participants to trade contracts, where the payout is based on the outcome of a predicted event.</li> <li>Incentivize participants to aggregate information and tend to outperform experts (corporations) and polls (IEM).</li> </ul> <p>We can interpret the price as a (biased) proability, aggregating the market's beliefs.</p> <p>The No-trade theorems tell us that, even if some traders have more information, it is irrational to trade.</p>"},{"location":"lecture-9/#information-cascades","title":"Information Cascades","text":"<p>Information cascades are different from prediction markets, but they are also about aggregating information.</p> <p>Information Aggregation Dynamics</p> <p>Information aggregation is a dynamic process. Agents learn what other agents think, and that changes their own opinion.</p> <p>What information is revealed in the pricess is very important, e.g., IEM traders see the last prices traded, but not the quantities.</p> <p>In the rest of this lecture, we will focus on examples where information cascades go wrong.</p> <p>Wram-Up: The Sky Experiment</p> <p>You are walking down the street. Do you stop to look at the sky? Probably not.</p> <p>Now, you keep looking down the street, and you see other people looking at the sky: do you also look at the sky?</p> <p>Here is the model of this experiment:</p> <ul> <li>Each person makes a decision (to look up or not).</li> <li>Decisions are made sequentially: each person sees decisions made before them, and then they decide what to do.</li> <li>Before making a decision, each person leanrs:<ul> <li>A private signal (e.g., did something fall on their head or not?),</li> <li>Other people's actions so far (did they look up or not?),</li> </ul> </li> <li>However, before making a decision, no person learns other people's signals (e.g., did something fall on their head or not?).</li> </ul> <p>Example The Urn Experiment</p> <p>This is an in-class experiment, with the following rules:</p> <ul> <li>Aviad printed many \"balls\" in an urn, either with \\((66\\% \\; Blue, 33\\% \\; Red)\\) or \\((33\\% \\; Blue, 66\\% \\; Red)\\).</li> <li>The goal is to decide if there are more blue balls or red balls in the urn.</li> </ul> <p>The model of the experiment is similar to the Sky Experiment:</p> <ul> <li>Each person makes a guess (more blue or red balls).</li> <li>Decisions are made sequentially: each group sees guesses made before them, and then they make a guess themselves.</li> <li>Before making a guess, each group leanrs:<ul> <li>A private signal, i.e. they can see one random ball from the urn,</li> <li>Previous groups' guesses so far (more blue or red balls),</li> </ul> </li> <li>However, before making a guess, no group learns other groups' signals (e.g., the color of the random ball they saw).</li> </ul> <p>Here are the guesses of the groups in class: Red, Red, Red, Red, Red, Blue.</p> <p>How does this work?</p> <ul> <li>Player 1: Guesses the color of the ball they see. Let's assume it is Blue.</li> <li>Player 2: Guesses the color of the ball they see. Let's assume it is Blue again.</li> <li>Player 3: Guesses Blue\u2014even if they see Red.</li> <li>Player 4: Guesses Blue\u2014even if they see Red.</li> <li>Player 5: Guesses Blue\u2014even if they see Red.</li> </ul> <p>In other words, players 3, 4, 5, ... ignore the color of the ball they see and blindly trust players 1 and 2: this is an information cascade.</p> <p>Conclusion: Even when we have a lot of information, we may not be aggregating well.</p> <p>How to model this?</p> <p>Assume the urn contains two red balls:</p> <ul> <li>Correct cascade: \\(Pr[Players \\; 1+2 \\; both \\; see \\; Red] = \\frac{2}{3} \\cdot \\frac{2}{3} = \\frac{4}{9}\\).</li> <li>Wrong cascade: \\(Pr[Players \\; 1+2 \\; both \\; see \\; Blue] = \\frac{1}{3} \\cdot \\frac{1}{3} = \\frac{1}{9}\\).</li> <li>No cascade (yet): \\(Pr[Players \\; 1+2 \\; see \\; Blue \\; and \\; Red] = \\frac{1}{3} \\cdot \\frac{2}{3} + \\frac{2}{3} \\cdot \\frac{1}{3} = \\frac{4}{9}\\).</li> </ul> <p>This means that, after two players:</p> <ul> <li>Correct cascade with \\([Pr = \\frac{4}{9}]\\) \\(\\rightarrow\\) All remaining players guess correctly.</li> <li>Wrong cascade with \\([Pr = \\frac{1}{9}]\\) \\(\\rightarrow\\) All remaining players guess incorrectly.</li> <li>No cascade (yet) with \\([Pr = \\frac{4}{9}]\\) \\(\\rightarrow\\) So far, Red and Blue are even: this is equivalent to restarting the experiment over.</li> </ul> <p>More generally, after \\(2t\\) iterations:</p> <ul> <li>The correct cascade is four times more likely than the wrong cascade (\\([Pr = \\frac{4}{9}]\\) vs. \\([Pr = \\frac{1}{9}]\\)).</li> <li>A cascade will eventually happen, because the probability that a cascade has not happened yet decays exponentially: \\(Pr[No \\; cascade \\; yet] = (\\frac{4}{9})^t\\).</li> </ul> <p>This means that \\(Pr[Correct \\; cascade] = 80\\%\\) and \\(Pr[Wrong \\; cascade] = 20\\%\\). This is not terrible, but given that we have infinitely many samples, this is really bad information aggregation, because we still have a \\(20\\%\\) chance of being completely wrong even when we have infinitely many samples.</p>"},{"location":"lecture-9/#two-stories-of-information-cascades","title":"Two Stories of Information Cascades","text":"<p>Stroy #1 Price Bubble</p> <p>Why would a biology textbook be offered for over \\(\\$18M\\) on Amazon?</p> <p></p> <p>Two pricing algorithms were mis-informing each other and updating their respective price every day based on the competitor's price the previous day. One algorithm was trying to be slightly lower than the other (to be cheaper), but the other was trying to be slightly (to be more prestigious). This spiraled into very big prices.</p> <p>Moral: Information cascades are even worse with algorithms that don't perform Bayesian reasoning very well.</p> <p>Stroy #2 2010 Flash Crash</p> <p>On May 6, 2010, stock values lost \\(\\approx \\$1T\\)... and then recovered within 36 minutes.</p> <p>Why? There were several competing theories. In April 2015? a trader in London was arrested for allegedly causing a crash via a cascade of trading algorithms panic.</p> <p>Information Cascades Recap</p> <ul> <li>Information cascades happen when agents are acting solely based on public information and are not contributing their private information, which causes information aggregation to fail.</li> <li>The agent action space (e.g., \"red/blue\" vs. \\(Pr[Blue]\\)) is important.</li> <li>Examples of information cascades: the Sky Experiment, the Urn experiment, the Price Bubble, and the Flash Crash.</li> </ul>"},{"location":"lecture-9/#recap","title":"Recap","text":"<p>Wisdom of The Crowd Recap</p> <p>Crowd often possess collective wisdwoms about:</p> <ul> <li>The crowd's opinions (election predictions).</li> <li>Large complicated processes (corporate projects).</li> </ul> <p>We need to think carefully about incentives to extract these wisdoms, in particular to avoid:</p> <ul> <li>Information cascades.</li> <li>Biased polls.</li> </ul> <p>Prediction markets are one popular option for learning from the crowds:</p> <ul> <li>Equilibrium price as a (biased) predicted probability.</li> <li>No-trade theorems: if everyone is rational, no one wants to trade.</li> </ul>"}]}