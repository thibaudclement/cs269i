{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Computers/algorithms control a lot of economic processes, including online retail, online advertising, algo-trading, cryptocurrencies, etc. There is a need to jointly understand economics and computation to analyze these applications. While relying on an increase of computing power (more GPUs) may be an option to compensate for inefficiencies, some specific cases may actually justify the need for efficient algorithms.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>When it comes to incentives, leveraging more GPUs is not the solution. Computer scientists often need to build systems that interact with other agents, such as users, algorithms, Large Language Models, and even other systems. It is crucial to consider that these other agents are going to behave selfishly to predict how they are going to interact with the system, and design an implement said accordingly.</p>"},{"location":"#goals","title":"Goals","text":"<p>CS269I aims to teach students how to think about incentives (through lagnauge, frameworks, and practice), to get familiar with economic theory concepts (i.e. stable matching, proof-of-work, etc.), and analyze case studies to understand potential gaps between theory and practice.</p>"},{"location":"#credit","title":"Credit","text":"<p>These course notes for Stanford CS269I are based on the content of Aviad Rubinstein's lectures in the Spring 2025 quarter. This site is developed by Thibaud Clement, based on Eric Gao's original course notes from the Winter 2023 quarter.</p>"},{"location":"lecture1/","title":"One-Sided Matching and Serial Dictatorship","text":"<p>(3/31/2025)</p> <p>The Stanford Undergrad Housing Problem:</p> <ul> <li>There are \\( n \\) students, each with some preference over dorms.</li> <li>There are \\( m \\) dorms, each with some capacity (for simplicity, assume capacity is 1 for every dorm).</li> </ul> <p>How can we assign students to dorms?</p>"},{"location":"lecture1/#max-weight-matching","title":"Max Weight Matching","text":"<p>One possible solution is to maximize total happiness:</p> <p>Mechanism: Happiness Maximization</p> <ol> <li>Create a bipartite graph where:<ul> <li>One side represents students.</li> <li>The other side represents dorms.</li> <li>Each edge from a student to a dorm represents how much that student likes that dorm.</li> </ul> </li> <li>Determine the max-weight matching (i.e., Hungarian Algorithm).</li> </ol> <p>Key question: How does the algorithm determine how much each student likes each residency? Only the students themselves know how much they like each dorm, so we need to rely on students to tell the algorithm.</p> <p>Challenge: Each student has an incentive to exaggerate how much they like their favorite dorm and undercut how much they like other dorms (using a \"0\" or \"negative infinity\" weight for those dorms). In other words: max-weight bipartite matching allows students to game the system.</p> <p>This occurs because finding the matching that maximizes total happiness is conceptually right, but this naive max-weight matching fails to take incentives into account. Another possible algorithm is Serial Dictatorship.</p>"},{"location":"lecture1/#serial-dictatorship","title":"Serial Dictatorship","text":"<p>This is how Stanford actually assigns dorms to students.</p> <p>Mechanism: Serial Dictatorship</p> <ol> <li>Sort students in some fixed order (random, seniority, alphabetically, etc.).</li> <li>Go through the list in that order and allow each student (the \"dictator\") to select their most preferred available dorm.</li> </ol> <p>How is this better? There can still be students unhappy with their result. Indeed, a common complaint about Serial Dictatorship is unfairness: the first student chooses whichever dorm they want, while the last student gets only the last pick.</p> <p>When sorting is random, Serial Dictatorship guarantees ex-ante fairness: before the random sorting, all students have equal chances. However, after the sorting happens (even randomly), Serial Dictatorship loses fairness: there's no guarantee of ex-post fairness.</p> <p>Definition: Mechanism</p> <p>A mechanism consists of three things:</p> <ol> <li>A method of collecting inputs from agents,</li> <li>An algorithm that acts on the inputs,</li> <li>An action taken based on the output of the algorithm.</li> </ol> <p>In our context (Stanford Undergrad Housing Problem), based on inputs (dorm preferences) and the algorithm (available dorm after each student's pick), the mechanism takes actions (it assigns dorms).</p> <p>Note: All three components matter because there's a feedback loop: how we use inputs impacts what students report as preferences. The naive bipartite matching approach encourages students to game the system by misreporting their preferences.</p> <p>Thus, when designing a mechanism, we must consider:</p> <ul> <li>The algorithm itself,</li> <li>How/where inputs come from,</li> <li>Actions taken based on inputs,</li> <li>How promised actions affect reported inputs.</li> </ul> <p>Definition: Strategyproofness/Truthfulness</p> <p>A mechanism is strategyproof if it's in every agent's best interest to act truthfully, i.e., to report true preferences.</p> <p>Even more formally, game theorists somtimes use the term Dominant Strategy Incentive Compatible:</p> <p>Definition: Dominant Strategy Incentive Compatible - DSIC</p> <p>A mechanism is dominant strategy incentive compatible if truthfulness is a dominant strategy for each participant. That is, being truthful is a best response regardless of other players' actions.</p> <p>Theorem: You Cannot Game Serial Dictatorship</p> <p>It is in every student's best interest to choose their favorite available dorm in their turn. Formally, we say that Serial Dictatorship is strategyproof or truthful.</p> <p>Proof:</p> <ul> <li>Your room choice doesn't affect room availability before your turn.</li> <li>When your turn arrives, your best action is choosing your favorite available room.</li> </ul> <p>Why does this prove that Serial Dictatorship is strategyproof? Until your turn comes, it is other students who are bidding, and there is nothing you can do to affect it. However, when comes your turn, you choose what you get, so you should choose the best thing for you.</p> <p>Definition: Pareto Optimality</p> <p>An assignment \\( A \\) is Pareto optimal if, for any other assignment \\( B \\), there's at least one participant strictly preferring \\( A \\) over \\( B \\).</p> <p>Theorem: You Cannot Make Everyone Happier Without Making Someone Sadder</p> <p>Serial Dictatorship assignments are Pareto Optimal.</p> <p>Proof:</p> <p>Assume, for the sake of contradiction, that a different assignment exists making everyone as happy or happier.</p> <p>Consider the first student assigned differently: all better dorms are already assigned identically to students before them.</p> <p>Thus, the first differing student gets a worse dorm and becomes less happy\u2014we have reached a contradiction.</p> <p>Therefore, Serial Dictatorship is Pareto Optimal.</p>"},{"location":"lecture1/#additional-discussion","title":"Additional Discussion","text":"<p>Why is truthfulness good? It prevents corruption and ensures fairness by removing any insider advantage.</p> <p>When is truthfulness important? When optimizing happiness, truthful inputs ensure correct objectives.</p> <p>When is relaxing truthfulness acceptable?  In contexts with social norms where truthfulness isn't expected (e.g., poker games), strategyproofness isn't crucial.</p> <p>Fairness/equity issues with Serial Dictatorship: Fairness depends on sorting order. Seniority or randomness provide fairness ex-ante but not necessarily ex-post.</p> <p>Is there any other flaws in Serial Dictatorship? A dictator's seemingly insignificant decision may have huge impact on other agents. For instance, by the time my turn arrive, only my 9th and 10th choices are available. Since I am almost indifferent between them, I break ties and take my 9th choice. However, it is possible that my 9th choice was in fact someone else's top choice and they really wanted it. This may be even worse if their 2nd-10th choices happen to be already taken.</p>"},{"location":"lecture1/#recap","title":"Recap","text":"<ul> <li>Definition: Mechanism. A mechanism meaning soliciting inputs, running an algorithm, and taking actions.</li> <li>Definition: Strategyproof/truthful. A mechanism is strategyproof/truthful is misreporting preferences can never make a participant better off.</li> <li>Defintion: Pareto-optimal. An assignment \\( A \\) is Pareto-optimal if for any other assignment \\( B \\), there is a participan that (strictly) prefers \\( A \\) over \\( B \\).</li> </ul>"},{"location":"lecture2/","title":"Stable Two-Sided Matchings","text":"<p>(4/2/2025)</p> <p>After medical school, med students start their internship called a \"residency.\" Each (prospective) doctor has preferences over hospitals, and each hospital has preferences over doctors. How should doctors and hospitals be matched?</p> <p>Key Nuances:</p> <ul> <li>The biggest difference between doctor-residency matching and student-dorm matching is that we now have two-sided preferences (each doctor and hospital have preferences over each other).</li> <li>Matching students to dorms is a centralized process, while matching doctors to hospitals requires incentivizing participants to use a centralized process to prevent side deals.</li> </ul>"},{"location":"lecture2/#stable-matching","title":"Stable Matching","text":"<p>Definition: Blocking Pair</p> <p>Given a match \\(M\\), the pair (doctor \\(i\\), hospital \\(j\\)) forms a blocking pair if they prefer each other to their current assignments in \\(M\\).</p> <p>For example, if Doctor \\(n\\) prefers Stanford over UCSF and Stanford prefers Doctor \\(n\\) over Doctor 1, who is currently matched, then (Doctor \\(n\\), Stanford) form a blocking pair. This results in an Unstable Matching.</p> <p>Note: Blocking pairs exist only in two-sided matching markets. In a 1-sided matching, we cannot have blocking pairs, because in a blocking pair, both participants need to prefer each other. In the Stanford Undergraduate Housing Problem, even though students have preferences over dorms, dorms do not have preferences over students.</p> <p>Definition: Stable Matching</p> <p>A matching \\(M\\) is stable if there are no blocking pairs. Equivalently, for every unmatched pair \\((i,j)\\), either:</p> <ul> <li>Doctor \\(i\\) prefers Hospital \\(M(i)\\) over Hospital \\(j\\), or;</li> <li>Hospital \\(j\\) prefers Doctor \\(M(j)\\) over Doctor \\(i\\).</li> </ul> <p>Key Point: Stability removes incentives to deviate from the centralized matching. For instance, if you have a stable matching between doctors and hospitals, then neither doctors nor hospitals have any incentive to deviate from the matching and match outside of the process. They might as well stay in the centralized matching, because they cannot get anything better outside of the matching process.</p>"},{"location":"lecture2/#deferred-acceptance","title":"Deferred Acceptance","text":"<p>The Deferred Acceptance Algorithm is an algorithm that finds stable matchings.</p> <p>Main idea: Each doctor proposes to their favorite hospital that hasn't rejected them yet. Hospitals accept the best available candidate. If we discover a blocking pair, we switch the matching.</p> <p>Mechanism: Deferred Acceptance</p> <p>While there's an unmatched doctor \\(i\\):</p> <ul> <li>Doctor \\(i\\) proposes to their next-favorite hospital \\(j\\).</li> <li>If hospital \\(j\\) has no match, they accept doctor \\(i\\).</li> <li>Else, if hospital \\(j\\) prefers their current match over doctor \\(i\\), doctor \\(i\\) remains unmatched.</li> <li>Else, hospital \\(j\\) matches with doctor \\(i\\), releasing their previous match.</li> </ul> <p>The algorithm stops when everyone is matched.</p> <p>Example</p> <p>Consider the following agents and their respective preferences:</p> <ul> <li>Doctors: \\(Alice (X, Y, Z)\\), \\(Bob (Y, X, Z)\\), \\(Charlie (Y, Z, X)\\).</li> <li>Hospitals: \\(X (Bob, Alice, Charlie)\\), \\(Y (Alice, Bob, Charlie)\\), \\(Z (Bob, Charlie, Alice)\\).</li> </ul> <p>Stable matching result: \\((Alice, X), (Bob, Y), (Charlie, Z)\\).</p> <p>Note: No matter the order chosen to process the doctors, we always find the stable matching. </p> <p>Theorem: Runtime of Deferred Acceptance</p> <p>Deferred Acceptance runs in \\(O(n^2)\\) time.</p> <p>This is important, because when inputs are of non-trivial size, we want to make sure the algorithm is efficient.</p> <p>Proof:</p> <ul> <li>There are at most \\(n^2\\) proposals (each doctor proposes at most to \\(n\\) hospitals).</li> <li>Each proposal is \\(O(1)\\).</li> <li>So, the total runtime is \\(O(n^2)\\).</li> </ul> <p>Why do we have at most \\(n^2\\) iterations? A doctor never proposes to the same hospital twice (we never try to match the same pair of doctor-hospital twice), because we are going down the list of preferences. So, if there are \\(n\\) doctors and \\(n\\) hospitals, then there are \\(n^2\\) possible doctor-hospital pairs. This is why we have at most \\(n^2\\) iterations (there are instances where it really takes \\(n^2\\) time).</p> <p>Why is each proposal \\(O(1)\\)? We assume that doctor \\(i\\) already has a ranked list of preferences (how we get the preferences is non-trivial but outside of this algorithm). On every iteration, all we do is advance to the next hospital in the list of a doctor\u2019s preferences (for instance, in the case of \\(Bob\\) above, we moved from Hospital \\(Y\\) to Hospital \\(X\\)). When a hospital needs to determine whether it prefers its current match \\(i'\\) over \\(i\\) (or not), it can use an array with the preferences of doctors (ranked): this way, it can compare the ranks in \\(O(1)\\) time.</p> <p>Theorem: Deferred Acceptance is Stable</p> <p>Given \\(n\\) doctors and \\(n\\) hospitals, Deferred Acceptance outputs a complete stable matching.</p> <p>Corollary: A stable matching exists.</p> <p>(This is not obvious!)</p> <p>Proof (outline):</p> <p>We prove that Deferred Acceptance outputs a complete stable matching with three claims:</p> <ol> <li>The current match is stable at each iteration.</li> <li>Once matched, hospitals remain matched.</li> <li>At completion, everyone is matched.</li> </ol> <p>Therefore, no blocking pairs exist.</p> <p>How does this proof work? Claim 1 states that we have already matched some doctors and hospitals, and we assume that the matching so far is stable. Claim 2 states tgat once a hospital is matched, it may be matched to another doctor, but it never gets unmatched (however, doctors can get unmatched). Claim 3 states that no doctor, and no hospital, is unmatched at the end of the matching. Since we know that the matching is stable after each iteration, and there is no one left unmatched at the end, then the matching at the end is stable.</p> <p>Proof of Claims:</p> <ul> <li>Claim 1: We assume that doctor \\(d\\) and hospital \\(h\\) are currently matched to other matches and they are a blocking pair. This means that \\(d\\) is matched to a worse hospital, so we must have tried to match \\(d\\) to \\(h\\), and we must have gone down the preference list, either because \\(h\\) refused to match to \\(d\\), or because \\(h\\) preferred another doctor. This means that \\(h\\) must have already matched with someone better than \\(d\\). This is a contradiction with the fact that \\((d,h)\\) is a blocking pair.</li> <li>Claim 2: If you look at the pseudo code of the algorithm, there is simply no command to unmatch a hospital.</li> <li>Claim 3: If we have a free doctor, then we need to have a free hospital as well. However, if we reach the end of the algorithm, \\(d\\) already tried to propose to \\(h\\), but after that step, \\(h\\) is matched (either because \\(d\\) matches with \\(h\\), or because \\(h\\) is already matched to another doctor). By Claim 2, we know that a hospital stays matched until the end of the algorithm, so \\(h\\) cannot be free. This is a contradiction to \\(h\\) being free at the end of the algorithm (we cannot have a \\((d,h)\\) pair free).</li> </ul>"},{"location":"lecture2/#deferred-acceptance-as-a-mechanism","title":"Deferred Acceptance as a Mechanism","text":"<p>We know that Deferred Acceptance always finds a stable matching. Is it optimal? What does optimality mean?</p> <p>Theorem: Efficiency of Stable Matchings</p> <p>Every stable matching is Pareto-optimal.</p> <p>Reminder: An assignment \\( A \\) is Pareto-optimal if for any other assignment \\( B \\), there is a participan that (strictly) prefers \\( A \\) over \\( B \\).</p> <p>Proof: Any deviation from a stable matching worsens at least one participant\u2019s outcome.</p> <p>Comment: If we take the stable matching, and any other matching (whether it is stable or not), there are going to be some doctors and/or some hospitals that prefer the stable matching over that other matching.</p> <p>Theorem: The matching returned by the Deferred Acceptance mechanism is doctor-optimal.</p> <p>In other words, every doctor is matched to their favorite hospital possible in any stable matching.</p> <p>If we have multiple stable matchings, which one is the best one? Doctor-optimality means that every doctor gets the best hospital they can possibly get out of any stable matching. When we talked about the student-dorms 1-sided matching, it was possible that some students were happier, while others were sadder, depending on the matching (we had some trade-offs). However, here, among all stable matchings, the matching that DA outputs is the best for every single doctor simultaneously. An implication of this theorem is that the Deferred Acceptance mechanism is also doctor-strategyproof.</p> <p>Corollary: Deferred Acceptance is doctor-strategyproof.</p> <p>Doctors cannot gain from misreporting their preferences.</p> <p>Theorem: The matching returned by the Deferred Acceptance mechanism is hospital-worst.</p> <p>In other words, every hospital is matched to their least favorite doctor possible in any stable matching.</p> <p>Corollary: Deferred Acceptance is NOT hospital-strategyproof.</p> <p>Hospitals can benefit from misreporting their preferences.</p> <p>Example</p> <p>Consider the following agents and their respective preferences:</p> <ul> <li>Doctors: \\(Alice (X, Y, Z)\\), \\(Bob (Y, X, Z)\\), \\(Charlie (Y, Z, X)\\).</li> <li>Hospitals: \\(X (Bob, Alice, Charlie)\\), \\(Y (Alice, Charlie, Bob)\\), \\(Z (Bob, Charlie, Alice)\\).</li> </ul> <p>Stable matching result: \\((Alice, Y), (Bob, X), (Charlie, Z)\\).</p> <p>By changing their preferences over \\(Bob\\) and \\(Charlie\\) (compared to the previous example setup), Hospital \\(Y\\) gets matched to \\(Alice\\) (their top choice).</p> <p>Question 1: Does a hospital need to know the preferences of the other hospitals over the doctors to game the system?</p> <p>Yes, Hospital \\(Y\\) needs to know the preferences of all other hospitals over all other doctors to game the system, otherwise, it is pretty hard to figure out how to game the system on the hospital side. Aviad does not know of a good strategy for hospitals to game the system.</p> <p>Question 2: If a doctor ranks their safety choices higher, does it help them?</p> <p>No. For instance, if a doctor prefers Stanford, but understands that their second favorite hospital may be less competitive (i.e. rank them higher as a doctor), it is still not in their best interest to rank that second hospital higher than Stanford in their preference list, because the DA is completely doctor-strategyproof.</p> <p>Question 3: Why is the DA mechanism hospital-worst?</p> <p>Because we are processing things in order of doctor preferences, and we are only using hospital preferences for tie-breaking purposes.</p> <p>Question 4: Does Pareto optimality change if we allow participants to have non-strict preferences?</p> <p>In practice, doctors do not rank all possible hospitals, and hospitals do not rank all possible doctors. The theorem statements do not hold exactly in practice (in particular for Pareto optimality if we have non-strict preferences).</p> <p>Question 5: As a doctor, if you know that a hospital is misrepresenting their preferences, is it still doctor-stragegyproof?</p> <p>From the doctor\u2019s perspective, the hospitals submit some ranked lists of doctors, but they don\u2019t know whether it truthful or not, so this is still doctor-stragegyproof. However, if a doctor can influence how a hospital reports their preferences, then they can game the system.</p>"},{"location":"lecture2/#deferred-acceptance-in-practice","title":"Deferred Acceptance in Practice","text":"<p>Why don\u2019t we use a DA mechanism for undergrad admissions in the US? </p> <p>In the US, the system is very decentralized, and this is a challenge for schools, because they need to admit a number of students without knowing how many are actually going to enroll\u2014unlike hospitals, which generally have 1 or 2 residency positions available, so if none shows up, they don\u2019t have a doctor, and if 5 show up, they cannot pay everyone\u2014and these are generally large numbers.</p> <p>Also in the US, applications are holistic, which costs money, so application fees limit the number of universities students apply to.</p> <p>In contrast, in other countries where standardized tests scores are used instead of holistic reviews, DA mechanisms can be used (such as in Brazil, China, Hungary).</p> <p>How do doctors rank hospitals?</p> <ul> <li>1950s: DA-like algorithms initially used. At the time, there were very few female doctors (not to mention openly gay doctors).</li> <li>1960s: Couples complicate preferences. More couples want to be near each other, i.e. doctors no longer have a ranked preference over hospitals.</li> <li>1980s: Negative theory results on stable matching with couples. A stable matching may not exist: it is possible that there is no stable matching when you try to match couples to the same hospitals in the same cities. Deciding if a stable matching exists is an NP-complete computational problem (it is, in theory, intractable).</li> <li>1990s: Extended DA for couples adopted.</li> </ul> <p>How do hospitals rank doctors?</p> <ul> <li>Interviews (costly process): hospitals strategize over which doctors to interview (\"safety choices\").</li> <li>Standardized Tests (probably not any more: USMLE switched to pass/fail in January 2022).</li> <li>Letters of recommendation, medical school evaluation, personal statement, CV.</li> </ul> <p>Theorem: In DA, using safety choices is never safer for hospitals.</p> <p>Formally, manipulating true ranks to move \\(i-th\\) doctor (\"safety choice\") to a higher position cannot help a hospital match with a doctor of rank \\(i-or-better\\).</p> <p>Hospitals also use \"safety choices\" after interviews: they may not rank first the candidates that they evaluate as \u201ctoo good to come here.\u201d This is particularly interesting since we have seen in doctor-proposing DA that ranking safety choices higher is never safer for hospitals (this is provable).</p> <p>Proof:</p> <ul> <li>Until doctor \\(i\\) tries to match with a hospital, manipulation has no effect.</li> <li>After doctor \\(i\\) tries to match with a hospital, they can only be replaced by a better doctor.</li> </ul> <p>Yet, hospitals still strategically use \"safety choices\" when ranking doctors post interview. Why?</p> <ul> <li>One possible explanation is ignorance of matching mechanisms and their properties (they did not take CS269I).</li> <li>Another possible explanation is that they consider their reputation/ego. after the matching is complete, the organization that handles the matching publishes the \u201cnumber needed to fill\u201d metric, which is the lowest ranking a hospital had to go to fill their positions. This is helpful for doctors the following year to get a sense of how competitive a program is, which is therefore also a signal of how prestigious a program is. For instance, Stanford probably don\u2019t have to be turned down by many doctors, so their number is low, while less prestigious hospitals may need to go through more doctors and have a higher number. If a hospital ranks higher doctors that they think will join them, they can reduce their \u201cnumber needed to fill\u201d metric and appear more prestigious.</li> </ul> <p>Other DA Applications:</p> <ul> <li>Routing Network Packets: Suppose that instead of doctors and hospitals, you want to match packets to servers on the internet. When you own all the servers, you don't have to worry about them matching outside of your algorithm. This would apply to a company like Akamai, who owns a lot of servers\u2014and it actually works better in practice than in theory (because DA is very fact in practice). Packets typically get one of the top servers, so preferences lists are truncated, and the total running time is closer to \\(O(n)\\). Given the highly distributed nature of packet routing, every packet looks for its own server. </li> <li>Stanford Marriage Pact: Matches are made between Stanford students who want to make a pact, i.e. \"If we don't get married by time X, we will marry each other.\" DA is not used anymore (they use something closer to max weight matching instead) because in the 21st century, the graph is not bipartite, due to people having many different preferences. Stability is not a very important part of the requirement, because in practice, Stanford students spend their time looking for a partner outside of the pact/matching.</li> </ul>"},{"location":"lecture2/#recap","title":"Recap","text":"<p>Recap:</p> <ul> <li>In theory: DA in theory is Pareto-optimal among all matchings, doctor-optimal and hospital-worst among stable matchings, and doctor-strategyproof but not not hospital-strategyproof.</li> <li>In practice: DA is expensive (collecting preferences is costly, e.g. holistic admissions in US colleges and interview in hospitals) and preferences may not be captured by the model (e.g. matching couples).</li> </ul>"},{"location":"lecture3/","title":"Online Learning and Regret Minimization","text":"<p>(4/9/2025)</p> <p>Previously, agents had dominant strategies. Now, what should agents do without dominant strategies or even knowledge of the game's rules? Let's explore from a single agent's perspective.</p>"},{"location":"lecture3/#regret-minimization","title":"Regret Minimization","text":"<p>Consider investing in stocks without knowledge of future performance:</p> <ul> <li>There are \\(n\\) possible actions (stocks).</li> <li>We run an algorithm over \\(T\\) days.</li> <li>On day \\(t\\), algorithm picks action \\(ALG(t)\\).</li> <li>Reward for action \\(i\\) on day \\(t\\) is \\(r_{i,t}\\), bounded by \\(-1 \\leq r_{i,t} \\leq 1\\).</li> </ul> <p>Our goal: Maximize \\(\\sum r_{ALG(t),t}\\).</p> <p>Key Idea #1: Use historical performance to inform decisions.</p> <p>Algorithm: Follow The Leader (FTL)</p> <p>On day \\(t\\): Take the action with the highest total reward up to day \\(t-1\\).</p> <p>To measure success, we use \"regret\":</p> <p>Definition: External Regret</p> <p>External Regret = \\( \\underset{i}{\\max} \\sum_t r_{i,t} - \\sum_t r_{ALG(t),t} \\)</p> <p>Claim 1: For independently, identically distributed (iid) rewards, FTL's expected regret is \\(O(\\sqrt{T \\log(n)})\\).</p> <p>Claim 2: No algorithm can outperform \\(\\Omega(\\sqrt{T \\log(n)})\\) with iid rewards.</p>"},{"location":"lecture3/#regret-minimization-algorithms","title":"Regret Minimization Algorithms","text":"<p>FTL fails under adversarial conditions. Thus, we add randomness:</p> <p>Key Idea #4: Balance randomness with historical performance.</p> <p>Algorithm: Follow The Regularized Leader (FTRL)</p> <p>On day \\(t\\), choose distribution \\(x\\) maximizing: [\\sum_i r_{x,t} - \\frac{1}{\\eta}\\varphi(x)] - \\(\\eta\\): Balances randomness and history. - \\(\\varphi\\): Regularizer penalizing unbalanced distributions.</p> <p>Alternative (equivalent) algorithm: Multiplicative Weight Update (MWU)</p> <p>Algorithm: Multiplicative Weight Update (MWU)</p> <ul> <li>Initialize weights \\(z_{i,0} = 1\\).</li> <li>At day \\(t\\):</li> <li>Choose action \\(i\\) with probability \\(\\frac{z_{i,t}}{\\sum_j z_{j,t}}\\).</li> <li>Update weights: \\(z_{i,t+1} \\leftarrow z_{i,t} e^{\\eta r_{i,t}}\\).</li> </ul> <p>MWU and FTRL achieve expected regret \\(O(\\sqrt{T \\log(n)})\\) even with adversarial input.</p>"},{"location":"lecture3/#swap-regret-minimization-algorithms","title":"Swap-Regret Minimization Algorithms","text":"<p>Definition: Swap-Regret (Internal Regret)</p> <p>Swap-Regret = \\( \\underset{\\Phi : [n] \\to [n]}{\\max} \\sum_t (r_{\\Phi(ALG(t)),t} - r_{ALG(t),t}) \\)</p> <p>where \\(\\Phi\\) maps each action to another action.</p> <p>Swap-regret is always at least external regret. Minimizing swap-regret is harder but better.</p> <p>Algorithm: Swap-Regret Minimization</p> <ul> <li>Define meta-actions for each possible swap choice \\(\\Phi\\).</li> <li>Run MWU/FTRL on meta-actions.</li> <li>At each iteration, solve for distribution \\(x_t = \\mathbb{E}[\\Phi(x_t)]\\).</li> </ul> <p>Total Swap-Regret: \\(O(\\sqrt{T n \\log(n)})\\)</p>"},{"location":"lecture3/#regret-minimization-with-bandit-feedback","title":"Regret Minimization with Bandit Feedback","text":"<p>With partial (bandit) feedback, we can't observe rewards of actions we didn't take.</p> <p>Algorithm: UCB1 (Upper Confidence Bound)</p> <p>Define: \\(UCB_i = \\text{avg reward}_i + \\sqrt{\\frac{2\\ln(T)}{\\text{# samples of } i}}\\).</p> <ul> <li>On day \\(t\\), pick arm with max UCB.</li> </ul> <p>With iid rewards, UCB1 regret is \\(O(\\sqrt{n T \\log(T)})\\).</p> <p>Algorithm: Exp3 (Exponential-weight for Exploration and Exploitation)</p> <ul> <li>On day \\(t\\), MWU chooses an arm.</li> <li>Pseudo-rewards:</li> <li>If arm not selected: \\(\\hat{r}_{i,t}=0\\)</li> <li>If arm selected: \\(\\hat{r}_{i,t}=\\frac{r_{i,t}}{Pr[\\text{selecting }i]}\\) (Inverse Propensity Score)</li> </ul> <p>Exp3 achieves expected regret \\(O(\\sqrt{n T \\log(n)})\\) with adversarial input.</p> <p>Think-Pair-Share: Should UCB or Exp3 be used practically (e.g., news feed editor)? Consider contextual factors, clickbait, user addiction, and fake news.</p>"},{"location":"lecture3b/","title":"Top Trading Cycles","text":"<p>(1/18/2023)</p> <p>Note: This lecture was skipped during the Spring 2025 quarter. Below are notes from the Winter 2023 quarter.</p> <p>Previously, we studied:</p> <ul> <li>Random Serial Dictatorship (one-sided matching)</li> <li>Deferred Acceptance (two-sided matching)</li> </ul> <p>This lecture: What happens when participants are already endowed with goods?</p> <p>Example: Stanford PhD housing renewal\u2014students face tradeoffs between renewal and lottery entry.</p> <p>Problem Setup:</p> <ul> <li>\\(n\\) students, \\(n\\) rooms</li> <li>Each student has a current room and preference over all rooms</li> </ul> <p>Mechanism: Top Trading Cycles (TTC)</p> <p>While there are unmatched students/rooms:</p> <ol> <li>Create a graph with unmatched rooms and students as nodes.</li> <li>Draw edges:</li> <li>from each room to its current owner.</li> <li>from each student to their most preferred available room.</li> <li>Identify cycles, remove them, and execute the trades indicated by cycles.</li> </ol> <p>Theorem: Runtime of TTC</p> <p>Top Trading Cycles runs in \\(O(n^2)\\) time.</p> <p>Proof: Constructing graph and edges is \\(O(n)\\). Finding cycles via directed edges visits at most \\(2n+1\\) nodes (pigeonhole principle), thus \\(O(n)\\). Each iteration removes at least one student; thus, overall complexity is \\(O(n^2)\\).</p> <p>Input size is \\(n^2\\) (preference lists of length \\(n\\)), so TTC is linear relative to input size.</p> <p>Definition: Individual Rationality (IR)</p> <p>A mechanism is individually rational if no agent is worse off after participating.</p> <p>Theorem: TTC is Individually Rational</p> <p>Proof: Students trade rooms only if they prefer the new room.</p> <p>Theorem: Strategyproofness of TTC</p> <p>Top Trading Cycles is strategyproof.</p> <p>This proof was noted as flawed; a corrected proof was expected.</p> <p>Theorem: Efficiency of TTC</p> <p>TTC allocation is Pareto-optimal.</p> <p>Proof: Equivalent to serial dictatorship in order cycles formed. Serial dictatorship is Pareto-optimal, thus TTC is Pareto-optimal.</p>"},{"location":"lecture3b/#top-trading-cycles-and-chains-ttcc","title":"Top Trading Cycles and Chains (TTCC)","text":"<p>Graduating students and new students (without rooms) require modifications:</p> <p>Mechanism: Top Trading Cycles with Chains (TTCC)</p> <ol> <li>Process students in random order.</li> <li> <p>Mark student \\(i\\) visited:</p> </li> <li> <p>If \\(i\\)'s top room is unoccupied, assign room, release previous.</p> </li> <li>If room occupied by unvisited student \\(j\\), move \\(j\\) ahead of \\(i\\).</li> <li>If room occupied by visited student \\(j\\), cycle identified.</li> </ol> <p>TTCC maintains strategyproofness, Pareto-optimality, individual rationality, and efficiency.</p>"},{"location":"lecture3b/#ttcc-in-practice","title":"TTC(C) in Practice","text":"<p>Why isn't TTCC common?</p> <ul> <li>Running TTCC repeatedly can break strategyproofness over multiple years.</li> <li>Increased strategic behavior over \"popular\" rooms.</li> </ul> <p>Application: School choice (New Orleans, 2011-12) used TTC but switched to DA (Deferred Acceptance) due to simplicity in explanation.</p> <p>College Admissions: Universities prefer specific applicants; direct trades don't apply.</p> <p>Kidney Transplants: Kidney exchange involves patient-donor compatibility issues and logistical constraints.</p> <p>Considerations in Kidney Exchange:</p> <ul> <li>Strategyproofness less critical (compatibility-based).</li> <li>Priority considerations (health urgency).</li> <li>Stability and central mechanism incentivization.</li> <li>Logistical challenge: Long cycles require simultaneous transplants; chains manageable.</li> <li>Dynamic arrivals/departures.</li> <li>Strategic hospitals may internally match.</li> <li>High failure probability (93% matches fail).</li> <li>Ethical issues and multi-organ exchange possibilities.</li> </ul>"},{"location":"lecture4/","title":"Equilibria in Games","text":"<p>(4/14/2025)</p> <p>Previously, agents had dominant strategies. Now, we consider what happens when no dominant strategy exists.</p> <p>Example: Penalty Kicks</p> Kick Left Kick Right Jump Left 0.5 0.8 Jump Right 0.9 0.2 <p>No pure-strategy equilibrium exists because each player has an incentive to deviate.</p> <p>Solution: Mixed strategies\u2014kicker left/right (0.6, 0.4), goalie left/right (0.7, 0.3). Expected scoring probability becomes equal (0.62), providing no incentive to deviate.</p> <p>Definition: Mixed Strategy</p> <p>A probability distribution over pure strategies.</p> <p>Definition: Nash Equilibrium</p> <p>A strategy profile where no player benefits from deviating unilaterally.</p>"},{"location":"lecture4/#equilibrium-in-2-player-zero-sum-games","title":"Equilibrium in 2-Player Zero-Sum Games","text":"<p>Assumptions: - Exactly 2 players. - Sum of payoffs is zero.</p> <p>Player one aims: \\(\\max_{s_1} \\min_{s_2} u(s_1,s_2)\\)</p> <p>Player two aims: \\(\\min_{s_2} \\max_{s_1} -u(s_1,s_2)\\)</p> <p>Theorem: Min-Max Payoffs</p> <p>In two-player zero-sum games, the max-min payoff equals Nash equilibrium payoff equals min-max payoff.</p> <p>Definition: Game Theory Terminology</p> <ul> <li>Game Node: State in the game.</li> <li>Game Tree: Graph showing reachable game nodes.</li> <li>Information Set: Game nodes indistinguishable given player's information.</li> </ul> <p>Example: Poker</p> <p>\"Heads Up\" poker has \\(10^{161}\\) states. Approximate strategies (blueprints) and regret minimization algorithms enable practical computation.</p> <p>Theorem: Regret Minimization Convergence</p> <p>In two-player zero-sum games, regret minimization converges to Nash equilibrium.</p>"},{"location":"lecture4/#nash-equilibrium-in-non-zero-sum-games","title":"Nash Equilibrium in Non-Zero-Sum Games","text":"<p>Theorem: Nash's Existence Theorem (1951)</p> <p>Every finite game has at least one Nash equilibrium (possibly mixed strategies).</p> <p>Issues: - Equilibria not unique. - Hard to compute. - May not make practical sense.</p> <p>Example: Grade Game</p> <p>Two students choose grades \\(x, y \\in \\{2,...,99\\}\\). Equilibrium: \\(x = y = 2\\).</p> <p>Example: Intersection Game</p> Go Wait Go (-99,-99) (1,0) Wait (0,1) (0,0) <ul> <li>Pure equilibria: (Go, Wait), (Wait, Go)</li> <li>Mixed equilibrium: small probability for simultaneous Go.</li> <li>Correlated Equilibrium: Randomize between (Go, Wait) and (Wait, Go).</li> </ul> <p>Definition: Correlated Equilibrium</p> <p>Players follow a correlated distribution of recommended actions. No player benefits from deviating given the recommendation.</p> <p>Correlated equilibria can be efficiently computed.</p> <p>Definition: Stackelberg Equilibrium</p> <p>Strategy profile with leader committing first, follower optimally responds, and leader optimally chooses commitment.</p> <p>Leader commitment can result in higher payoffs than correlated equilibria.</p> <p>Example: Security Games</p> <ul> <li>Airport security</li> <li>Infrastructure defense</li> <li>Cybersecurity</li> <li>Anti-poaching</li> </ul>"},{"location":"lecture5/","title":"P2P File-sharing Dilemma","text":"<p>(4/14/2025)</p> <p>Historical Context: Napster (1999-2001), Gnutella (2000 onwards). Issue: Free-riding.</p> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <p>Dominant Nash equilibrium: both free-ride (socially worst outcome).</p>"},{"location":"lecture5/#repeated-games","title":"Repeated Games","text":"<p>Iterating game \\(n\\) times. What happens?</p> <p>Definition: Subgame Perfect Nash Equilibrium (SPNE)</p> <p>A Nash equilibrium valid in every subgame.</p> <p>Repeated free-riding emerges by backward induction.</p>"},{"location":"lecture5/#repeated-games-with-discounting","title":"Repeated Games with Discounting","text":"<p>Probability \\(p\\) that the game stops each round (discount future payoffs).</p> <p>Grim Trigger Strategy: Free-ride forever if opponent ever free-rides.</p> <ul> <li>Always uploading payoff: \\(\\frac{2}{p}\\)</li> <li>Always free-riding payoff: \\(3\\)</li> </ul> <p>Equilibrium if \\(p &lt; 2/3\\).</p> <p>Tit-for-Tat: Initially upload, then match opponent\u2019s previous action. Robust and stable.</p>"},{"location":"lecture5/#bittorrent-strategies","title":"BitTorrent Strategies","text":"<p>BitTorrent is a popular P2P protocol (~30% upload traffic).</p> <p>Default Strategy: - Tracker coordinates peers. - Tit-for-tat-based uploads, optimistic unchoking for random peers.</p> <p>BitThief: Never upload, frequent peer querying.</p> <p>BitTyrant: Upload strategically to maximize future downloads (ratio-based).</p>"},{"location":"lecture6/","title":"Market Equilibrium","text":"<p>(4/16/2025)</p> <p>Previously, we examined scenarios without monetary transfers. Introducing money changes market dynamics substantially:</p> <ul> <li>Stanford housing would differ if rooms were auctioned.</li> <li>Buying organs is illegal in most countries.</li> <li>Hospitals-student monetary matches face legal restrictions.</li> </ul> <p>Issues with non-monetary markets: 1. Limited to ordinal rather than cardinal preferences. 2. Emergence of underground markets. 3. Potential exploitation (bots manipulating donor lists).</p> <p>Definition: Cardinal vs Ordinal Utilities</p> <ul> <li>Cardinal: Assign numeric values to preferences.</li> <li>Ordinal: Only ranks preferences by order.</li> </ul> <p>Cardinal utilities convey richer information (ordinal can be derived from cardinal), are intuitive in economic analysis, and typically monetarily measurable.</p> <p>Definition: Fungible vs Idiosyncratic Goods</p> <ul> <li>Fungible goods: Interchangeable units.</li> <li>Idiosyncratic goods: Unique goods.</li> </ul> <p>Many real-world goods blend these traits (e.g., ridesharing).</p> <p>Definition: Supply and Demand Curves</p> <ul> <li>Demand curve: Quantity consumers buy at each price.</li> <li>Supply curve: Quantity firms sell at each price.</li> </ul> <p>Typically:</p> <ul> <li>Price is vertical (\\(y\\)-axis).</li> <li>Quantity is horizontal (\\(x\\)-axis).</li> </ul> <p>Usually, demand slopes downward and supply upward. However, exceptions (monopoly markets, certain special goods) exist.</p> <p>Definition: Market Clearing Price</p> <p>The price where supply equals demand. All buyers and sellers transact, \"clearing\" the market.</p>"},{"location":"lecture6/#unit-demand-market-model","title":"Unit-Demand Market Model","text":"<p>Setup:</p> <ul> <li>\\(m\\) idiosyncratic goods.</li> <li>\\(n\\) buyers (each buys at most one good).</li> <li>Buyer \\(i\\) values good \\(j\\) at \\(v_{i,j}\\).</li> <li>Buyer \\(i\\)'s payoff for good \\(j\\): \\(U_{i,j} = v_{i,j} - p_j\\).</li> <li>Utility without buying: \\(U_{i,\\varnothing} = 0\\).</li> </ul> <p>Definition: Competitive Equilibrium</p> <p>A price vector \\(p = (p_1,\\dots,p_m)\\) and matching \\(M:\\{1,\\dots,n\\}\\to\\{1,\\dots,m\\}\\) satisfying:</p> <ol> <li>Buyers get their preferred good at price \\(p\\):    \\(\\(v_{i,M(i)} - p_{M(i)} \\geq v_{i,j} - p_j \\quad \\forall i,j.\\)\\)</li> <li>Unmatched goods have price zero.</li> <li>Buyers are unmatched only if \\(v_{i,j}-p_j&lt;0\\) for all goods \\(j\\).</li> </ol> <p>This implies individual rationality, ensuring buyers never choose negatively valued outcomes.</p> <p>Definition: Social Welfare</p> <p>Sum of buyers' values: \\(\\(U(M)=\\sum_i v_{i,M(i)}.\\)\\)</p> <p>Prices excluded as buyer costs equal seller profits.</p>"},{"location":"lecture6/#properties-of-competitive-equilibrium","title":"Properties of Competitive Equilibrium","text":"<p>Theorem: First Welfare Theorem</p> <p>Competitive equilibrium allocation maximizes social welfare: \\(\\(\\sum_i v_{i,M(i)} \\geq \\sum_i v_{i,M'(i)} \\quad \\forall M'.\\)\\)</p> <p>Proof: By competitive equilibrium definition, for any alternative matching \\(M'\\): \\(\\(\\sum_i(v_{i,M(i)}-p_{M(i)})\\geq \\sum_i(v_{i,M'(i)}-p_{M'(i)}).\\)\\) Since total prices paid are equal, we get welfare optimality.</p> <p>Theorem: Existence of Competitive Equilibrium</p> <p>Competitive equilibrium always exists in finite unit-demand markets with discrete prices.</p> <p>In more complex markets, equilibrium existence isn't guaranteed.</p> <p>Proof (via Deferred Acceptance with Prices):</p> <p>Construct ranked lists of (good, price) pairs for each buyer (discarding negative options). Buyers iteratively propose to their next-best option; goods tentatively accept highest offers. Termination guaranteed (similar to deferred acceptance).</p> <p>Resulting allocation meets competitive equilibrium conditions:</p> <ul> <li>Buyers matched to best possible option.</li> <li>Unmatched goods priced zero.</li> </ul> <p>Proposition: Deferred Acceptance with Prices</p> <p>The deferred acceptance with prices algorithm is strategyproof and buyer-optimal.</p> <p>Proof: Direct from standard deferred acceptance reasoning.</p>"},{"location":"lecture7/","title":"Market Failures","text":"<p>(4/21/2025)</p> <p>Previously, we assumed free markets converge naturally to optimal outcomes. However, markets often fail to achieve optimal outcomes\u2014known as market failures.</p>"},{"location":"lecture7/#externalities-and-public-goods","title":"Externalities and Public Goods","text":"<p>Externalities: Effects of transactions on third parties not directly involved.</p> <p>Public Goods: Goods accessible to everyone and not owned individually.</p> <p>Examples: - Environmental harm from air travel (negative externality). - WiFi bandwidth overuse (negative externality).</p> <p>Solutions: 1. Pigouvian Tax: Tax transactions proportional to the externality they cause. 2. Coasian Bargaining: Privatize public goods and let market bargaining reach social efficiency (Coase theorem).</p>"},{"location":"lecture7/#transaction-costs","title":"Transaction Costs","text":"<p>Costs associated with making transactions that prevent beneficial trades.</p> <p>Examples: - Sales tax blocking a mutually beneficial sale. - Time/information costs preventing trades (hungry student looking for apples).</p> <p>Transaction costs can be monetary or non-monetary (time, effort, information).</p>"},{"location":"lecture7/#market-thickness","title":"Market Thickness","text":"<ul> <li>Thick Market: Many buyers/sellers, competitive prices, efficient outcomes.</li> <li>Thin Market: Few buyers/sellers, monopoly pricing, inefficiencies.</li> </ul> <p>Why monopolies are problematic: - Reduced quantity sold. - Higher prices. - Lower consumer surplus.</p> <p>The first welfare theorem applies with cost-covering reserve prices, but these reserve prices aren't strategyproof:</p> <p>Proof:</p> <p>If reserve prices are nonbinding, sellers can slightly increase them and remain profitable. If binding (equal to cost), sellers earn zero profit, thus increasing prices slightly above cost yields positive profit. This creates a profitable deviation.</p> <p>Ways around this: - In large markets, deviations from strategyproofness diminish (strategyproof-in-the-large). - Limited seller information on buyer valuations restricts strategic price setting.</p> <p>Thickening Markets: - Encourage participation and retention. - Merge markets (e.g., larger kidney exchanges). - Batch transactions to aggregate buyers and sellers.</p>"},{"location":"lecture7/#timing-issues","title":"Timing Issues","text":"<p>Market Unraveling: - Matches occur prematurely (medical students matched early, creating mismatches over time).</p> <p>Exploding Offers: - Offers requiring rapid responses due to competitive pressures or regulatory constraints.</p> <p>Examples: - APPIC psychologist hiring (1970\u20131990s) faced early and premature matching despite rules.</p> <p>Resolving Timing Issues: - Centralized matching mechanisms. - Allow reneging on exploding offers to remove incentives to rush decisions.</p>"},{"location":"lecture7/#information-asymmetries","title":"Information Asymmetries","text":"<p>Sellers know more about goods than buyers, causing uncertainty and inefficiency.</p> <p>Example: Market for Lemons (used cars) - Good and bad cars indistinguishable by buyers. - Buyers pay less due to uncertainty. - Good cars leave market; only bad cars remain (adverse selection).</p> <p>Possible equilibria: - Bad equilibrium: only lemons sold at low prices. - Good equilibrium: enough good cars maintain higher average quality and price.</p> <p>Other Examples: - Health insurance markets (buyer health knowledge). - Clickbait (creators know content quality; users do not).</p> <p>Adverse Selection: Poor-quality products dominate markets due to asymmetry.</p> <p>Solutions: - Increase transparency (mandatory disclosure, reputation systems). - Restrict informational use (universal healthcare, insider trading rules). - Filter lemons through warranties or guarantees.</p>"},{"location":"lecture8/","title":"Single-Unit Auctions","text":"<p>(4/23/2025)</p> <p>Auctions are valuable in settings where price discovery is needed, such as:</p> <ul> <li>Monopolies: Wireless spectrum auctions.</li> <li>Niche products: Rare items on eBay.</li> <li>Specialized products: Ad auctions.</li> </ul> <p>Auctions intersect significantly with Computer Science:</p> <ul> <li>Ad auctions fund many CS researchers.</li> <li>Fast auctions necessitate algorithmic bidders.</li> <li>Complex auctions require algorithmic auctioneers.</li> </ul>"},{"location":"lecture8/#case-of-one-buyer","title":"Case of One Buyer","text":"<p>Motivation: Digital goods pricing for maximizing revenue.</p> <ul> <li>Demand curves derived from users' willingness to pay.</li> <li>Revenue = Price \u00d7 Number of buyers willing to pay that price.</li> <li>Roger Myerson (1981): For a demand curve \\(D\\), a formula \\(p(D)\\) exists that maximizes revenue (optimal reserve price).</li> </ul> <p>Application: Ad auctions with specialized advertisers (single bidder per spot) using prior beliefs as demand curves.</p>"},{"location":"lecture8/#case-of-multiple-buyers","title":"Case of Multiple Buyers","text":"<p>Model: 1. Set of bidders \\(I\\), each with valuation \\(v_i\\). 2. Seller doesn't know valuations but has prior beliefs. 3. Bidder payoff: \\(v_i - p_i\\) if they win, else \\(-p_i\\).</p>"},{"location":"lecture8/#first-price-auction","title":"First-Price Auction","text":"<ul> <li>Each bidder submits a sealed bid \\(b_i\\).</li> <li>Highest bidder wins and pays their bid.</li> </ul> <p>Note: Equilibrium bids are below true valuations (\\(b_i &lt; v_i\\)).</p>"},{"location":"lecture8/#all-pay-auction","title":"All-Pay Auction","text":"<ul> <li>Each bidder submits a sealed bid \\(b_i\\).</li> <li>Highest bidder wins the item.</li> <li>All bidders pay their bids.</li> </ul> <p>Used for modeling non-auction scenarios (political donations, animal contests). Equilibrium bids are lower than valuations and generally lower than in first-price auctions.</p>"},{"location":"lecture8/#second-price-auction","title":"Second-Price Auction","text":"<ul> <li>Each bidder submits a sealed bid \\(b_i\\).</li> <li>Highest bidder wins, paying the second-highest bid.</li> </ul> <p>Theorem: Equilibrium in Second-Price Auctions</p> <p>Second-price auctions are strategyproof: truthful bidding (\\(b_i = v_i\\)) is dominant and thus an equilibrium.</p> <p>Proof: Fix other bids \\(b_j\\), let \\(b^{(-i)} = \\max_{j \\ne i} b_j\\). Two cases:</p> <ol> <li>If \\(v_i &lt; b^{(-i)}\\), bidder \\(i\\) prefers losing; bidding truthfully is optimal.</li> <li>If \\(v_i &gt; b^{(-i)}\\), bidder \\(i\\) prefers winning; again, truthful bid is optimal.</li> </ol> <p>Theorem: Payoffs of Second-Price Auctions</p> <p>Second-price auctions are individually rational at equilibrium: \\(v_i - p_i \\geq 0\\).</p> <p>Proof: If bidder doesn't win, payoff = 0. If bidder wins, pays at most their valuation, thus non-negative payoff.</p> <p>Theorem: Optimality</p> <p>In equilibrium, second-price auctions allocate the good to the highest-valued bidder.</p> <p>Proof: In equilibrium \\(b_i = v_i\\), thus highest bidder corresponds to highest valuation.</p>"},{"location":"lecture8/#revenue-maximization","title":"Revenue Maximization","text":"<p>Example: Full Information Scenario</p> <ul> <li>\\(A\\) values item at 1, \\(B\\) values at 2.</li> <li>Second-price auction: \\(B\\) wins, pays 1.</li> <li>First-price auction: Equilibrium bids near 1, revenue approximately 1.</li> </ul> <p>To handle uncertainty, we define Bayesian Nash Equilibrium:</p> <p>Definition: Bayesian Nash Equilibrium</p> <p>A strategy profile where each player's strategy maximizes expected payoff given beliefs about others' strategies.</p> <p>Example: Bayesian Agents</p> <ul> <li>\\(A,B\\) values drawn uniformly from [0,1].</li> <li>Second-price auction expected revenue: \\(\\frac{1}{3}\\).</li> <li>First-price auction equilibrium bids: \\(b_i = \\frac{v_i}{2}\\), also yielding expected revenue \\(\\frac{1}{3}\\).</li> </ul> <p>Theorem: Revenue Equivalence</p> <p>At equilibrium, expected payments depend only on the auction\u2019s allocation rule.</p> <p>Corollary: First-price, second-price, and all-pay auctions yield identical equilibrium revenue under Bayes-Nash equilibria with standard allocation rules (highest bidder wins).</p>"},{"location":"lecture8/#deviations-from-optimal-allocation","title":"Deviations from Optimal Allocation","text":"<p>Auctions don't always allocate to the highest valuation bidder:</p> <ul> <li>Overbidding in second-price auctions (highest bidder might bid excessively).</li> <li>All-pay auctions may have equilibria not awarding highest value bidder.</li> <li>Reserve prices might result in no winner.</li> </ul>"}]}