{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>These course notes for Stanford CS269I are based on the content of Aviad Rubinstein's lectures in the Spring 2025 quarter. This site is developed by Thibaud Clement, based on Eric Gao's original course notes from the Winter 2023 quarter.</p> <p>Computers/algorithms control a lot of economic processes, including online retail, online advertising, algo-trading, cryptocurrencies, etc. There is a need to jointly understand economics and computation to analyze these applications. While relying on an increase of computing power (more GPUs) may be an option to compensate for inefficiencies, some specific cases may actually justify the need for efficient algorithms.</p> <p>When it comes to incentives, leveraging more GPUs is not the solution. Computer scientists often need to build systems that interact with other agents, such as users, algorithms, Large Language Models, and even other systems. It is crucial to consider that these other agents are going to behave selfishly to predict how they are going to interact with the system, and design an implement said accordingly.</p> <p>CS269I aims to teach students how to think about incentives (through lagnauge, frameworks, and practice), to get familiar with economic theory concepts (i.e. stable matching, proof-of-work, etc.), and analyze case studies to understand potential gaps between theory and practice.</p>"},{"location":"lecture1/","title":"One-Sided Matching and Serial Dictatorship","text":"<p>(3/31/2025)</p> <p>The Stanford Undergrad Housing Problem:</p> <ul> <li>There are \\( n \\) students, each with some preference over dorms.</li> <li>There are \\( m \\) dorms, each with some capacity (for simplicity, assume capacity is 1 for every dorm).</li> </ul> <p>We want to assign students to dorms.</p>"},{"location":"lecture1/#max-weight-matching","title":"Max Weight Matching","text":"<p>One possible solution is to maximize total happiness:</p> <p>Mechanism: Happiness Maximization</p> <ol> <li>Create a bipartite graph where:<ul> <li>One side represents students.</li> <li>The other side represents dorms.</li> <li>Each edge from a student to a dorm represents how much that student likes that dorm.</li> </ul> </li> <li>Determine the max-weight matching (i.e., Hungarian Algorithm).</li> </ol> <p>Key question: How does the algorithm determine how much each student likes each residency? Only the students themselves know how much they like each dorm, so we need to rely on students to tell the algorithm.</p> <p>Challenge: Each student has an incentive to exaggerate how much they like their favorite dorm and undercut how much they like other dorms (using a \"0\" or \"negative infinity\" weight for those dorms). In other words: max-weight bipartite matching allows students to game the system.</p> <p>This occurs because finding the matching that maximizes total happiness is conceptually right, but this naive max-weight matching fails to take incentives into account. Another possible algorithm is Serial Dictatorship.</p>"},{"location":"lecture1/#serial-dictatorship","title":"Serial Dictatorship","text":"<p>Mechanism: Serial Dictatorship</p> <ol> <li>Sort students in some fixed order (random, seniority, alphabetically, etc.).</li> <li>Go through the list in that order and allow each student to select their most preferred available dorm.</li> </ol> <p>How is this better? There can still be students unhappy with their result. Indeed, a common complaint about Serial Dictatorship is unfairness: the first student chooses whichever dorm they want, while the last student gets only the last pick.</p> <p>When sorting is random, Serial Dictatorship guarantees ex-ante fairness: before the random sorting, all students have equal chances. However, after the sorting happens (even randomly), Serial Dictatorship loses fairness: there's no guarantee of ex-post fairness.</p> <p>Definition (Mechanism): A mechanism consists of three things:</p> <ol> <li>A method of collecting inputs from agents,</li> <li>An algorithm that acts on the inputs,</li> <li>An action taken based on the output of the algorithm.</li> </ol> <p>In our context (Stanford Housing Problem), based on inputs (dorm preferences) and the algorithm (available dorm after each student's pick), the mechanism assigns dorms.</p> <p>Note: All three components matter because there's a feedback loop: how we use inputs impacts what students report as preferences. The naive bipartite matching approach encourages students to game the system by misreporting their preferences.</p> <p>Thus, when designing a mechanism, we must consider: - The algorithm itself, - How/where inputs come from, - Actions taken based on inputs, - How promised actions affect reported inputs.</p> <p>Definition (Strategyproofness/Truthfulness): A mechanism is strategyproof if it's in every agent's best interest to act truthfully, i.e., to report true preferences.</p> <p>Definition (Dominant Strategy Incentive Compatible - DSIC): (Formal Game Theory concept) A mechanism is dominant strategy incentive compatible if truthfulness is a dominant strategy for each participant. That is, being truthful is a best response regardless of other players' actions.</p> <p>Theorem: You Cannot Game Serial Dictatorship</p> <p>It is in every student's best interest to choose their favorite available dorm in their turn.</p> <p>Proof: Your room choice doesn't affect room availability before your turn. When your turn arrives, your best action is choosing your favorite available room.</p> <p>Definition (Pareto Optimality): An assignment \\( A \\) is Pareto optimal if, for any other assignment \\( B \\), there's at least one participant strictly preferring \\( A \\) over \\( B \\).</p> <p>Theorem: You Cannot Make Everyone Happier Without Making Someone Sadder</p> <p>Serial Dictatorship assignments are Pareto Optimal.</p> <p>Proof: Assume, for contradiction, a different assignment exists making everyone as happy or happier. Consider the first student assigned differently: All better dorms are already assigned identically to students before them. Thus, the first differing student gets a worse dorm and becomes less happy\u2014a contradiction. Therefore, Serial Dictatorship is Pareto Optimal.</p>"},{"location":"lecture1/#additional-discussion","title":"Additional Discussion","text":"<p>Why is truthfulness good? It prevents corruption and ensures fairness by removing any insider advantage.</p> <p>When is truthfulness important? When optimizing happiness, truthful inputs ensure correct objectives.</p> <p>When is relaxing truthfulness acceptable?  In contexts with social norms where truthfulness isn't expected (e.g., poker games), strategyproofness isn't crucial.</p> <p>Fairness/equity issues with Serial Dictatorship: Fairness depends on sorting order. Seniority or randomness provide fairness ex-ante but not necessarily ex-post.</p>"},{"location":"lecture2/","title":"Stable Two-Sided Matchings","text":"<p>(4/2/2025)</p> <p>After medical school, med students start their internship called a \"residency.\" Each (prospective) doctor has preferences over hospitals, and each hospital has preferences over doctors. How should doctors and hospitals be matched?</p> <p>Key Nuances:</p> <ul> <li>The biggest difference between doctor-residency matching and student-dorm matching is that we now have two-sided preferences (each doctor and hospital have preferences over each other).</li> <li>Matching students to dorms is a centralized process; matching doctors to hospitals requires incentivizing participants to use a centralized process to prevent side deals.</li> </ul> <p>Definition: Blocking Pair</p> <p>Given a match \\(M\\), the pair (doctor \\(i\\), hospital \\(j\\)) forms a blocking pair if they prefer each other to their current assignments in \\(M\\).</p> <p>For example, if Doctor \\(n\\) prefers Stanford over UCSF and Stanford prefers Doctor \\(n\\) over Doctor 1, who is currently matched, then (Doctor \\(n\\), Stanford) form a blocking pair. This results in an Unstable Matching.</p> <p>Note: Blocking pairs exist only in two-sided matching markets.</p> <p>Definition: Stable Matching</p> <p>A matching \\(M\\) is stable if there are no blocking pairs. Equivalently, for every unmatched pair \\((i,j)\\), either:</p> <ul> <li>Doctor \\(i\\) prefers Hospital \\(M(i)\\) over Hospital \\(j\\), or;</li> <li>Hospital \\(j\\) prefers Doctor \\(M(j)\\) over Doctor \\(i\\).</li> </ul> <p>Key Point: Stability removes incentives to deviate from the centralized matching.</p>"},{"location":"lecture2/#deferred-acceptance","title":"Deferred Acceptance","text":"<p>Main idea: Each doctor proposes to their favorite hospital that hasn't rejected them yet. Hospitals accept the best available candidate.</p> <p>Mechanism: Deferred Acceptance</p> <p>While there's an unmatched doctor \\(i\\):</p> <ol> <li>Doctor \\(i\\) proposes to their next-favorite hospital \\(j\\).</li> <li>If hospital \\(j\\) has no match, they accept doctor \\(i\\).</li> <li>Else, if hospital \\(j\\) prefers their current match over doctor \\(i\\), doctor \\(i\\) remains unmatched.</li> <li>Else, hospital \\(j\\) matches with doctor \\(i\\), releasing their previous match.</li> </ol> <p>The algorithm stops when everyone is matched.</p> <p>Example: - Doctors: Alice (X, Y, Z), Bob (Y, X, Z), Charlie (Y, Z, X). - Hospitals: X (Bob, Alice, Charlie), Y (Alice, Bob, Charlie), Z (Bob, Charlie, Alice).</p> <p>Stable matching result: \\((Alice, X), (Bob, Y), (Charlie, Z)\\).</p> <p>Theorem: Runtime of Deferred Acceptance</p> <p>Deferred Acceptance runs in \\(O(n^2)\\) time.</p> <p>Proof: There are at most \\(n^2\\) proposals (each doctor proposes at most \\(n\\) hospitals). Each proposal is \\(O(1)\\), hence total runtime is \\(O(n^2)\\).</p> <p>Theorem: Deferred Acceptance is Stable</p> <p>Deferred Acceptance outputs a complete stable matching.</p> <p>Proof (sketch): Proven by three claims:</p> <ol> <li>Current match stable at each iteration.</li> <li>Once matched, hospitals remain matched.</li> <li>At completion, everyone is matched.</li> </ol> <p>Therefore, no blocking pairs exist.</p> <p>Theorem: Efficiency of Stable Matchings</p> <p>Every stable matching is Pareto-optimal.</p> <p>Proof: Any deviation from a stable matching worsens at least one participant\u2019s outcome.</p> <p>Theorem: Proposing Optimality</p> <p>Doctor-proposing Deferred Acceptance yields the doctor-optimal stable matching.</p> <p>Corollary: Doctor-proposing DA is strategyproof for doctors.</p> <p>Theorem: Receiving Non-Optimality</p> <p>Doctor-proposing Deferred Acceptance yields the worst stable matching for hospitals.</p> <p>Theorem: Receiving Non-Strategyproofness</p> <p>Hospitals can benefit from misreporting preferences.</p> <p>Think-Pair-Share: How to make DA hospital-optimal? Flip roles (hospital-proposing).</p>"},{"location":"lecture2/#deferred-acceptance-in-practice","title":"Deferred Acceptance in Practice","text":"<p>Why no DA in US undergrad admissions? Decentralized applications, holistic admission processes, larger applicant pools.</p> <p>Historical Doctor-Hospital Matching: - 1950s: DA-like algorithms initially used. - 1960s: Couples complicate preferences. - 1980s: Negative results\u2014stable matching existence with couples is NP-complete. - 1990s: Extended DA for couples adopted.</p> <p>Practical Doctor Ranking: - Interviews (costly) - Tests (changing role due to USMLE) - Safety choices (not actually safer)</p> <p>Theorem: No Improvements from Safety Choices</p> <p>Misranking doctors (safety choices) does not secure a better match.</p> <p>Proof: Misreporting can only worsen or have no effect on outcomes.</p> <p>Hospitals use safety choices due to ignorance or prestige metrics (\"number needed to fill\").</p> <p>Other DA Applications: - Routing Network Packets: Distributed, truncated preference lists. - Stanford Marriage Pact: Originally DA-based, now max-weight matching; stability less critical.</p> <p>Recap: DA theoretically optimal but practical challenges (evaluating preferences) remain significant.</p>"}]}