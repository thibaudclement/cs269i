{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Computers/algorithms control a lot of economic processes, including online retail, online advertising, algo-trading, cryptocurrencies, etc. There is a need to jointly understand economics and computation to analyze these applications. While relying on an increase of computing power (more GPUs) may be an option to compensate for inefficiencies, some specific cases may actually justify the need for efficient algorithms.</p>"},{"location":"#motivation","title":"Motivation","text":"<p>When it comes to incentives, leveraging more GPUs is not the solution. Computer scientists often need to build systems that interact with other agents, such as users, algorithms, Large Language Models, and even other systems. It is crucial to consider that these other agents are going to behave selfishly to predict how they are going to interact with the system, and design an implement said accordingly.</p>"},{"location":"#goals","title":"Goals","text":"<p>CS269I aims to teach students how to think about incentives (through lagnauge, frameworks, and practice), to get familiar with economic theory concepts (i.e. stable matching, proof-of-work, etc.), and analyze case studies to understand potential gaps between theory and practice.</p>"},{"location":"#credit","title":"Credit","text":"<p>These course notes for Stanford CS269I are based on the content of Aviad Rubinstein's lectures in the Spring 2025 quarter. This site is developed by Thibaud Clement, based on Eric Gao's original course notes from the Winter 2023 quarter and Thibaud Clement's own course notes from the Spring 2025 quarter..</p>"},{"location":"lecture1/","title":"One-Sided Matching and Serial Dictatorship","text":"<p>March 31, 2025</p> <p>The Stanford Undergrad Housing Problem:</p> <ul> <li>There are \\( n \\) students, each with some preference over dorms.</li> <li>There are \\( m \\) dorms, each with some capacity (for simplicity, assume capacity is 1 for every dorm).</li> </ul> <p>How can we assign students to dorms?</p>"},{"location":"lecture1/#max-weight-matching","title":"Max Weight Matching","text":"<p>One possible solution is to maximize total happiness:</p> <p>Mechanism: Happiness Maximization</p> <ol> <li>Create a bipartite graph where:<ul> <li>One side represents students.</li> <li>The other side represents dorms.</li> <li>Each edge from a student to a dorm represents how much that student likes that dorm.</li> </ul> </li> <li>Determine the max-weight matching (i.e., Hungarian Algorithm).</li> </ol> <p>Key question: How does the algorithm determine how much each student likes each residency? Only the students themselves know how much they like each dorm, so we need to rely on students to tell the algorithm.</p> <p>Challenge: Each student has an incentive to exaggerate how much they like their favorite dorm and undercut how much they like other dorms (using a \"0\" or \"negative infinity\" weight for those dorms). In other words: max-weight bipartite matching allows students to game the system.</p> <p>This occurs because finding the matching that maximizes total happiness is conceptually right, but this naive max-weight matching fails to take incentives into account. Another possible algorithm is Serial Dictatorship.</p>"},{"location":"lecture1/#serial-dictatorship","title":"Serial Dictatorship","text":"<p>This is how Stanford actually assigns dorms to students.</p> <p>Mechanism: Serial Dictatorship</p> <ol> <li>Sort students in some fixed order (random, seniority, alphabetically, etc.).</li> <li>Go through the list in that order and allow each student (the \"dictator\") to select their most preferred available dorm.</li> </ol> <p>How is this better? There can still be students unhappy with their result. Indeed, a common complaint about Serial Dictatorship is unfairness: the first student chooses whichever dorm they want, while the last student gets only the last pick.</p> <p>When sorting is random, Serial Dictatorship guarantees ex-ante fairness: before the random sorting, all students have equal chances. However, after the sorting happens (even randomly), Serial Dictatorship loses fairness: there's no guarantee of ex-post fairness.</p> <p>Definition: Mechanism</p> <p>A mechanism consists of three things:</p> <ol> <li>A method of collecting inputs from agents,</li> <li>An algorithm that acts on the inputs,</li> <li>An action taken based on the output of the algorithm.</li> </ol> <p>In our context (Stanford Undergrad Housing Problem), based on inputs (dorm preferences) and the algorithm (available dorm after each student's pick), the mechanism takes actions (it assigns dorms).</p> <p>Note: All three components matter because there's a feedback loop: how we use inputs impacts what students report as preferences. The naive bipartite matching approach encourages students to game the system by misreporting their preferences.</p> <p>Thus, when designing a mechanism, we must consider:</p> <ul> <li>The algorithm itself,</li> <li>How/where inputs come from,</li> <li>Actions taken based on inputs,</li> <li>How promised actions affect reported inputs.</li> </ul> <p>Definition: Strategyproofness/Truthfulness</p> <p>A mechanism is strategyproof if it's in every agent's best interest to act truthfully, i.e., to report true preferences.</p> <p>Even more formally, game theorists somtimes use the term Dominant Strategy Incentive Compatible:</p> <p>Definition: Dominant Strategy Incentive Compatible - DSIC</p> <p>A mechanism is dominant strategy incentive compatible if truthfulness is a dominant strategy for each participant. That is, being truthful is a best response regardless of other players' actions.</p> <p>Theorem: You Cannot Game Serial Dictatorship</p> <p>It is in every student's best interest to choose their favorite available dorm in their turn. Formally, we say that Serial Dictatorship is strategyproof or truthful.</p> <p>Proof:</p> <ul> <li>Your room choice doesn't affect room availability before your turn.</li> <li>When your turn arrives, your best action is choosing your favorite available room.</li> </ul> <p>Why does this prove that Serial Dictatorship is strategyproof? Until your turn comes, it is other students who are bidding, and there is nothing you can do to affect it. However, when comes your turn, you choose what you get, so you should choose the best thing for you.</p> <p>Definition: Pareto Optimality</p> <p>An assignment \\( A \\) is Pareto optimal if, for any other assignment \\( B \\), there's at least one participant strictly preferring \\( A \\) over \\( B \\).</p> <p>Theorem: You Cannot Make Everyone Happier Without Making Someone Sadder</p> <p>Serial Dictatorship assignments are Pareto Optimal.</p> <p>Proof:</p> <p>Assume, for the sake of contradiction, that a different assignment exists making everyone as happy or happier.</p> <p>Consider the first student assigned differently: all better dorms are already assigned identically to students before them.</p> <p>Thus, the first differing student gets a worse dorm and becomes less happy\u2014we have reached a contradiction.</p> <p>Therefore, Serial Dictatorship is Pareto Optimal.</p>"},{"location":"lecture1/#additional-discussion","title":"Additional Discussion","text":"<p>Why is truthfulness good? It prevents corruption and ensures fairness by removing any insider advantage.</p> <p>When is truthfulness important? When optimizing happiness, truthful inputs ensure correct objectives.</p> <p>When is relaxing truthfulness acceptable?  In contexts with social norms where truthfulness isn't expected (e.g., poker games), strategyproofness isn't crucial.</p> <p>Fairness/equity issues with Serial Dictatorship: Fairness depends on sorting order. Seniority or randomness provide fairness ex-ante but not necessarily ex-post.</p> <p>Is there any other flaws in Serial Dictatorship? A dictator's seemingly insignificant decision may have huge impact on other agents. For instance, by the time my turn arrive, only my 9th and 10th choices are available. Since I am almost indifferent between them, I break ties and take my 9th choice. However, it is possible that my 9th choice was in fact someone else's top choice and they really wanted it. This may be even worse if their 2nd-10th choices happen to be already taken.</p>"},{"location":"lecture1/#recap","title":"Recap","text":"<ul> <li>Definition: Mechanism. A mechanism meaning soliciting inputs, running an algorithm, and taking actions.</li> <li>Definition: Strategyproof/truthful. A mechanism is strategyproof/truthful is misreporting preferences can never make a participant better off.</li> <li>Defintion: Pareto-optimal. An assignment \\( A \\) is Pareto-optimal if for any other assignment \\( B \\), there is a participan that (strictly) prefers \\( A \\) over \\( B \\).</li> </ul>"},{"location":"lecture2/","title":"Stable Two-Sided Matchings","text":"<p>April 2, 2025</p> <p>After medical school, med students start their internship called a \"residency.\" Each (prospective) doctor has preferences over hospitals, and each hospital has preferences over doctors. How should doctors and hospitals be matched?</p> <p>Key Nuances:</p> <ul> <li>The biggest difference between doctor-residency matching and student-dorm matching is that we now have two-sided preferences (each doctor and hospital have preferences over each other).</li> <li>Matching students to dorms is a centralized process, while matching doctors to hospitals requires incentivizing participants to use a centralized process to prevent side deals.</li> </ul>"},{"location":"lecture2/#stable-matching","title":"Stable Matching","text":"<p>Definition: Blocking Pair</p> <p>Given a match \\(M\\), the pair (doctor \\(i\\), hospital \\(j\\)) forms a blocking pair if they prefer each other to their current assignments in \\(M\\).</p> <p>For example, if Doctor \\(n\\) prefers Stanford over UCSF and Stanford prefers Doctor \\(n\\) over Doctor 1, who is currently matched, then (Doctor \\(n\\), Stanford) form a blocking pair. This results in an Unstable Matching.</p> <p>Note: Blocking pairs exist only in two-sided matching markets. In a 1-sided matching, we cannot have blocking pairs, because in a blocking pair, both participants need to prefer each other. In the Stanford Undergraduate Housing Problem, even though students have preferences over dorms, dorms do not have preferences over students.</p> <p>Definition: Stable Matching</p> <p>A matching \\(M\\) is stable if there are no blocking pairs. Equivalently, for every unmatched pair \\((i,j)\\), either:</p> <ul> <li>Doctor \\(i\\) prefers Hospital \\(M(i)\\) over Hospital \\(j\\), or;</li> <li>Hospital \\(j\\) prefers Doctor \\(M(j)\\) over Doctor \\(i\\).</li> </ul> <p>Key Point: Stability removes incentives to deviate from the centralized matching. For instance, if you have a stable matching between doctors and hospitals, then neither doctors nor hospitals have any incentive to deviate from the matching and match outside of the process. They might as well stay in the centralized matching, because they cannot get anything better outside of the matching process.</p>"},{"location":"lecture2/#deferred-acceptance","title":"Deferred Acceptance","text":"<p>The Deferred Acceptance Algorithm is an algorithm that finds stable matchings.</p> <p>Main idea: Each doctor proposes to their favorite hospital that hasn't rejected them yet. Hospitals accept the best available candidate. If we discover a blocking pair, we switch the matching.</p> <p>Mechanism: Deferred Acceptance</p> <p>While there's an unmatched doctor \\(i\\):</p> <ul> <li>Doctor \\(i\\) proposes to their next-favorite hospital \\(j\\).</li> <li>If hospital \\(j\\) has no match, they accept doctor \\(i\\).</li> <li>Else, if hospital \\(j\\) prefers their current match over doctor \\(i\\), doctor \\(i\\) remains unmatched.</li> <li>Else, hospital \\(j\\) matches with doctor \\(i\\), releasing their previous match.</li> </ul> <p>The algorithm stops when everyone is matched.</p> <p>Example</p> <p>Consider the following agents and their respective preferences:</p> <ul> <li>Doctors: \\(Alice (X, Y, Z)\\), \\(Bob (Y, X, Z)\\), \\(Charlie (Y, Z, X)\\).</li> <li>Hospitals: \\(X (Bob, Alice, Charlie)\\), \\(Y (Alice, Bob, Charlie)\\), \\(Z (Bob, Charlie, Alice)\\).</li> </ul> <p>Stable matching result: \\((Alice, X), (Bob, Y), (Charlie, Z)\\).</p> <p>Note: No matter the order chosen to process the doctors, we always find the stable matching. </p> <p>Theorem: Runtime of Deferred Acceptance</p> <p>Deferred Acceptance runs in \\(O(n^2)\\) time.</p> <p>This is important, because when inputs are of non-trivial size, we want to make sure the algorithm is efficient.</p> <p>Proof:</p> <ul> <li>There are at most \\(n^2\\) proposals (each doctor proposes at most to \\(n\\) hospitals).</li> <li>Each proposal is \\(O(1)\\).</li> <li>So, the total runtime is \\(O(n^2)\\).</li> </ul> <p>Why do we have at most \\(n^2\\) iterations? A doctor never proposes to the same hospital twice (we never try to match the same pair of doctor-hospital twice), because we are going down the list of preferences. So, if there are \\(n\\) doctors and \\(n\\) hospitals, then there are \\(n^2\\) possible doctor-hospital pairs. This is why we have at most \\(n^2\\) iterations (there are instances where it really takes \\(n^2\\) time).</p> <p>Why is each proposal \\(O(1)\\)? We assume that doctor \\(i\\) already has a ranked list of preferences (how we get the preferences is non-trivial but outside of this algorithm). On every iteration, all we do is advance to the next hospital in the list of a doctor\u2019s preferences (for instance, in the case of \\(Bob\\) above, we moved from Hospital \\(Y\\) to Hospital \\(X\\)). When a hospital needs to determine whether it prefers its current match \\(i'\\) over \\(i\\) (or not), it can use an array with the preferences of doctors (ranked): this way, it can compare the ranks in \\(O(1)\\) time.</p> <p>Theorem: Deferred Acceptance is Stable</p> <p>Given \\(n\\) doctors and \\(n\\) hospitals, Deferred Acceptance outputs a complete stable matching.</p> <p>Corollary: A stable matching exists.</p> <p>(This is not obvious!)</p> <p>Proof (outline):</p> <p>We prove that Deferred Acceptance outputs a complete stable matching with three claims:</p> <ol> <li>The current match is stable at each iteration.</li> <li>Once matched, hospitals remain matched.</li> <li>At completion, everyone is matched.</li> </ol> <p>Therefore, no blocking pairs exist.</p> <p>How does this proof work? Claim 1 states that we have already matched some doctors and hospitals, and we assume that the matching so far is stable. Claim 2 states tgat once a hospital is matched, it may be matched to another doctor, but it never gets unmatched (however, doctors can get unmatched). Claim 3 states that no doctor, and no hospital, is unmatched at the end of the matching. Since we know that the matching is stable after each iteration, and there is no one left unmatched at the end, then the matching at the end is stable.</p> <p>Proof of Claims:</p> <ul> <li>Claim 1: We assume that doctor \\(d\\) and hospital \\(h\\) are currently matched to other matches and they are a blocking pair. This means that \\(d\\) is matched to a worse hospital, so we must have tried to match \\(d\\) to \\(h\\), and we must have gone down the preference list, either because \\(h\\) refused to match to \\(d\\), or because \\(h\\) preferred another doctor. This means that \\(h\\) must have already matched with someone better than \\(d\\). This is a contradiction with the fact that \\((d,h)\\) is a blocking pair.</li> <li>Claim 2: If you look at the pseudo code of the algorithm, there is simply no command to unmatch a hospital.</li> <li>Claim 3: If we have a free doctor, then we need to have a free hospital as well. However, if we reach the end of the algorithm, \\(d\\) already tried to propose to \\(h\\), but after that step, \\(h\\) is matched (either because \\(d\\) matches with \\(h\\), or because \\(h\\) is already matched to another doctor). By Claim 2, we know that a hospital stays matched until the end of the algorithm, so \\(h\\) cannot be free. This is a contradiction to \\(h\\) being free at the end of the algorithm (we cannot have a \\((d,h)\\) pair free).</li> </ul>"},{"location":"lecture2/#deferred-acceptance-as-a-mechanism","title":"Deferred Acceptance as a Mechanism","text":"<p>We know that Deferred Acceptance always finds a stable matching. Is it optimal? What does optimality mean?</p> <p>Theorem: Efficiency of Stable Matchings</p> <p>Every stable matching is Pareto-optimal.</p> <p>Reminder: An assignment \\( A \\) is Pareto-optimal if for any other assignment \\( B \\), there is a participan that (strictly) prefers \\( A \\) over \\( B \\).</p> <p>Proof: Any deviation from a stable matching worsens at least one participant\u2019s outcome.</p> <p>Comment: If we take the stable matching, and any other matching (whether it is stable or not), there are going to be some doctors and/or some hospitals that prefer the stable matching over that other matching.</p> <p>Theorem: The matching returned by the Deferred Acceptance mechanism is doctor-optimal.</p> <p>In other words, every doctor is matched to their favorite hospital possible in any stable matching.</p> <p>If we have multiple stable matchings, which one is the best one? Doctor-optimality means that every doctor gets the best hospital they can possibly get out of any stable matching. When we talked about the student-dorms 1-sided matching, it was possible that some students were happier, while others were sadder, depending on the matching (we had some trade-offs). However, here, among all stable matchings, the matching that DA outputs is the best for every single doctor simultaneously. An implication of this theorem is that the Deferred Acceptance mechanism is also doctor-strategyproof.</p> <p>Corollary: Deferred Acceptance is doctor-strategyproof.</p> <p>Doctors cannot gain from misreporting their preferences.</p> <p>Theorem: The matching returned by the Deferred Acceptance mechanism is hospital-worst.</p> <p>In other words, every hospital is matched to their least favorite doctor possible in any stable matching.</p> <p>Corollary: Deferred Acceptance is NOT hospital-strategyproof.</p> <p>Hospitals can benefit from misreporting their preferences.</p> <p>Example</p> <p>Consider the following agents and their respective preferences:</p> <ul> <li>Doctors: \\(Alice (X, Y, Z)\\), \\(Bob (Y, X, Z)\\), \\(Charlie (Y, Z, X)\\).</li> <li>Hospitals: \\(X (Bob, Alice, Charlie)\\), \\(Y (Alice, Charlie, Bob)\\), \\(Z (Bob, Charlie, Alice)\\).</li> </ul> <p>Stable matching result: \\((Alice, Y), (Bob, X), (Charlie, Z)\\).</p> <p>By changing their preferences over \\(Bob\\) and \\(Charlie\\) (compared to the previous example setup), Hospital \\(Y\\) gets matched to \\(Alice\\) (their top choice).</p> <p>Question 1: Does a hospital need to know the preferences of the other hospitals over the doctors to game the system?</p> <p>Yes, Hospital \\(Y\\) needs to know the preferences of all other hospitals over all other doctors to game the system, otherwise, it is pretty hard to figure out how to game the system on the hospital side. Aviad does not know of a good strategy for hospitals to game the system.</p> <p>Question 2: If a doctor ranks their safety choices higher, does it help them?</p> <p>No. For instance, if a doctor prefers Stanford, but understands that their second favorite hospital may be less competitive (i.e. rank them higher as a doctor), it is still not in their best interest to rank that second hospital higher than Stanford in their preference list, because the DA is completely doctor-strategyproof.</p> <p>Question 3: Why is the DA mechanism hospital-worst?</p> <p>Because we are processing things in order of doctor preferences, and we are only using hospital preferences for tie-breaking purposes.</p> <p>Question 4: Does Pareto optimality change if we allow participants to have non-strict preferences?</p> <p>In practice, doctors do not rank all possible hospitals, and hospitals do not rank all possible doctors. The theorem statements do not hold exactly in practice (in particular for Pareto optimality if we have non-strict preferences).</p> <p>Question 5: As a doctor, if you know that a hospital is misrepresenting their preferences, is it still doctor-stragegyproof?</p> <p>From the doctor\u2019s perspective, the hospitals submit some ranked lists of doctors, but they don\u2019t know whether it truthful or not, so this is still doctor-stragegyproof. However, if a doctor can influence how a hospital reports their preferences, then they can game the system.</p>"},{"location":"lecture2/#deferred-acceptance-in-practice","title":"Deferred Acceptance in Practice","text":"<p>Why don\u2019t we use a DA mechanism for undergrad admissions in the US? </p> <p>In the US, the system is very decentralized, and this is a challenge for schools, because they need to admit a number of students without knowing how many are actually going to enroll\u2014unlike hospitals, which generally have 1 or 2 residency positions available, so if none shows up, they don\u2019t have a doctor, and if 5 show up, they cannot pay everyone\u2014and these are generally large numbers.</p> <p>Also in the US, applications are holistic, which costs money, so application fees limit the number of universities students apply to.</p> <p>In contrast, in other countries where standardized tests scores are used instead of holistic reviews, DA mechanisms can be used (such as in Brazil, China, Hungary).</p> <p>How do doctors rank hospitals?</p> <ul> <li>1950s: DA-like algorithms initially used. At the time, there were very few female doctors (not to mention openly gay doctors).</li> <li>1960s: Couples complicate preferences. More couples want to be near each other, i.e. doctors no longer have a ranked preference over hospitals.</li> <li>1980s: Negative theory results on stable matching with couples. A stable matching may not exist: it is possible that there is no stable matching when you try to match couples to the same hospitals in the same cities. Deciding if a stable matching exists is an NP-complete computational problem (it is, in theory, intractable).</li> <li>1990s: Extended DA for couples adopted.</li> </ul> <p>How do hospitals rank doctors?</p> <ul> <li>Interviews (costly process): hospitals strategize over which doctors to interview (\"safety choices\").</li> <li>Standardized Tests (probably not any more: USMLE switched to pass/fail in January 2022).</li> <li>Letters of recommendation, medical school evaluation, personal statement, CV.</li> </ul> <p>Theorem: In DA, using safety choices is never safer for hospitals.</p> <p>Formally, manipulating true ranks to move \\(i-th\\) doctor (\"safety choice\") to a higher position cannot help a hospital match with a doctor of rank \\(i-or-better\\).</p> <p>Hospitals also use \"safety choices\" after interviews: they may not rank first the candidates that they evaluate as \u201ctoo good to come here.\u201d This is particularly interesting since we have seen in doctor-proposing DA that ranking safety choices higher is never safer for hospitals (this is provable).</p> <p>Proof:</p> <ul> <li>Until doctor \\(i\\) tries to match with a hospital, manipulation has no effect.</li> <li>After doctor \\(i\\) tries to match with a hospital, they can only be replaced by a better doctor.</li> </ul> <p>Yet, hospitals still strategically use \"safety choices\" when ranking doctors post interview. Why?</p> <ul> <li>One possible explanation is ignorance of matching mechanisms and their properties (they did not take CS269I).</li> <li>Another possible explanation is that they consider their reputation/ego. after the matching is complete, the organization that handles the matching publishes the \u201cnumber needed to fill\u201d metric, which is the lowest ranking a hospital had to go to fill their positions. This is helpful for doctors the following year to get a sense of how competitive a program is, which is therefore also a signal of how prestigious a program is. For instance, Stanford probably don\u2019t have to be turned down by many doctors, so their number is low, while less prestigious hospitals may need to go through more doctors and have a higher number. If a hospital ranks higher doctors that they think will join them, they can reduce their \u201cnumber needed to fill\u201d metric and appear more prestigious.</li> </ul> <p>Other DA Applications:</p> <ul> <li>Routing Network Packets: Suppose that instead of doctors and hospitals, you want to match packets to servers on the internet. When you own all the servers, you don't have to worry about them matching outside of your algorithm. This would apply to a company like Akamai, who owns a lot of servers\u2014and it actually works better in practice than in theory (because DA is very fact in practice). Packets typically get one of the top servers, so preferences lists are truncated, and the total running time is closer to \\(O(n)\\). Given the highly distributed nature of packet routing, every packet looks for its own server. </li> <li>Stanford Marriage Pact: Matches are made between Stanford students who want to make a pact, i.e. \"If we don't get married by time X, we will marry each other.\" DA is not used anymore (they use something closer to max weight matching instead) because in the 21st century, the graph is not bipartite, due to people having many different preferences. Stability is not a very important part of the requirement, because in practice, Stanford students spend their time looking for a partner outside of the pact/matching.</li> </ul>"},{"location":"lecture2/#recap","title":"Recap","text":"<p>Recap:</p> <ul> <li>In theory: DA in theory is Pareto-optimal among all matchings, doctor-optimal and hospital-worst among stable matchings, and doctor-strategyproof but not not hospital-strategyproof.</li> <li>In practice: DA is expensive (collecting preferences is costly, e.g. holistic admissions in US colleges and interview in hospitals) and preferences may not be captured by the model (e.g. matching couples).</li> </ul>"},{"location":"lecture3/","title":"Online Learning and Regret Minimization","text":"<p>April 9, 2025</p> <p>So far in this class, agents had dominant strategies, such as doctor reporting their true preferences in DA, so we had a good guess of what they should do. However, what should agents do without dominant strategies or even knowledge of the game's rules? In other words, what should selfish agents do when the mechanism is not strategyproof, and possibly not fully specified? Let's explore this question from a single agent's perspective.</p>"},{"location":"lecture3/#regret-minimization","title":"Regret Minimization","text":"<p>If we know which stocks are going to go up and down, we should buy the stocks that are going to go up and sell them before they are going to go down. But what do we do when we don\u2019t know that? We want an algorithm that will explain how to invest well in the stock market First, we will try to use historical performance of stocks to determine what to do tomorrow.</p> <p>Scenario 1: Consider investing in stocks without knowledge of future performance:</p> <ul> <li>There are \\(n\\) possible actions (stocks).</li> <li>We run an algorithm over \\(T\\) days.</li> <li>On day \\(t\\), the algorithm picks action \\(ALG(t)\\).</li> <li>Reward for action \\(i\\) on day \\(t\\) is \\(r_{i,t}\\), bounded by \\(-1 \\leq r_{i,t} \\leq 1\\).</li> </ul> <p>Our goal is to maximize the sum of the rewards of the actions the algorithm chooses, namely \\(\\sum r_{ALG(t),t}\\).</p> <p>Key Idea #1: Use historical performance to inform decisions.</p> <p>A pretty natural guess is to try a greedy algorithm, called Follow The Leader, where on every day, we just take the action that has generated the most reward so far.</p> <p>Algorithm: Follow The Leader (FTL)</p> <p>On day \\(t\\): Take the action with the highest total reward up to day \\(t-1\\). Break ties arbitrarily.</p> <p>How can we reason about whether this first idea is a good idea or not? We measure the success of online learning algorithms in terms of regret. Our benchmark is a fixed action over time: we are comparing with the best stock overall, not the best stock every single day. The intuition is that, if the algorithm has low regret, then we are doing almost as well as the best stock.</p> <p>Definition: External Regret</p> <p>External Regret = \\( \\underset{i}{\\max} \\underset{t}{\\sum} r_{i,t} - \\underset{t}{\\sum} r_{ALG(t),t} \\)</p> <p>Question: Can regret be negative? Yes! That is a fantastic case, when we are doing better than the best stock.</p> <p>So, is FTL (i.e. the greedy algorithm that takes the best action every time) a good idea?</p> <p>Claim 1: For independently, identically distributed (iid) rewards, FTL's expected regret is \\(O(\\sqrt{T \\log(n)})\\).</p> <p>In other words, if, for each action, the rewards on different days are independently, identically distributed, aka iid (i.e. every day, the rewards are drawn from the same distribution\u2014some stocks tend to do better, some stocks tend to do worse), then FTL has an expected regret in the order of \\(O(\\sqrt{T \\log(n)})\\).</p> <p>What does this mean? It means that, as long as \\(T\\) is much bigger than \\(\\log(n)\\), the whole thing inside the square root is going to be less than \\(T^2\\) (because \\(\\log(n)\\) is less than \\(T\\), so \\(T \\log(n)\\) is less than \\(T \\cdot T\\), so the whole thing is less than \\(T^2\\).</p> <p>Regret is less than 1 on average, so regret is less than the number of days, and regret per day is diminishing (going to 0).</p> <p>For instance, the NYSE only lists 2,000 companies, and \\(\\log(2000) \\approx 11\\), so it takes about \\(11\\) days to start doing as well as the best stock on the NYSE (we can replace \\(11\\) days with \\(11\\) seconds if we are trading really fast).</p> <p>Claim 2: No algorithm can outperform \\(\\Omega(\\sqrt{T \\log(n)})\\) with iid rewards.</p> <p>Why is this claim true?</p> <p>Let\u2019s assume that the reward for every action, every day, is just a random coin flip (\\(+1\\) or \\(-1\\)), completely independently at random. So, in any algorithm based on history, the next choice is going to be a random coin flip.</p> <p>However, if we have \\(n\\) actions, one of them is going to do a little bit better than average (each one has a \\(50/50\\) expectation, but one of them is going to be a little bit better than average). If we do the math of how much better than average it is going to be, it comes out to roughly this magic number of \\(O(\\sqrt{T \\log(n)})\\).</p> <p>The number \\(O(\\sqrt{T \\log(n)})\\) itself is not super important. However, the intuition is that it is the right bound: \\(O(\\sqrt{T \\log(n)})\\) converges very fast, and as long as \\(T\\) is much bigger than \\(\\log(n)\\). Since \\(\\log(n)\\) is a tiny number, the algorithm is doing really well.</p> <p>Key Idea #2: External regret is the difference between the algorithm's total reward and the reward from the single best-in-hindsight action, i.e. </p> <p>External Regret = \\( \\underset{i}{\\max} \\underset{t}{\\sum} r_{i,t} - \\underset{t}{\\sum} r_{ALG(t),t} \\).</p>"},{"location":"lecture3/#regret-minimization-algorithms","title":"Regret Minimization Algorithms","text":"<p>So, we are doing almost as well as the best stock. The catch is that the stock market is not iid: the stocks may go up one day, two days, three days, but they can only go up so much.</p> <p>In an adversarial context, it is as if someone is trying to make it hard for us. FTL does not do well with adversarial input. In fact, no deterministic algorithm does well with adversarial input.</p> <p>Key Idea #3: To minimize regret in an adversarial context, we introduce randomness in the algorithm.</p> <p>However, choosing actions uniformly at random may also be a bad idea, because some actions are consistently worse than others.</p> <p>Key Idea #4: We need an algorithm with a good balance between having enough randomness and paying attention to historical performance.</p> <p>One idea to minimize regret is to pick a distribution of actions rather than a pure action.</p> <p>Algorithm: Follow The Regularized Leader (FTRL)</p> <p>On day \\(t\\), choose distribution \\(x\\) maximizing \\(\\underset{t}{\\sum} (r_{x,t} - \\frac{1}{\\eta}\\varphi(x))\\) where:</p> <ul> <li>\\(r_{x,t}\\) provides the historical performance,</li> <li>\\(\\eta\\) balances randomness and history,</li> <li>\\(\\varphi\\) is the regularizer, which penalizes unbalanced distributions.</li> </ul> <p>How does \\(\\varphi\\) work? \\(\\varphi\\) is a (usually convex) function that \u201cpenalizes\u201d unbalanced distributions. For instance, if we have a distribution that puts all the weight on one stock, then \\(\\varphi\\) is going to determine that it is a bad distribution and give it a bad score. We can pick different functions that are going to give different performance.</p> <p>Now, let's consider another famous algorithm.</p> <p>Algorithm: Multiplicative Weight Update (MWU)</p> <ul> <li>Initialize weights \\(z_{i,0} = 1\\) for all \\(i\\).</li> <li>At day \\(t\\):<ul> <li>Choose action \\(i\\) with probability \\(\\frac{z_{i,t}}{\\underset{j}{\\sum} z_{j,t}}\\).</li> <li>Update weights: \\(z_{i,t+1} \\leftarrow z_{i,t} e^{\\eta r_{i,t}}\\).</li> </ul> </li> </ul> <p>The idea is that, at the beginning, all the weights are going to be 1. Each day, we are going to choose an action with a probability proportional to the weight. Then, there are updates: we multiply the weight of each action by the reward the the action got. If an action got a big positive reward, we are multiplying its weight by a big number, and its weight next time is going to be bigger. If an action got a small or negative reward, we are multiplying its weight by a small number, and its weight next time is going to be smaller.</p> <p>\\(\\eta\\) is called \u201clearning rate\u201d. It is a parameter that balances between randomness and FTRL:</p> <ul> <li>If we have a higher \\(\\eta\\), it is putting a lot of weight into learning really fast (fitting to historical performance).</li> <li>If we have a smaller \\(\\eta\\), we are staying very close for a long time to the original distribution, which is uniform over all actions, so it is more random.</li> </ul> <p>Although we are not going to prove this claim in lecture, it turns out that this Multiplicative weight update (MWU) algorithm is the same as FTL when using the entropy regularizer.</p> <p>Claim: MWU = FTRL with entropy regularizer \\(\\varphi(x) = - \\sum x_i \\log(x_i) \\).</p> <p>The point is that these are two ways to look at the same algorithm, trying to balance randomness and historical performance to avoid an adversarial example.</p> <p>Theorem: MWU and FTRL achieve expected regret \\(O(\\sqrt{T \\log(n)})\\).</p> <p>This is optimal, even against adversarial input. As long as the adversary does not know the inside randomness of the algorithm, this algorithm is completely adversary proof.</p>"},{"location":"lecture3/#swap-regret-minimization-algorithms","title":"Swap-Regret Minimization Algorithms","text":"<p>So far, we have formalized the idea of measuring online algorithms with regret, and we have seen actual optimal algorithms that minimize regret even against very adversarial input. However, what happens if there is an adversary who can look at our algorithm\u2019s choices, and use that to do better than us? This is embarrassing: how can avoid that?</p> <p>In Judo, winning is not about our own strength, but about using our opponent\u2019s strength to make them fall over. Similarly, in the stock market, winning is not about knowing the market ourselves, but instead using oour opponent\u2019s power to beat them.</p> <p>Definition: Swap-Regret (Internal Regret)</p> <p>Swap-Regret = \\( \\underset{\\Phi : [n] \\to [n]}{\\max} \\underset{t}{\\sum} (r_{\\Phi(ALG(t)),t} - r_{ALG(t),t}) \\) where \\(\\Phi\\) maps each action to another action.</p> <p>In the context of external regret, our algorithm was just competing with the best single action (i.e. the best stock). In the context of swap regret, our algorithm has to compete with a friend, who is seeing what our algorithm is recommending, and using that to do something else.</p> <p>How is the swap function determined? We are taking the best of all possible swap functions?</p> <p>Between external regret and swap regret, which one is higher? The swap regret is always at least as high as the external regret.</p> <p>Which kind of regret is it better to minimize? It is better to minimize swap regret.</p> <p>One way to see this is that we can have everything mapping to the same action (i.e. the best action in hindsight) in the swap function. So, if we can minimize the swap regret, it is better, although it is also much harder because the benchmark is more complex.</p> <p>Why don\u2019t we use the best possible scenario to calculate regret? Indeed, another benchmark we can use is the best action each day (rather than the best action overall). The benchmark is just too high. For instance, if everyday we are playing a game where we flip a coin, and we need to determine whether it is going to be head or tail, there is no good algorithm for this, it is helpless.</p> <p>We saw that, for external regret, there is an algorithm that can do as well as the best stock, even with adversarial input. It turns out that with swap regret, there is also a pretty good algorithm.</p> <p>Algorithm: Swap-Regret Minimization</p> <ul> <li>Define meta-actions for each possible swap choice \\(\\Phi\\).</li> <li>Run MWU/FTRL on meta-actions.</li> <li>At each iteration, solve for distribution \\(x_t = \\mathbb{E}[\\Phi(x_t)]\\).</li> </ul> <p>Claim: The Total Swap-Regret is in the order of \\(O(\\sqrt{T n \\log(n)})\\)).</p> <p>The total swap regret looks like the external regret, except that instead of \\(n\\) action, we have to take the \\(\\log\\) of \\(n^n\\) actions. This is a higher regret, so it is going to take more time to achieve vanishing regret.</p> <p>For instance, if we think about the NYSE, with 2,000 actions, instead of \\(\\log(2000) \\approx 11\\), it is going to be \\(\\log(2000^{2000})\\), which is \\(\\log(2000) \\cdot 2000 \\approx 11 \\cdot 2000 \\approx 22000\\).</p> <p>So, for instance, if \\(T\\) is in days, it is going to take \\(60\\) years for the swap to beat the entire stock exchange, but if we are talking in seconds, then \\(22000\\) seconds is not very long (about \\(6\\) hours).</p>"},{"location":"lecture3/#regret-minimization-with-bandit-feedback","title":"Regret Minimization with Bandit Feedback","text":"<p>Scenario 2: Consider a new portal/feed editor:</p> <ul> <li>We run an algorithm over \\(T\\) days.</li> <li>There are \\(n\\) possible \"arms\" (i.e. actions), where an \"arm\" is a news category or a reporter, for instance.</li> <li>On the \\(t\\)-th day, a user comes, and we can show them one news item.</li> <li>The reward \\(r_{i,t}\\) measures how long the user engaged with the news item.</li> </ul> <p>Our goal is to minimize regret, namely \\(\\underset{i}{max} \\underset{t}{\\sum}(r_{i,t} - r_{ALG(t),t})\\).</p> <p>Challenge: With partial (bandit) feedback, we see how long a user engaged with what we showed them, but we don\u2019t know long the user would have engaged with the content we did not show them. Yet, we still have to compete with all the other actions.</p> <p>Solution: We need to arbitrate how much we want to learn about how each arm is doing (i.e. exploration) and how much we want to extract reward from the best arm (i.e. exploitation).</p> <p>Key Idea #5: To account for bandit/partial feedback, we may apply a similar regret minimization framework, with a trade-off between exploration and exploitation.</p> <p>Mechanism: Multi-Armed Bandit Model Warm-Up Algorithm</p> <ul> <li>Try all possible arms to see which one yields the best reward.</li> <li>Keep pulling on the best arm identified.</li> </ul> <p>This approach works well if rewards are fixed (e.g., one arm consistently rewards highly). However, it struggles with randomness, leading to potential overfitting.</p> <p>How much exploration is needed? By the Law of Large Numbers, the average reward converges to the expectation over time. Concentration Inequalities quantify this convergence rate.</p> <p>Using the Hoeffding Inequality:</p> \\[ Pr\\left[\\text{expectation} &gt; \\text{average of samples} + \\sqrt{\\frac{X}{\\text{number of samples}}}\\right] &lt; e^{-2X} \\] <p>To ensure confidence across \\(T\\) iterations:</p> \\[ Pr\\left[\\text{expectation} &gt; \\text{average of samples} + \\sqrt{\\frac{2\\ln(T)}{\\text{number of samples}}}\\right] &lt; \\frac{1}{T^4} \\] <p>By setting \\(X = 2\\ln(T)\\), we derive an Upper Confidence Bound (UCB). This ensures minimal mistakes over the algorithm's runtime.</p> <p>Mechanism: UCB1 Algorithm</p> <p>Let \\(\\text{UCB}_i = \\text{(average of $i$'s samples)} + \\sqrt{\\frac{2\\ln(T)}{\\text{number of $i$'s samples}}}\\).</p> <p>On day \\(t\\):</p> <ul> <li>Compute UCB for each arm.</li> <li>Pull the arm with the maximum UCB (breaking ties randomly).</li> </ul> <p>This greedy algorithm balances exploration and exploitation:</p> <ul> <li>Under-explored arms (few samples) have a high exploration incentive.</li> <li>High-performing arms (high averages) encourage exploitation.</li> </ul> <p>The expected regret under iid rewards (between -1 and 1) is in the order of \\(O(\\sqrt{nT\\log(T)})\\) as long as \\(T &gt; n\\log(n)\\), which is not as good as full feedback scenarios remains but reasonable for bandit feedback.</p> <p>In other words:</p> <ul> <li>Good: Regret per day approaches 0 if \\(T &gt; n\\log(n)\\).</li> <li>Bad: Not as optimal as full feedback.</li> <li>Ugly: Requires iid rewards.</li> </ul> <p>Caveat: Because UCB1 is a greedy, deterministic algorithm (with no randomness), we can construct an adversarial input.</p> <p>Let's explore another algorithm that works for bandit feedback AND against an adversarial input. The idea is to use MWU again. The challenge is that in MWU, every time we want to update the weights of the actions, we need to know the rewards, but we don\u2019t have the rewards. Instead, we are going to feed the algorithm pseudo-rewards, something that is going to replace the reward and hopefully work as well as the reward.</p> <p>Mechanism: Exp3 Algorithm</p> <p>On day \\(t\\):</p> <ul> <li>MWU selects an arm.</li> <li>Define pseudo-rewards:<ul> <li>If arm \\(i\\) is not selected: \\(\\hat{r}_{i,t} = 0\\).</li> <li>If arm \\(i\\) is selected: \\(\\hat{r}_{i,t} = \\frac{r_{i,t}}{Pr[\\text{selecting } i]}\\).</li> </ul> </li> </ul> <p>Pseudo-reward rationale:</p> <ul> <li>Unselected actions yield zero (unknown) rewards.</li> <li>Selected actions are scaled to compensate selection probability, ensuring unbiased estimates (Inverse Propensity Score).</li> </ul> <p>Key Idea #6: The Inverse Propensity Score means that \\(\\hat{r}_{i,t}\\) is an unbiased estimator, i.e. \\(\\mathbb{E}[\\hat{r}_{i,t}] = \\mathbb{E}[{r}_{i,t}]\\).</p> <p>The expected regret for Exp3 is \\(O(\\sqrt{nT\\log(n)})\\), slightly worse than MWU with full feedback (i.e. \\(O(\\sqrt{T\\log(n)})\\), though better algorithms exist to reduce variance.</p>"},{"location":"lecture3/#recap","title":"Recap","text":"<p>Key Ideas</p> <ul> <li>Key Idea #1: Use historical performance to forecast the future.</li> <li>Key Idea #2: Use regret to measure the success of algorithms.</li> <li>Key Idea #3: Use randomness as safety against adversarial inputs.</li> <li>Key Idea #4: Balance randomness and optimization.</li> <li>Key Idea #5: Balance exploration and exploitation.</li> <li>Key Idea #6: Use unbiased estimates of rewards.</li> </ul>"},{"location":"lecture3b/","title":"Top Trading Cycles","text":"<p>January 18, 2023</p> <p>Note: This lecture was skipped during the Spring 2025 quarter. Below are notes from the Winter 2023 quarter.</p> <p>Previously, we studied:</p> <ul> <li>Random Serial Dictatorship (one-sided matching)</li> <li>Deferred Acceptance (two-sided matching)</li> </ul> <p>This lecture: What happens when participants are already endowed with goods?</p> <p>Example: Stanford PhD housing renewal\u2014students face tradeoffs between renewal and lottery entry.</p> <p>Problem Setup:</p> <ul> <li>\\(n\\) students, \\(n\\) rooms</li> <li>Each student has a current room and preference over all rooms</li> </ul> <p>Mechanism: Top Trading Cycles (TTC)</p> <p>While there are unmatched students/rooms:</p> <ol> <li>Create a graph with unmatched rooms and students as nodes.</li> <li>Draw edges:</li> <li>from each room to its current owner.</li> <li>from each student to their most preferred available room.</li> <li>Identify cycles, remove them, and execute the trades indicated by cycles.</li> </ol> <p>Theorem: Runtime of TTC</p> <p>Top Trading Cycles runs in \\(O(n^2)\\) time.</p> <p>Proof: Constructing graph and edges is \\(O(n)\\). Finding cycles via directed edges visits at most \\(2n+1\\) nodes (pigeonhole principle), thus \\(O(n)\\). Each iteration removes at least one student; thus, overall complexity is \\(O(n^2)\\).</p> <p>Input size is \\(n^2\\) (preference lists of length \\(n\\)), so TTC is linear relative to input size.</p> <p>Definition: Individual Rationality (IR)</p> <p>A mechanism is individually rational if no agent is worse off after participating.</p> <p>Theorem: TTC is Individually Rational</p> <p>Proof: Students trade rooms only if they prefer the new room.</p> <p>Theorem: Strategyproofness of TTC</p> <p>Top Trading Cycles is strategyproof.</p> <p>This proof was noted as flawed; a corrected proof was expected.</p> <p>Theorem: Efficiency of TTC</p> <p>TTC allocation is Pareto-optimal.</p> <p>Proof: Equivalent to serial dictatorship in order cycles formed. Serial dictatorship is Pareto-optimal, thus TTC is Pareto-optimal.</p>"},{"location":"lecture3b/#top-trading-cycles-and-chains-ttcc","title":"Top Trading Cycles and Chains (TTCC)","text":"<p>Graduating students and new students (without rooms) require modifications:</p> <p>Mechanism: Top Trading Cycles with Chains (TTCC)</p> <ol> <li>Process students in random order.</li> <li> <p>Mark student \\(i\\) visited:</p> </li> <li> <p>If \\(i\\)'s top room is unoccupied, assign room, release previous.</p> </li> <li>If room occupied by unvisited student \\(j\\), move \\(j\\) ahead of \\(i\\).</li> <li>If room occupied by visited student \\(j\\), cycle identified.</li> </ol> <p>TTCC maintains strategyproofness, Pareto-optimality, individual rationality, and efficiency.</p>"},{"location":"lecture3b/#ttcc-in-practice","title":"TTC(C) in Practice","text":"<p>Why isn't TTCC common?</p> <ul> <li>Running TTCC repeatedly can break strategyproofness over multiple years.</li> <li>Increased strategic behavior over \"popular\" rooms.</li> </ul> <p>Application: School choice (New Orleans, 2011-12) used TTC but switched to DA (Deferred Acceptance) due to simplicity in explanation.</p> <p>College Admissions: Universities prefer specific applicants; direct trades don't apply.</p> <p>Kidney Transplants: Kidney exchange involves patient-donor compatibility issues and logistical constraints.</p> <p>Considerations in Kidney Exchange:</p> <ul> <li>Strategyproofness less critical (compatibility-based).</li> <li>Priority considerations (health urgency).</li> <li>Stability and central mechanism incentivization.</li> <li>Logistical challenge: Long cycles require simultaneous transplants; chains manageable.</li> <li>Dynamic arrivals/departures.</li> <li>Strategic hospitals may internally match.</li> <li>High failure probability (93% matches fail).</li> <li>Ethical issues and multi-organ exchange possibilities.</li> </ul>"},{"location":"lecture4/","title":"Equilibria in Games","text":"<p>April 14, 2025</p> <p>Last time, we explored what selfish agents should do when the mechanism is not strategyproof, and possibly not fully specified, from a single agent's perspective. Now, let's analyze systems with multiple agents. This serves as an introduction to game theory.</p> <p>Example: The Penalty Kick Game</p> <p>Consider the scenario of a penalty kick between Messi (kicker) and Lloris (goalie), where the probability of goal is defined as follows:</p> Kick Left Kick Right Jump Left 0.5 0.8 Jump Right 0.9 0.2 <p>Questions:</p> <ul> <li>Where should Messi kick?</li> <li>Where should Lloris jump?</li> </ul> <p>Lloris does not know which direction Messi is going to kick\u2014and needs to think about it. Messi does not know which direction Lloris is going to jump\u2014and needs to think about it.</p> <p>The answer is not obvious, but Game Theory tells us:</p> <ul> <li>Messi should kick left with probability 0.6 and right with probability 0.4.</li> <li>Lloris should jump left with probability 0.7 and right with probability 0.3.</li> <li>The probability of goal is 0.62.</li> </ul> <p>More specifically:</p> <ul> <li>When Messi kicks left, there is a \\(0.5 \\cdot 0.7 + 0.9 \\cdot 0.3 = 0.62\\)  probability of goal.</li> <li>When Messi kicks right, there is a \\(0.8 \\cdot 0.7 + 0.2 \\cdot 0.3 = 0.62\\) probability of goal, too.</li> <li>When Lloris jumps left, there is a \\(0.5 \\cdot 06 + 0.8 \\cdot 0.4 = 0.62\\) probability of goal.</li> <li>When Lloris jumps right, there is a \\(0.9 \\cdot 0.6 + 0.2 \\cdot 0.4 = 0.62\\) probability of goal, too.</li> </ul> <p>This is called a Nash Equilibrium:</p> <ul> <li>If Lloris jumps with these probabilities, Messi is indifferent when kicking left or kicking right.</li> <li>If Mess kicks with these probabilities, Lloris is indifferent when jumping left or jumping right.</li> <li>When both are kicking/jumping with these probabilities, neither has an incentive to deviate from these probabilities.</li> </ul> <p>Definition: Nash Equilibrium</p> <p>A pair of (possibly randomized) strategies, such that no player has an incentive to deviate. </p> <p>Question: How do we come up with these probabilities?</p> <p>There is an algorithm that comes up with those.</p> <p>Question: Do Messi and Lloris know each other\u2019s probabilities?</p> <p>No, they don\u2019t. They are deciding in the moment.</p> <p>Question: Are both Messi and Lloris\u2019s decisions happening at the same time or is one reacting to the other?</p> <p>Lloris does not have time to see what Messi does, so both are happening at the same time.</p>"},{"location":"lecture4/#equilibrium-in-2-player-zero-sum-games","title":"Equilibrium in 2-Player Zero-Sum Games","text":"<p>Assumptions: - There are 2 players (Lloris and Messi). - This is a Zero-sum game, meaning that their incentives are perfectly misaligned: what Lloris wants is exactly the opposite of what Messi wants.</p> <p>Messi wants to optimize the probability of goal, i.e. he wants to maximize over all his possible kick strategies, assuming the worst possible jumping strategy for Lloris:</p> \\[   Messi's \\; opt: \\underset{kick-stratregy}{max} \\underset{jump-stratregy}{min} Pr[goal] \\] <p>Lloris has the opposite optimization problem: he wants to figure out how to jump, he knows Messi is going to kick in the direction that makes it as hard as possible for him, and he wants to minimize the probability of goal:</p> \\[   Lloris's \\; opt: \\underset{jump-stratregy}{min} \\underset{kick-stratregy}{max}  Pr[goal] \\] <p>Theorem: Messi\u2019s max-min = Nash equilibrium = Lloris\u2019s min-max.</p> <p>In other words: In a 2-player zero-sum game, even with more than 2 strategies per player, Messi\u2019s max-min optimization problem is the same as the Nash Equilibrium (which tells us that the Nash Equilibrium is unique), which is going to be the same as Lloris\u2019 min-max optimization problem.</p> <p>Where did these probabilities come from?</p> <p>One way to compute them is to use Linear Programming (LP), which generalizes the \\(s-t \\; Max-Flow\\), which is the same as the \\(s-t \\; Min-Cut\\), and this duality is the same reason why Messi\u2019s problem is equivalent to Lloris\u2019s problem. This gives us an efficient algorithm to compute these probabilities.</p>"},{"location":"lecture4/#mini-case-study-poker-algorithms","title":"Mini Case Study: Poker Algorithms","text":"<p>Case Study: \"Heads Up\" (2-Player) Poker</p> <p>Rough rules of Poker:</p> <ul> <li>Every player is dealt 2 cards (private information)</li> <li>Players bet during betting round</li> <li>There are some open cards on the table (public information)</li> <li>We determine which player's private cards are a better match to the cards on the table</li> </ul> <p>Can we use Linear Programming to compute the Nash Equilibrium strategy?</p> <p>Linear Programming is fast, but Poker is a very large game. How large? Large enough that someone write a research paper about it\u2014actually, about an algorithm for estimating how large it is.</p> <p>In fact, there 56 trillion possible card combinations. However, this is not the only thing to consider: we also need to take into account how the other player is betting. Then, the number of possible combinations of what could happen becomes \\(10^{160}\\). Efficient Linear Programming algorithms are not fast enough.</p> <p>Poker is an extensive-form game, because there are turns. In game theory, an extensive-form game represents\u00a0a strategic interaction where players make decisions sequentially, and the order of moves is explicitly modeled using a game tree.</p> <p>Definitions: Game Theory Terminology Overload</p> <ul> <li>Game Node: State of the game (i.e. all cards dealt, and all bets made, so far)</li> <li>Game Tree: Graph representing which game nodes are reachable from which game node (i.e. there is an edge if we can go from \\(A\\) to \\(B\\) by calling another bet)</li> <li>Information Set: All the game nodes consistent with a player's information.</li> </ul> <p>The most important part is the Information Set, which represents all the games nodes consistent with a player\u2019s information, i.e. there are different states of the game that correspond to different cards that other players could be holding.</p> <p>Why should we worry about all the possible game states? Why can't we just solve the current state/information ste in real time?</p> <p>What a player wants to do at a given state of the the game depends on what they know (what they see), but it also depends on what the other players are going to do, which in turns depends on what they think that original player is going to do, and what they think that player is going to do depends on the cards they have now, but also what they might do with other cards.</p> <p>Algorithmic Insight #1: Use the Blueprint strategy.</p> <ol> <li>Blueprint solves a corase approximation of Poker (\"only\" \\(10^13\\) states, i.e. \\(50TB\\) to store 1 strategy).</li> <li>During live play: we can use Blueprint to solve the actual information set, i.e. to estimate what the other player thinks we would do if we had their informations set.</li> </ol> <p>Algorithmic Insight #2: Use regret-minimizing algorithms, e.g., MWU/FTRL.</p> <p>Theorem: If two players play a zero-sum game, and both use a regret-minimization algorithm, they will converge towards a Nash equilibrium.</p> <p>Case Study: \"Heads Up\" (2-Player) Poker (Cont'd)</p> <p>In 2017, CMU's Poker bot beat top human Poker players for the first time.</p> <p>Recap: 2-Player 0-Sum Game</p> <p>At Nash equilibirum:</p> <ul> <li>Both players choose actions randomly</li> <li>Neither player can gain from changing distribution</li> <li>Theorem: max-min = Nash equilibrium = min-max</li> </ul> <p>Algorithms for computing Nash equilibrium:</p> <ul> <li>Linear programming</li> <li>Regret minimization</li> </ul>"},{"location":"lecture4/#nash-equilibrium-in-non-zero-sum-games","title":"Nash Equilibrium in Non-Zero-Sum Games","text":"<p>The Penalty Kick Game, Revisited</p> <p>Lloris's incentives:</p> <ul> <li>Lloris wants to win the World Cup (he already did in 2018).</li> <li>Lloris also wants to be the goalie who stopped Messi's penalty kick (to earn more fame and sponsorship opportunities).</li> </ul> Pr[goal] Messi kicks Left Messi kicks Right Lloris jumps Left 0.5 0.8 Lloris jumps Right 0.9 0.2 Pr[save] Messi kicks Left Messi kicks Right Lloris jumps Left 0.4 0 Lloris jumps Right 0 0.6 <p>Therefore: \\(Lloris's utility = Pr[save] - Pr[goal]\\).</p> U_{Messi} Messi kicks Left Messi kicks Right Lloris jumps Left 0.5 0.8 Lloris jumps Right 0.9 0.2 U_{Lloris} Messi kicks Left Messi kicks Right Lloris jumps Left -0.1 -0.8 Lloris jumps Right -0.9 0.4 <p>This is what Lloris is really maximizing (while Messi is only optimizing the probability of goal).</p> <p>Where should Messi kick now? Where should Lloris jump now?</p> <p>It is no longer a zero-sum game, because the incentives of Messi and Lloris are neither perfectly aligned not perfectly misaligned. Messi does not care whether he misses or Lloris saves, but Lloris does (he would rather save).</p> <p>How should we model this question?</p> <p>We can use a Nash equilibrium, which always exists in a finite game, even if it is not a zero-sum game.</p> <p>Theorem: Nash's Existence Theorem (1951)</p> <p>Every finite game has at least one Nash equilibrium (possibly mixed strategies). If players play the Nash equilibrium, neither wants to deviate.</p> <p>Caveats: The Nash equilibrium...</p> <ul> <li>... is no longer unique.</li> <li>... is no longer equal to the max-min and the min-max.</li> <li>... is not approached by Regret Minimization.</li> <li>... is intractable to compute, even approximately.</li> <li>... sometimes does not make sense (see below).</li> </ul> <p>Example: The CS269I Grade Game</p> <p>You and your partner submitted a wonderful project, but the instructor is not sure how much each of you contributed to it. So, they will assign your grades using the following game:</p> <ol> <li>You send the instructor \\(x \\in \\{2, ..., 99\\}\\), and your partner sends the instructor \\(y \\in \\{2, ..., 99\\}\\).</li> <li> <p>Then the instructor assigns you grade as follows:</p> <ul> <li>If \\(x = y\\), then your grade is \\(x\\).</li> <li>If \\(x &lt; y\\), then your grade is \\(min\\{x,y\\}+2\\).</li> <li>If \\(x &gt; y\\), then your grade is \\(min\\{x,y\\}-2\\).</li> </ul> </li> </ol> <p>Which grade should you send to the instructor?</p> <p>If you know your partner is going to put 99, then you put 98 so you get 100, and your friend get 96. So, maybe, you are going to put 97 and get 99, but if your friend it doing 97, you would rather put 96 and get 98. But then your friend is doing 96, so you would rather put 95, etc. It turns out that the unique Nash equilibrium in this game is when both players play 2 (x = y = 2), which is not expected in practice with real players.</p> <p>So, we have this theorem that says that a Nash equilibrium always exists, but it has all these caveats. Let's consider alternative solution concepts that have emerged in game theory which, sometimes, may be a better model for this kind of games or strategic situations.</p>"},{"location":"lecture4/#correlated-equilibrium","title":"Correlated Equilibrium","text":"<p>Example: The Intersection Game</p> <p>This is similar to the \"Chicken\" and the \"Hawk-Dove\" games. Essentially, you arrive at an intersection, where another car arrives as well, and you need to decide what to do:</p> Go Wait Go (-99,-99) (1,0) Wait (0,1) (0,0) <p>What does this mean?</p> <ul> <li>If we both wait, we both get 0, nobody is moving.</li> <li>If we go and other person waits, we get 1 and they get 0.</li> <li>If we wait and the other person goes, we get 0 and they get 1.</li> <li>If we both go, we are very happy, we get a very negative utility, we have an accident.</li> </ul> <p>This implies the following equilibria:</p> <ul> <li>Asymmmetric equilibria: (Go, Wait), (Wait, Go), i.e. if they go, we want to wait, and vice versa.</li> <li>Symmetric equilibria: Go with a probability of \\(1\\%\\) and Wait with a probability of \\(99\\%\\), i.e. we each independently go with a probability of \\(1\\%\\) and wait with a probability of \\(99\\%\\)\u2014which is not great, since there is still a \\(1/10000\\) probability of colliding.</li> <li>Correlated Equilibrium: (Go, Wait) with a probability of \\(50\\%\\) and (Wait, Go) with a probability of \\(50\\%\\), i.e. we go and they wait with a probability of \\(50\\%\\), and we wait and they go with a probability of \\(50\\%\\). This is a correlated distribution: it is not independent. To implement this, we need a correlating device, i.e. something that is going to help us correlate our choices, such as a spotlight. For instance, the spotlight, with a probability of \\(50\\%\\) is going to show us red and show them green, or vice versa, but it is never going to show us both green (hopefully).</li> </ul> <p>Definition: Correlated Equilibrium</p> <p>A correlating device sends each player a secret recommended action (\"signal\") from a pubicly-known correlated distribution. No player can gain from deviating from the recommended action.</p> <p>In other words, when we have a correlated distribution over actions that gives a signal/recommended action, if the other player follows their recommended action, it is in our best interest to follow our own recommended action.</p> <p>For instance, if the spotlight shows us red, it is probably because someone else has green, so we don\u2019t want to go, but if the spotlight shows us green, then we know everyone must have red, so we might as well go.</p> <p>Note: Every Nash equilibrium is an (un)correlated equilibrium.</p> <p>Good news:</p> <ul> <li>A correlated equilibrium can be computed efficiently (e.g. with Linear Programming).</li> <li>If every player runs a Swap-Regret-minimizing algorithm, then the play converges towards the set of correlated equilibria. It is not really an equilibrium, though, as players will continue to cycle around the set of correlated equilibria forever.</li> <li>If every player runs an External-Regret-minimizing algorithm, then the play converges towards the set of coarse correlated equilibria.</li> </ul>"},{"location":"lecture4/#coarse-correlated-equilibrium","title":"Coarse Correlated Equilibrium","text":"<p>What is the difference between a correlated equilibrium and a coarse correlated equilibrium?</p> <p>Both types of equilibria rely on a correlating device to send each player a secret recommended action (\"signal\") from a publicly-known correlated distribution. However:</p> <ul> <li>In the correlated equilibrium: Players choose to follow the recommended action after seeing it.</li> <li>In the coarse correlated equilibrium: Players want to follow their average recommended action, byt may not like some recommendations.</li> </ul> <p>Question: How do we know which algorithm and the equilibrium are appropriate?</p> <p>One way to think about it is to remember that agents are going to do what is good for them. If you think players are going to use the simplest algorithm, it is probably external regret minimizing, they will like end up at the coarse correlated equilibrium. If you expect them to use a more sophisticated swap-regret algorithm (which has other game theory advantages), you expect them to end up at the correlated algorithm.</p> <p>Bad News:</p> <ul> <li>Just like the Nash equilibrium, the correlated equilibrium is not unique, and sometimes, it does not even make sense (cf. the CS269I Grade Game).</li> <li>Without a correlating device, we are not in an equilibrium: players have an incentive to deviate.</li> </ul>"},{"location":"lecture4/#stackelberg-equilibrium","title":"Stackelberg Equilibrium","text":"<p>Example: The Intersection Game, Revisited</p> <p>As a reminder:</p> Go Wait Go (-99,-99) (1,0) Wait (0,1) (0,0) <p>However, suppose now that you are playing against a \"dog driver\":</p> <ul> <li>You don't trust the dog to be rational.</li> <li>You don't trust the dog to follow the correlating device (the spotlight signal).</li> <li>You wait... so the dog can go.</li> </ul> <p>Conclusion: The dog is better than you at the Intersection Game. More generally, committing to a strategy gives power.</p> <p>This is what we call a Stackelberg equilibrium.</p> <p>Definition: Stackelberg Equilibrium</p> <p>A Stackelberg equilibrium is a pair of strategies (Leader's strategy, Follower's strategy) such that:</p> <ol> <li>The Follower's strategy is optimal given the Leader's strategy.</li> <li>The Leader's commitment is optimal, i.e. the payoff is optimal for the Leader among all pairs satisfying #1.</li> </ol> <p>In other words, the Follower's strategy is a best response to what the strategy chosen by the Leader, and the Leader chooses the optimal strategy for them assuming that the Follower is going to pick a best response.</p> <p>The Leader\u2019s utility, in the optimal Stackerlberg equilibrium, is at least what they can get in any possible equilibrium.</p> <p>Note: If we think about the games we talked about so far, we talked about mixed strategies. For instance, Messi did not want to tell Lloris which side he was going to kick. However, in the Stackelberg equilibrium, without loss of generality, the follower is using a deterministic action, because the leader is already committed to a strategy, and the fact that the follower is using a deterministic action gives us an efficient algorithm using Linear Programming agin (it is something that we can compute efficiently).</p> <p>Mini Case Study: Security Games</p> <p>A Stackelberg equilibrium may be used to model how security games where:</p> <ul> <li>The goal for defense forces (i.e. security checks, patrols, etc.) is to choose the optimal strategy.</li> <li>The assumption is that attackers can observe the defense strategy.</li> </ul> <p>In practice, this is deployed in a variety of domains:</p> <ul> <li>Infrastructure (airport security)</li> <li>Nature (wildlife protection)</li> <li>Urban crime (LA Metro)</li> <li>Cybersecurity (honeypots, audits)</li> </ul> <p>Challenges in real-world applications include:</p> <ul> <li>Algorithmic difficulty: How to deal with the combinatorial number of possible actions (e.g. a route in a large road network)?</li> <li>Uncertainty: What are the attacker's payoffs? Is the attacker rational? Will the defender be able to executed their strategy as planned?</li> <li>Collagoration with human defenders: Will patrol teams follow, or feel \"micro-managed\" by, the algorithm?</li> </ul>"},{"location":"lecture4/#recap","title":"Recap","text":"<p>Three Solution Concepts</p> <ul> <li> <p>Nash Equilibrium: Each player samples independently from a distribution. Nobody has an incentive to deviate from the distribution.</p> <ul> <li>Cleanest game theory assumption.</li> <li>May not be achievable in complex environments.</li> </ul> </li> <li> <p>Correlated Equilibrium: A correlated distribution of actions that every player would rather follow. This may arise when agents independently run ML algorithms.</p> </li> <li> <p>Stackelberg Equilibrium: The Follower's strategy is optimal given the Leader's strategy, and the Leader's commitment is optimal.</p> <ul> <li>Applies when one player has commitment power.</li> <li>It is unclear how to generalize for more than two players.</li> </ul> </li> </ul> <p>Example: The Penalty Kick Game\u2014What Really Happened (Spoiler Alert)</p> <p>Messi kicked left. Lloris jumped left. Messi scored.</p>"},{"location":"lecture5/","title":"P2P File-Sharing Dilemma","text":"<p>April 14, 2025</p> <p>P2P File-Sharing History:</p> <ul> <li>Napster (1999-2001)<ul> <li>Major file-sharing network at the time (mostly .mp3 music files)</li> <li>Represented an estimated 40\\%-60\\% of college dorms Internet traffic</li> <li>Network shut down in 2001 due to copyright infringement</li> </ul> </li> <li>Gnutella (2000 onwards).<ul> <li>Decentralized P2P file-sharing network, harder to shut down</li> <li>2010 court order to shut down popular client LimeWire (other clients remain available, but the popularity of the network is in decline)</li> </ul> </li> </ul> <p>Challenge: Free-Riding</p> <ul> <li>P2P file-sharing networks rely on users uploading content for sharing</li> <li>Users want to download files for free, but there aren\u2019t really many incentives when uploading files, so some people do not want to contribute back.</li> <li>On Gnutella, a large fraction of users only download (66\\% in 2000, 85\\% in 2005)</li> </ul> <p>A Simplified Game Theory Model of File-Sharing</p> <p>Game theorists usually call this the Prisoners' Dilemma.</p> <p>Consider the following game:</p> <ul> <li>There are two players: Harry and Marvin.</li> <li>Each player has two actions: \"upload\" and \"free-ride\".</li> <li>Their payoff is \\(-1\\) for uploading (due to legal risks) and \\(+3\\) for dowloading, which means:</li> </ul> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <p>In other words:</p> <ul> <li>If Harry free-rides and Marvin uploads, their utility is respectively \\(3\\) and \\(-1\\)\u2014and vice versa, if Harry uploads and Marvin free rides, their utility is respectively \\(-1\\) and \\(3\\).</li> <li>If they are both uploading, they both get \\(2\\): \\(3\\) for downloading and \\(-1\\) for uploading.</li> <li>If they are both free riding, there is nothing happening in the network, so their utility is \\(0\\).</li> </ul> <p>What should they do?</p> <p>For anything Marv would do, Harry would rather free ride (and vice versa), so the optimal thing for them to do is to free ride. However, if everyone free rides, there are no files available for download, so the social welfare is the worst.</p>"},{"location":"lecture5/#iterated-file-sharing-dilemma","title":"Iterated File-Sharing Dilemma","text":"<p>Let's now file-sharing dilemma game \\(n\\) times.</p> <p>Harry and Marvin now repeat the file-sharing game \\(n\\) times.</p> <ul> <li>In each iteration, they play the same file-sharing dilemma game. Their strategy may depend on past iterations.</li> <li>Their goal is to maximize their total payoff across all iterations.</li> </ul> <p>What should they do? How do we model this question?</p> <p>One consideration: Can agents commit to a specific strategy or not? If there are two agents sharing files on the internet, it is really hard to enforce commitments towards future rounds of the game.</p> <p>Definition: Subgame Perfect Equilibrium (SPE)</p> <ul> <li>On day \\(n\\), the agents play a Nash equilibrium.</li> <li>On day \\(n-1\\), the agents evaluate their actions assuming that they will play a Nash equilibrium on day \\(n\\). Then, they play a Nash equilibrium for this particular game.</li> <li>On day \\(n-2\\), we assume the agents will play an SPE in the future.</li> <li>Etc.</li> </ul> <p>An SPE means that agents cannot commit to playing suboptimal strategies in the future. For instance, if the optimal action for them is to free-ride tomorrow, they cannot commit to uploading tomorrow.</p> <p>Another way to think about this is that it is like a Nash equilibrium (they are thinking about what happens on the last day), with backward induction (they are thinking back and modeling the future with that thinking).</p> <p>In a way, this inability to commit is almost like the opposite of the Stackelberg equilibrium where the leader cannot commit to a strategy profile even when the strategy profile is suboptimal given what the follower is doing.</p> <p>Note: Anything that Harry does will change what Marvin does, and vice versa, and Harry needs to have some model of what Marvin will do in order to determine what will maximize his utility. However, one assumption we can make is that they are never doing something that is suboptimal.</p> <p>Example: SPE for the File-Sharing Dilemma Game Iterated \\(n\\) Times</p> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <ul> <li>In the \\(n\\)-th iteration, free-riding is the dominant strategy. Note that what happened in the past does not change incentives here.</li> <li>In the \\((n-1)\\)-th iteration, free-riding is still strategic. No matter what a player does now, both will free-ride in the next iteration.</li> <li>By induction, free-riding is strategic in every iteration.</li> </ul>"},{"location":"lecture5/#repeated-games-with-discounting","title":"Repeated Games with Discounting","text":"<p>Example: \\((1-p)\\)-Discounted File-Sharing Dilemma</p> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <p>Random-stopping assumption: We are playing\u2014possibly forever, when \\(p = 0\\). At every iteration, we stop playing with probability \\(p\\) (for instance because another player\u2019s connection defaults, or because there was a lawsuit against the network, etc.).</p> <p>In other words, Harry and Marvin repeat the file-sharing game:</p> <ul> <li>In each iteration, they play the same file-sharing dilemma game. Now, their strategy may depend on past iterations.</li> <li>At each iteration, they stop with probability \\(p\\).</li> <li>Their goal is to maximize their total payoff across all iterations.</li> </ul> <p>Here, it is less obvious what players should do (due to the random stopping rule).</p> <p>Note: This is also a popular model in general (outside of file-sharing) to model interest rates because you would rather have your money today than tomorrow, so every day in the future is worth a little less.</p> <p>One possible strategy that Harry can choose is called the Grim Trigger Strategy.</p> <p>Definition: The Grim Trigger Strategy</p> <ul> <li>If there was ever a time when Marvin did not upload, do not upload ever again.</li> <li>Otherwise, upload.</li> </ul> <p>Essentially, if there is ever a time when Marvin free-rides, then from that point on, Harry free-rides forever, but until then, he uploads. This is all about making a threat: if Marvin ever stops sharing, then Harry will stop sharing with him. Note however that, on the last day, there is no threat, because there is no future.</p> <p>Let's analyze Marvin\u2019s optimal strategy, given that Harry is playing the Grim Trigger Strategy. We are assuming that Marvin believes that Harry will actually play Grim Trigger (we will see later why this is a possible assumption). In this scenario, Marvin\u2019s optimal strategy is either to always upload or to never upload, and which one is optimal depends on \\(p\\).</p> <p>Analysis: Marvin's Strategy When Harry Plays The Grim Trigger</p> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <p>Always Uploading: If Marvin always uploads, Harry also always uploads, so every day they keep playing, Marvin has an expected payoff of \\(2\\). This is why the payoff is basically \\(2 \\cdot the \\; geometric \\; sequence \\; for \\; (1-p)\\), which comes down to \\(2/p\\). In other words, if Marvin always uploads, his expected utility is \\(2/p\\).</p> <p>Never Uploading: If Marvin never uploads, in the first iteration, Harry uploads, and then Harry sees that Marvin did not upload, so he is never going to upload again. The payoff for Marvin is \\(3\\) for the first day, and \\(0\\) for the rest of time, meaning that his utility is \\(3\\).</p> <p>Conclusion: To decide whether he should always upload or never upload, Marvin should ask when \\(3\\) is greater than \\(2/p\\), which means that he should always upload when \\(p \\leq \\frac{2}{3}\\).</p> <p>If Harry is playing the Grim Trigger strategy, then it makes sense for Marvin to always upload whenever \\(p \\leq \\frac{2}{3}\\). So, if the game ends every day with probability \\(\\frac{1}{2}\\), then it makes sense for both Harry and Marvin to always upload, which is why this is an SPE.</p> <p>Question: We saw earlier that for any number of iteration \\(n\\), both players should play the strategy of never uploading. How could it be that it makes sense for Marvin to always upload whenever \\(p \\leq \\frac{2}{3}\\), if for any \\(n\\), the SPE is to never upload?</p> <p>The point is that, in the random-stopping game, the players do not know in advance when the game is going to end, so any day, they are still doing the same calculations that we just did (on day \\(7\\), it is still the same as on day \\(1\\)), which is why there is always an incentive to upload and make their future better, because they don\u2019t know when the future terminates. However, when we analyze the SPE with \\(n\\) days, we know when the last day is, so we can determine when there is no point uploading (because there is no future).</p> <p>Theorem: Folk Theorem (Game Strategy)</p> <p>If the players are patient enough, then repeated interaction can result in virtually any average payoff in an SPE equilibrium.</p> <p>In other words, the Folk theorem gives general conditions on when Harry can incentivize Marvin to play certain actions (e.g., uploading) using threats like the Grim Trigger strategy.</p> <p>On issue with the Grim Trigger strategy is that, if for some reason, Marvin's connection goes down one day and he cannot upload, then Harry will never upload again, and Marvin in turn will not upload again, so they get stuck with a utilit of \\(0\\) until the end of the game.</p> <p>A more practical, robust alternative to the Grim Trigger strategy is called the Tit-for-Tat strategy.</p> <p>Definition: The Tit-for-Tat Strategy</p> <ul> <li>In stage \\(1\\), upload.</li> <li>In stage \\(i\\), reproduce the action of the other player on stage \\(i-1\\).</li> </ul> <p>Analysis: Marvin's Choices When Playing Tit-for-Tat</p> Upload Free-ride Upload (2,2) (-1,3) Free-ride (3,-1) (0,0) <p>At stage \\(i\\), Marvin can:</p> <ul> <li>Upload on stage \\(i\\) (utility of \\(-1\\)), and then download on stage \\(i+1\\) (utility of \\(+3\\)) with probability \\((1-p)\\), because there is the probability that there is no next day.</li> <li>Not upload on stage \\(i\\) (utility of \\(0\\)), and then not download on stage \\(i+1\\) (utility of \\(0\\)).</li> </ul> <p>Conclusion: To decide whether he should always upload or never upload, Marvin should ask when \\(-1 + 3 \\cdot (1-p) \\geq 0\\), which means that he should always upload whenever \\(p \\leq \\frac{2}{3}\\).</p> <p>Notes:</p> <ul> <li>The conclusion is the same for Tit-for-Tat as for Grim Trigger, but this is a coincidence: in general, Grim Trigger is a stronger threat.</li> <li>When \\(p = \\frac{2}{3}\\), then both \u201calways upload\u201d \u201cnever upload\u201d are strategic.</li> </ul> <p>Iterated File-Sharing Dilemma Recap</p> <ol> <li>In a one-shot game, free-riding is a strictly dominant strategy, i.e. for any action Marvin takes, Harry is always better of not uploading.</li> <li>With any fixed number of iterations, free-riding is the SPE.</li> <li>With a random number of iterations, a credible threat of future retaliation can lead to cooperation.</li> </ol> <p>In practice, game theory predicts that:</p> <ol> <li>Players do not cooperate in one-round games.</li> <li>Players do not cooperate in \\(n\\)-round games.</li> <li>Players likely cooperate in games with a random number of rounds.</li> </ol> <p>In fact, in the real world, we frequently observe \\((1)\\) and \\((3)\\): in tourist traps, restaurants they might as well charge customers as much as they can, because they will only see them once, while in locals\u2019 favorite spots, restaurants expect customers to come over and over, so they have incentives to serve them well.</p> <p>What about \\((2)\\)?</p> <p>Experiment: Axelrod Games</p> <p>Around 1980, Professor Robert Axelrod invited friends to write computer programs for a tournament of iterated file-sharing dilemma with a fixed number of round \\(n = 200\\).</p> <p>Out of 15 submissions, Tit-for-Tat won first place.</p> <p>Notes:</p> <ul> <li>Tit-for-Tat can never win a head-to-head match, but encouraging cooperating leads to a higher score on average.</li> <li>You can always do better than Tit-for-Tat: you can play Tit-for-Tat during \\(199\\) rounds, and not upload in the last round, but in practice, no participant tried this strategy.</li> </ul> <p>Later on, Professor Axelrod invited his friends to play again. Out of 62 submissions, Tit-for-Tat won first place again.</p> <p>Note: Veritasium's What Game Theory Reveals About Life, The Universe, and Everything YouTube video offers an excellent perspective on this topic.</p> <p>So, in practice, \\((2)\\) requires fragile assumptions: Harry assumes in round \\(i\\) that \\(\\rightarrow\\) Marvin assumes in round \\(i+1\\) that \\(\\rightarrow\\) Harry assumes in round \\(i+2\\) that \\(\\rightarrow\\) ... plays optimally in round \\(n\\).</p>"},{"location":"lecture5/#bittorrent-strategies","title":"BitTorrent Strategies","text":"<p>Definition: BitTorrent Overview</p> <ul> <li>BitTorrent is currently the main P2P protocol for file-sharing.</li> <li>In 2019, BitTorrent accounted for almost 30\\% of upload traffic (with major spikes after episodes of Game of Thrones).</li> <li>Users are organized into swrams sharing the same file.</li> <li>A decentralized tracker coordinates active users in a given swarm.</li> <li>Each file is broken down into a number of pieces (e.g. 1,000).</li> <li>Since users need many (all) pieces, they play the iterated file-sharing dilemma game.</li> </ul> <p>Definition: The BitTorrent Default Strategy</p> <p>This is the strategy most users play because they just download the default BitTorrent client that uses this strategy by default:</p> <ul> <li>Every 15-30 minutes, a user contact the tracker, requesting a new random subset of swarm peers. So, their number of known swarm peers grows over time.</li> <li>Each user attempts to download from their peers.</li> <li>Each user has \\(s\\) slots and allocates \\(\\frac{1}{s}\\) of upload bandwidth to each slot.</li> <li>\\(s\\) peers receive a slot (\"unchoked\"), chosen using a variant of the Tit-for-Tat strategy: each user prioritizes peers who sends them the most data. One slot is reserved for \"pity uploads\" to random peers (\"optimistic unchoking\"), i.e. a freebee for users who do not have data yet (to give \"newbies\" a chance).</li> <li>For each fixed peer, the priority is given to uploading rare file pieces.</li> </ul> <p>Definition: The BitThief Strategy</p> <ul> <li>Never upload anything.</li> <li>Ask tracker for peers much more frequently (to grow the number of peers quickly).</li> </ul> <p>The idea is to maximize the chances of optimistic unchoke. In experiements, though, this strategy is \\(5x\\) slower than the default BitTorrent strategy. It still completes downloads in reasonable time without any uploads.</p> <p>Definition: The BitTyrant Strategy</p> <p>In the Tit-for-Tat/BitTorrent strategy, the idea is to upload data to peers who send you the most data.</p> <p>In contrast, in the BitTyrant strategy, the idea is to upload data to peers who will send you the most data:</p> <ul> <li>For each peer \\(j\\), estimate the amount of upload \\(u_j\\) so that \\(j\\) unchokes you.</li> <li>For each peer \\(j\\), estimate the speed of dowload \\(d_j\\) if \\(j\\) unchockes you.</li> <li>Prioritize sending as much data as possible to peer with the maximum \\(\\frac{d_j}{u_j}\\) ratio.</li> </ul> <p>In other words, the rationale is to look for peers who will give you the most data if you give them a little bit of data.</p> <p>In experiments, this provides \\(\\approx 70\\%\\) download speed gains over the standard BitTorrent strategy.</p> <p>BitTorrent Strategies Recap</p> <ul> <li>Tit-for-Tat (default): Prioritize sendint data to peers who send you the most data. Reserve one upload slot for new users (\"optimistic unchoke\").</li> <li>BitThief: Don't upload anything. Contact as many peers as possible to maximize chances of optimistic unchoke.</li> <li>BitTyrant: Prioritize sending data to peers who would send you the most data in return.</li> </ul>"},{"location":"lecture5/#recap","title":"Recap","text":"<p>P2P File-Sharing Recap</p> <ul> <li>Free-riding Users only download, and don't contribute uploads to others.</li> <li>One-shot file-sharing dilemma: Free-riding is a dominant strategy.</li> <li>Iterated file-sharing dilemma: Tit-for-Tat encourages uploads.</li> <li>Subgame Perfect Equilibrium (SPE): Agents choose today's optimal strategy, assuming they will play an SPE in the future.</li> <li>BitTorrent (decentralized Tit-for-Tat): Free-riding is possible on optimistic unchokes. Strategizing over which peers give the best return for uploads is also possible.</li> </ul>"},{"location":"lecture6/","title":"Market Equilibrium","text":"<p>April 16, 2025</p> <p>Definition: Market</p> <p>\"A means by which the exchage of goods and services takes place as a result of buyers and sellers being in contact with one another, either directly or through mediating agents or situations.\" \u2014 Joan Violet Robinson, Britannica</p> <p>Previously, we examined scenarios without monetary transfers. Issues with non-monetary markets include:</p> <ol> <li>Limited to ordinal rather than cardinal preferences.</li> <li>Emergence of underground markets.</li> <li>Potential exploitation (bots manipulating donor lists).</li> </ol> <p>Starting today, we want to measure utility with money:</p> <ul> <li>Basic assumption: How much I want something is equal to how much I am willing to pay for it.</li> <li>Obvious caveat: How much I am willing to pay depends on how much money I have.</li> </ul>"},{"location":"lecture6/#modeling-buyers","title":"Modeling Buyers","text":"<p>Definition: Cardinal Utilities vs Ordinal Utilities</p> <ul> <li>Cardinal utilities: Assign numeric values to preferences.</li> <li>Ordinal utilities: Only rank preferences by order.</li> </ul> <p>Is one type of utilities better than the other?</p> <p>Cardinal utilities convey richer information (ordinal can be derived from cardinal), are intuitive in economic analysis, and typically monetarily measurable.</p> <p>For example, in the kidney exchange program, we may not know whether we prefer one kidney over another, but we can compare two risky matches with one perfect match (one perfect match is definitely better than one risky match) with numbers by adding the happiness that the two risky patients may get and comparing it with the happiness of the one perfect match patient, which is harder to determine with only ordinal utilities.</p> <p>In addition, we can think about probabilistic events, because when we have probabilities and numbers, we can check the expectation. If we can have a good kidney with \\(50\\%\\) or an okay match with \\(70\\%\\) (more likely), we can use expectation to compare those options, while with cardinal utilitie alone, comparison is more challenging.</p> <p>Finally, let's note that we typically quantify cardinal utilities with money, which is good in some applications, but problematic in some other applications. There are alternatives, such as \u201cquality-adjusted life years\u201d in health economics, which measure how many additional years a patient may live with a kidney. It is not perfect either, but it is some way to help us make these decisions.</p> <p>In other applications, where money is not a good idea, it is not clear what the right measure is. For instance, how do we quantify utility in education (e.g. how much utility do we get from taking CS269I?\u2014a lot, for sure, but how much?) or in dating apps (e.g. how do we quantify how we are making better matches?)</p> <p>In contrast, one disadvantage of cardinal utilities is that humans are better at comparing outcomes.</p> <p>Introducing money changes market dynamics substantially:</p> <ul> <li>Stanford housing would differ if rooms were auctioned.</li> <li>Buying organs is illegal in most countries.</li> <li>Hospitals-student monetary matches face legal restrictions.</li> </ul> <p>However, there are also some challenges in non-monetary transfers:</p> <ul> <li>Without monetary transfers, we are limited to ordinal preferences.</li> <li>When we force non-monetary transfers, people try to get their way under the table, which comes with legal issues (admissions scandal) and risks (illegal organ transplants, for both people who buy and sell organs)</li> <li>We need a permission systems (as opposed to a permission-less system, such as the Internet or Bitcoin), otherwise,<ul> <li>For instance in the Stanford housing lottery, you need a SUNet ID, so that applicants do not enter the draw with a bot and get a million lottery tickets.</li> <li>With online reviews, we really have a problem with restaurants writing really good reviews for themselves and bad reviews for competing restaurants.</li> </ul> </li> </ul> <p>Definition: Fungible Goods vs Idiosyncratic Goods</p> <ul> <li>Fungible goods: Interchangeable units.</li> <li>Idiosyncratic goods: Unique goods.</li> </ul> <p>In other words, goods are fungible when there lots of interchangeable copies of an item, such as copies of a new book. In contrast, goods idiosyncratic when they have intrinsic features. If you try to buy a house, every house is very different from the house down the street (problems with the roof, nicer garden, better location, etc.), so we need to understand the value of each house. In dating apps, it is similar: each person is very different. NFTs (Non-Fungible Tokens) by definition are idiosyncratic.</p> <p>In practice, many things are a mix of fungible and idiosyncratic. For instance, in a ride-sharing app, we don\u2019t really care who the driver is (the driver is fungible), but we really care where we go (the destination is idiosyncratic).</p>"},{"location":"lecture6/#supply-and-demand-of-fungible-goods","title":"Supply and Demand of Fungible Goods","text":"<p>Definition: Demand Curve</p> <p>Consider a perfectly fungible good, e.g., copies of a book:</p> <ul> <li>1 buyer is willing to pay $10 for the book.</li> <li>2 additional buyers are willing to pay $9 for the book.</li> <li>1 addition buyer is willing ti pay $8 for the book.</li> <li>Etc.</li> </ul> <p>We can plot this aggregate demand curve, which is typically decreasing, as more buyers are interested when the price is lower:</p> <p></p> <p>Note: The demand curve represents how many buyers are willing to buy at each price. We plot the Price as the y-axis and the Demand as the x-axis, even though it is really the demand as a function of the price.</p> <p>Definition: Supply Curve</p> <p>Let's specifically talk about used (but in identical conditions) books, which means that the books are still fungible:</p> <ul> <li>2 sellers are willing to give their copies for $1.</li> <li>1 addition seller is willing to give their copy for $2.</li> <li>3 additional sellers are willing to give their copies for $3.</li> <li>Etc.</li> </ul> <p>We can plot this aggregate supply curve, which is typically increasing, at least for used books.</p> <p></p> <p>Note: The supply curve represents how many sellers are willing to sell at each price. We plot the Price as the y-axis and the Supply as the x-axis, even though it is really the supply as a function of the price.</p> <p>Side Note: Law of Supply vs. Digital Goods</p> <p>If the applies to used books, then what about new books?</p> <p>The Law of Supply says that the supply curve should be increasing, but sometimes, this does not hold.</p> <p>For instance, for new books:</p> <ul> <li>There is a monopoly (the publisher is selling all the copies): increasing the price does not mean that they are willing to sell it more.</li> <li>Marginal cost of printing another book is negligible (most of the price is editorial work, promotion, etc.)</li> </ul> <p>If we think of digital goods, the marginal cost of making another copy is zero, so the Law of Supply does not hold anymore:</p> <ul> <li>For ordinary physical goods, the supply curve slopes upward because each additional unit costs a little more to make. Producers only expand output when price at least covers that rising marginal cost.</li> <li>For a pure digital good\u2014software, an e\u2011book, a music file\u2014the marginal cost of copying and distributing one more unit is essentially zero, so a seller is already willing to supply any quantity at any price just above zero.\u00a0Raising the price therefore doesn\u2019t unlock extra output the way it does for physical goods. Instead, the constraint is usually legal (who owns the copyright) or strategic (how the seller wants to segment the market), not technological cost.</li> </ul> <p>In the short run the supply curve is therefore flat (perfectly elastic) at roughly zero cost, and in the long run depends on covering the fixed cost of creating the first copy rather than on the per\u2011unit price, so the classic Law of Supply tied to marginal production cost doesn\u2019t bite.</p> <p>Definition: Market Clearing Price</p> <p>The price where demand and supply curves meet is called the market clearing price or market equilibrium.</p> <p></p> <p>The vanilla assumption in classical economics is that buyers and sellers should naturally converge to the special price \\(p^\\star\\), where the supply meets the demand. At market clearing price, the numbers of interested buyers and sellers are equal, so they can all transact (i.e. \"clear the market\").</p> <p>This is not complicated but super important. The reason we call this an equilibrium is because:</p> <ul> <li>At that price, all the buyers who want to buy for that price can buy exactly from all the sellers who want to sell at that price (the supply is exactly meeting the demand).</li> <li>There are some sellers who are willing to sell at a higher price, but they stay out of it.</li> <li>There are some buyers who are willing to buy at a lower price, but they stay out of it.</li> </ul> <p>We are trying to think of a single price that works for the entire market. We will formalize it later, but this price \\(p^\\star\\) maximizes the total happiness.</p>"},{"location":"lecture6/#supply-and-demand-of-idiosyncratic-goods","title":"Supply and Demand of Idiosyncratic Goods","text":"<p>The Airbnb Market Model</p> <ul> <li>\\(m\\) different (idiosyncratic) rooms for rent.</li> <li>\\(n\\) guests, each willing to stay in at most one room (unit-demand).</li> <li>Guest \\(i\\) has value \\(v_{i,j}\\) for staying in room \\(j\\).</li> <li>If guest \\(i\\) pays\\(p_j\\) to stay in room \\(j\\), their happiness is given by \\(U_{i,j} = v_{i,j} - p_j\\).</li> <li>If guest \\(i\\) doesn't stay in any room (e.g., camps outside), their happiness is given by \\(U_{i,\\emptyset} = 0\\).</li> </ul> <p>The Airbnb Market Model (in Economics Jargon)</p> <ul> <li>\\(m\\) different (idiosyncratic) goods for rent.</li> <li>\\(n\\) buyers, each willing to buy at most one good (unit-demand).</li> <li>Buyer \\(i\\) has value \\(v_{i,j}\\) for buying good \\(j\\).</li> <li>If buyer \\(i\\) pays\\(p_j\\) for good \\(j\\), their utility is given by \\(U_{i,j} = v_{i,j} - p_j\\).</li> <li>If buyer \\(i\\) doesn't buy any good (e.g., camps outside), their utility is given by \\(U_{i,\\emptyset} = 0\\).</li> </ul> <p>Definition: Competitive Equilibrium</p> <p>A competitive equilibrium is the combination of a price vector \\(\\overrightarrow{p} = (p_1, ..., p_n)\\) and a matching \\(M\\) of buyers to goods, such that:</p> <ul> <li>Each buyer is matched to their favorite good (given prices): </li> </ul> \\[ \\forall i,j' v_{i,M(i)} - p_{M(i)} \\geq v_{i,j'} - p_{j'}. \\] <ul> <li>If no buyer is matched to good \\(j\\), then \\(p_j = 0\\).</li> </ul> <p>There is one more condition for a competitive equilibrium. If a buyer is not matched, that means that they don\u2019t want any good. This means that the utility for every good is negative (the value minus what it would cost them is negative). So, they stay unmatched (they have the option to remain unmatched).</p> <p>This is an important concept called individual rationality: no agent is strictly less happy after participating in the mechanism. In other words, every buyer is no worse after transacting than they were before. If they cannot find anything in their price range, they don\u2019t get anything: they don\u2019t lose money by just going to the site and checking all the options.</p> <p>Question 1: How is it possible that every buyer is matched to their favorite good? What if everyone wants the same good?</p> <p>The intuition to manage this is through prices, because if a good is over-demanded, we increase the price, until buyers leave it.</p> <p>Question 2: What if there are two buyers with the exact same utility?</p> <p>Because we have a weak inequality (rather than a strict inequality), it means that every buyer is matched until they do not have another good that they prefer, so if two buyers have the same utility, we can increase the price until they are indifferent between two rooms, and then we can give either room to either buyer and they will both be happy.</p> <p>Question 3: What if there are two people who are willing to pay $6B for the same room?</p> <p>We increase the price to \\(\\$6\\)B, and at that point, they are indifferent between getting the room for \\(\\$6\\)B and not getting any room (or getting another cheap room), so we can tie-break arbitrarily and give the room to one of them.</p> <p>Question 4: Does buying the room provide additional utility, such as the satisfaction of winning the auction and showing off? We are not modeling this here. It is just that every buyer has some value for each good, but they do not care about what everyone else is getting.</p> <p>Why is a competitive equilibrium an equilibrium?</p> <p>Reminder: In a competitive equilibrium:</p> <ol> <li>Each buyer is matched to their favorit good (given prices) or no good if they don't want any.</li> <li>If no buyer is matched to good \\(j\\), then \\(p_j = 0\\).</li> </ol> <p>Therefore:</p> <ul> <li>By \\((1)\\), no buyer wants to deviate to a different room, they are given their favorite room given the prices.</li> <li>No good is over-demanded (we are matching each buyer to a room, but we are not matching two buyers to the same room), so no there is no pressure to increase prices.</li> <li>By \\((2)\\), under-demanded goods are priced at \\(0\\), so we can't decrease their price lower any more.</li> </ul> <p>Important Note</p> <p>Aviad pointed out a notion that is always confusing for students: A competitive equilibrium is the combination of prices and the assignment of guests to rooms together (all of that constitute the equilibrium, not just one or the other).</p> <p>The last condition essentially says that there is always an empty room that is priced at \\(0\\). Buyers compare all available rooms at their respective prices, but they also compare those with the option of not getting a room and not paying any price (staying in a tent for instance).</p> <p>Question: What about seller\u2019s utility?</p> <p>This already captures seller\u2019s utility. What this does not capture is production cost. This definition makes sense if we assume that sellers have \\(0\\) cost, which we will pretend for now, which in practice is not true but we will hide it to keep things simpler.</p> <p>Question: Why if no buyer is matched to a good, then the price is 0?</p> <p>If a good is unpopular and no one wants it, we don\u2019t know if it is because it is priced too high, or because it is bad (e.g. a room in a bad neighborhood). So, we lower the price, and we lower the price, until it reaches \\(0\\). If nobody wants it anyway, we know that the price is not too high.</p>"},{"location":"lecture6/#competitive-equilibrium-properties","title":"Competitive Equilibrium Properties","text":"<p>Anywhere in the world, we can go on Airbnb (from Brazil on a tablet, from India on a phone), and look up rooms in Paris. Once we have equilibrium prices, Airbnb can show all the rooms, with all the prices, to all the guests around the world, and there are not going to be conflicts (unlike when everyone is trying to \u201cenroll now\u201d in some classes with limited seats). The equilibrium prices coordinate things in such a way that no two people want the same room.</p> <p>In practice, it is an approximate equilibrium, so there might be conflicts, but the main thing that makes Airbnb different from Stanford classes or Burning Man tickets (where prices are fixed and good are over-demanded) is that prices are an approximate equilibrium.</p> <p>From a pure computer science system design perspective (leaving aside economics), equilibrium prices are great because the system is not going to crash (since people want different rooms).</p> <p>Definition: Social Welfare</p> <p>The social welfare of an allocation \\(M\\) is the total buyers' value: \\(\\underset{i}{\\sum} v_{i,M(i)}\\).</p> <p>Question: Why don't we include prices in the definition of social welfare?</p> <p>We did not include prices in the definition of social welfare, even though happiness is equal to the value minus the price, because we count the total utility of everyone, including sellers, so prices they cancel out.</p> <p>The price goes from the buyers to the sellers, so it is zero-sum, it does not change the total happiness, while what changes the total happiness is the quality of the matching.</p> <p>In contrast, if we wanted to calculate the total happiness of only the buyers, then we would subtract the prices, and if we wanted to calculate the total happiness of only the sellers, we would only care about the prices (this is called revenue maximization).</p> <p>We are also abstracting a bunch of things, such as production costs (including cleaning fees), and taxes (which may make the government happy).</p> <p>Theorem: First Welfare Theorem</p> <p>If \\((p,M)\\) is a competitive equilibrium, then \\(M\\) is a matching that maximizes social welfare:</p> \\[ \\forall M' \\; \\underset{i}{\\sum} v_{i,M(i)} \\geq \\underset{i}{\\sum} v_{i,M'(i)} \\] <p>Another amazing aspect of competitive equilibrium prices is that they maximize the total happiness (social welfare). In any competitive equilibrium, the matching maximizes the total sum of values. If we compare the matching \\(M\\) to any other matching \\(M'\\), the welfare in \\(M\\) at least the welfare in \\(M'\\). Competitive equilibrium prices are making Airbnb happy (they are making a lot of money), the hosts happy (they rent their houses), and guests happy (they find rooms to stay in).</p> <p>Proof: A competitive equilibrium is optimal.</p> <p>Assume by contradiction that \\(\\exists M' \\; \\underset{i}{\\sum} v_{i,M(i)} &lt; \\underset{i}{\\sum} v_{i,M'(i)}\\).</p> <p>By \\((1)\\), \\(\\underset{i}{\\sum} (v_{i,M(i)} - p_{M(i)}) \\geq \\underset{i}{\\sum} (v_{i,M'(i)} - p_{M'(i)})\\).</p> <p>Therefore, \\(\\underset{i}{\\sum} p_{M(i)} &lt; \\underset{i}{\\sum} p_{M'(i)}\\).</p> <p>But, by \\((2)\\), \\(\\underset{i}{\\sum} p_{M(i)} = \\underset{j}{\\sum} p_j \\geq \\underset{i}{\\sum} p_{M'(i)}\\).</p> <p>We have reached a contradiction, so \\(\\forall M' \\; \\underset{i}{\\sum} v_{i,M(i)} \\geq \\underset{i}{\\sum} v_{i,M'(i)}\\).</p> <p>As a reminder, the First Welfare Theorem tells us that the total sum of values is maximized at the competitive equilibrium matching \\(M\\). It is helpful to remember that the statement about total happiness does not include prices anywhere.</p> <p>We assume that there is a matching \\(M'\\) that violates the inequality in the theorem statement, i.e. \\(M'\\) has some strictly higher total happiness. Every \\(i\\) maximizes their happiness (value minus price) at the equilibrium matching \\(M\\).</p> <p>When we subtract both inequalities, it results that sum of prices in \\(M\\) is less than the sum of prices in \\(M'\\): these are the same prices, but summed over different instances (on the left, over goods that are assigned in \\(M\\), and on the right over the goods that are assigned in \\(M'\\)).</p> <p>However, condition \\((2)\\) in the definition of the competitive equilibrium tells us that any good that is not assigned in \\(M\\) has a price of \\(0\\), so the sum of anything that is assigned in \\(M\\) is the same as the sum of all the prices in general. Thus, the sum of all the prices has to be at least the sum of everything that is assigned in \\(M'\\).</p> <p>On one hand, we have an inequality that says that the sum of the prices of what is assigned in \\(M\\) is strictly less than in \\(M'\\), and on the other hand, we have the opposite inequality, so together, we get a contradiction.</p> <p>What is interesting in this proof is how it uses the second condition of the definition of competitive equilibrium, which says that goods that are not assigned have a price of \\(0\\).</p> <p>Question: Why do we have this inequality where the prices of all the goods is greater than the prices of the goods matched in M\u2019?</p> <p>There are more rooms than guests, so we are not necessarily matching all the rooms, but in \\(M\\) we are matching all the rooms that have a non-zero price. It is possible that something that is matched in \\(M\\) (with a positive price) is not matched in \\(M'\\) (so the price is \\(0\\)), because \\(M'\\) does not necessarily match all the goods, since it is not a competitive equilibrium).</p> <p>In the \"Airbnb market model\", a competitive equilibrium always exists.</p> <p>This is non-trivial: We will see later a simple example where each buyer wants two goods and a competitive equilibrium does not exist.</p>"},{"location":"lecture6/#deferred-acceptance-with-prices","title":"Deferred Acceptance With Prices","text":"<p>Let's consider the following setting:</p> <ul> <li>Assumption: Prices are always in some finite range (e.g., \\(\\$0-\\$1,000\\)), with finite increments (e.g., \\(\\$1\\)).</li> <li>Ordinal Preferences: For each buyer, we construct a list of all the \\((good,price)\\) options, ordered by utility. We can truncate the list a the \\((receive \\;nothing, pay \\; nothing)\\) option.</li> <li>At each iteration of the algorithm: The unmatched buyer whose next-favorite option is \\((j,p)\\) proposes price \\(p\\) to good \\(j\\). Good \\(j\\) tentatively accepts if price \\(p\\) is higher than the prices it was offered so far.</li> </ul> <p>This is a twist on the DA we saw before because it is DA with prices. Here, ordinal preferences are not only established (like in the past) based on ordinarily over rooms, but over combinations of rooms and prices. We can truncate the list at the (receive nothing, pay nothing) option, which is like an outside option (e.g. sleep in a tent instead). Note that the room preferences (over buyers) are only based on prices (i.e. rooms are indifferent over buyers).</p> <p>Let's consider the following scenario, when Mario, Luigi, and Yoshi are going to Paris and are looking for a place to stay near famous monuments:</p> <ul> <li>Mario is willing to pay \\(18\\) for the room near the Eiffel Tower, and \\(16\\) for the room near the Pasteur Institute.</li> <li>Luigi is willing to pay \\(14\\) for the room near the Eiffel Tower, and \\(6\\) for the room near the Pasteur Institute.</li> <li>Mario is willing to pay \\(12\\) for the room near the Eiffel Tower, and \\(8\\) for the room near the Pasteur Institute.</li> </ul> <p>Here is the result:</p> <p></p> <p>This is an overview of what happened:</p> <ul> <li>When the prices are \\(2\\) for Eiffel Tower or \\(0\\) for Pasteur, Mario is indifferent between both rooms, so Mario proposes \\(0\\) to Pasteur.</li> <li>But Luigi and Yoshi still competing for Eiffel Tower, so they keep raising their prices, until the price for Eiffel Tower is \\(4\\), which means that Yoshi is now indifferent between the two rooms.</li> <li>Then, Yoshi is going to switch back and forth between Paster and Eiffel Tower as Mario and Luigi raise the price for Eiffel Tower (every time one place is over-demanded and Mario and Luigi raise the price, Yoshi switches to the other one)- \u2026</li> <li>\u2026 until Yoshi gets priced out.</li> </ul> <p>Note: For more details, refer to slides 37-61.</p> <p>Question: What causes Mario to become indifferent to the Eiffel Tower in the first place?</p> <p>When the Eiffel Tower costs \\(2\\) and Pasteur costs \\(0\\), the difference between both rooms is \\(2\\) more in favor of the Eiffel Tower. Mario\u2019s preference for the Eiffel Tower over Pasteur is \\(2\\) more, which exactly balances the difference of \\(2\\).</p> <p>Here is how to interpret the results:</p> <ul> <li>The price that Mario wants to pay for Pasteur (\\(9\\)) is higher than Yoshi\u2019s (\\(8\\)), and similarly, the price Luigi wants to pay for Eiffel Tower (\\(12\\)) is equal than Yoshi\u2019s (\\(12\\)).</li> <li>Mario could have also paid \\(8\\), but this is just due to how the tie-breaking worked out.</li> <li>This is the matching that maximizes the total values on the edges.</li> <li>This is not only a competitive equilibrium, it is also the unique buyer-optimal competitive equilibrium.</li> </ul> <p>Question: Does the order matter?</p> <p>Just like in DA, order does not really matter, except for when it comes to tie-breaking, which may only matter up to \\(1\\) (increment).</p> <p>Question: What is the total social welfare 16 +14?</p> <p>Yes. \\(12+9\\) are the prices that Mario and Luigi pay. The welfare is the value that Mario see in Pasteur (\\(16\\)) and Luigi sees in Eiffel Tower (\\(14\\)). In other words, it is not about how much buyers actually pay for the rooms, but how much they are willing to pay for the room.</p> <p>Question: Are Luigi and Mario determining the prices based on the maximum prices that Luigi is willing to pay?</p> <p>Yes, because these are the prices it takes to price Yoshi out (since there is one more buyer than room, we need to price out one buyer).</p> <p>Claim: Da with prices always terminates.</p> <p>The algorithm always terminates because every buyer is either matched to a good or they reach a point where they prefer to pay nothing and buy nothing.</p> <p>The running time of DA with prices is in the order of \\(O(n \\cdot m \\cdot i)\\), where \\(i\\) is the number of increments.</p> <p>The running time is at most the number of buyers (\\(n\\)) times the number of rooms (\\(m\\)) times the number of increments (\\(i\\)), i.e 1,000 increments of $1 our example. This is really fast when (\\(i\\)) is small (and there is a way to make it faster when (\\(i\\)) is not too small), so it is a really fast algorithm with nice economic properties.</p> <p>The DA with prices algorithm finds a competitive equilibrium:</p> <ol> <li>Each buyer is matched to their favorite \\(\\(good,price\\)). If there were any more preferred \\(\\(good,price\\)), the buyer would have already proposed and get kicked out, so the other goods raise the price higher.</li> <li>If a good is unmatched, its price is zero. Once a buyer proposes to a good, a good is always matched, even if to another buyer (and its price is above \\(0\\)), so the only way a good is unmatched is if it is never proposed to (and its price is \\(0\\)).</li> </ol> <p>Bonus Features (inherited from DA):</p> <ul> <li>Buyer-optimal among competitive equilibria.</li> <li>Buyer-strategyproof.</li> </ul> <p>Note: This is not seller-strategy proof though, since they can set a reserve price, i.e. a minimum price that needs to be met.</p> <p>DA with Prices Recap</p> <p>At each iteration of the algorithm, the unmatched buyer whose next-favorite option is \\((j,p)\\) proposes price \\(p\\) to good \\(j\\). Good \\(j\\) tentatively accepts if price \\(p\\) is higher than the prices it was offered so far.</p> <ul> <li>Proves that a competitive equilibrium exists.</li> <li>Fast running time (for discretized prices).</li> <li>Buyer-optimal and buyer-strategyproof.</li> </ul> <p>Competitive Equilibrium Recap</p> <p>By definition:</p> <ul> <li>Each buyer is matched to their favorite good (given prices).</li> <li>If no buyer is matched to good \\(j\\), then \\(p_j = 0\\).</li> </ul> <p>Properties:</p> <ul> <li>At equilibrium prices, buyers can independently choose their favorite goods without conflicts.</li> <li>A competitive equilibrium maximizes social welfare.</li> <li>A competitive equilibrium always exists and can be found efficiently (DA with prices).</li> </ul>"},{"location":"lecture6/#recap","title":"Recap","text":"<p>Market Equilibrium Recap</p> <p>Modeling buyers</p> <ul> <li>Ordinal vs. cardinal utilities (expressivity, fairness, combining outcomes).</li> <li>Idiosyncratic vs. fungible goods (and unit-demand vs. combinatorial demand).</li> </ul> <p>Competitive equilibrium (supply meets demand)</p> <ul> <li>Prices coordinate the allocation (i.e. each guest can book their favorite room without conflicts).</li> <li>Proof of existence with DA with prices algorithm (it is also a buyer-strategyproof mechanism).</li> <li>First Welfare theorem (social welfare maximized at equilibrium).</li> </ul>"},{"location":"lecture7/","title":"Market Failures","text":"<p>April 21, 2025</p> <p>The vanilla assumption at the foundation of classical microeconomics is that a free market (\"invisible hand\") naturally converges to an optimal outcome.</p> <p>Definition: Market Failure</p> <p>When the market fails to converge to an optimal outcome.</p> <p>It is important to understand what can go wrong in market design and which strategies exist for mitigating these issues. In this lecture, we focus on five types of market failures:</p> <ol> <li>Externalities and public goods</li> <li>Transaction costs</li> <li>Market thinness/monopolies</li> <li>Timing issues</li> <li>Information asymmetry</li> </ol>"},{"location":"lecture7/#externalities-and-public-goods","title":"Externalities and Public Goods","text":"<p>This is probably the most important market failure.</p> <p>Definition: Externality</p> <p>A side-effect on someone other than the seller/buyer.</p> <p>In other words, an externality is the net effect a transaction has on everyone else. For instance, if you are watching a lecture online, as a side-effect, your roommate's Wi-Fi is congested.</p> <p>An externality is a market failure when market participants don't have incentives to reduce negative externalities and increase positive externalities.</p> <p>Definition: Public Good</p> <p>Something that belongs to everybody but owned by nobody.</p> <p>A market failure occurs when market participants are under-incentivized to invest in public goods.</p> <p>Ecological damage is considered as \"the biggest market failure of all time.\" It is a side-effect of many economic activities, from hunting, to farming, and Bitcoin mining.</p> <p>The environment is a public good. Whose it it? Who is vested enough to protect it? When we pollute and hurt nature, it hurts everyone (it has a lot of externality on other people, with a large total effect), and we cannot just let the free market take care of the environment.</p> <p>What can we do about it?</p> <p>Ecological Damage Mitigation Strategies</p> <ul> <li>Pigouvian Tax: Named after British economist Arthur Cecil Pigou, the idea is to tax proportionally to the externality. An example of this is a carbon tax: if you pollute, you have to pay for how much you pollute, which incentivizes you to pollute less. Similarly, if you smoke, there are externalities for other people (such as second-hand smoking and more expensive health care), so it makes sense to tax you.</li> <li>Coasian bargaining: Named after British economist Ronald Coase, the idea is to auction off public goods. Everyone can throw their pollution into the air because no one owns it, but if someone owned all the air, they would charge you for polluting it. However, is it a great idea that someone owns all the air? </li> </ul> <p>Question: Is a library an example of a public good?</p> <p>Yes, it is a great example of a public good.</p> <p>Question: Can the government be a participant in the market, given that libraries and national parks are public goods, but on government land?</p> <p>We usually think of those as public goods and we think of the government not as a participant in the market but like an organization that is formed to take care of those public goods.</p> <p>Key point: Environmental impact is a huge example of market failure, even though it is a fairly simple one.</p>"},{"location":"lecture7/#transaction-costs","title":"Transaction Costs","text":"<p>Note: Computer science really kicks in to address this market failure.</p> <p>Costs associated with making transactions that prevent beneficial trades.</p> <p>Example</p> <p>I would be willing to pay \\(\\$10,000\\) for a boat. You are happy at home, so you would rather have \\(\\$9,900\\) than your boat. Should you sell me your boat?</p> <ul> <li>In theory: Yes! Regardless of price, we would be \\(\\$100\\) happier in total.</li> <li>In practice: Probably not, considering a \\(9\\%\\) sales tax.</li> </ul> <p>What can we do about it? Legally, not much: \"nothing is certain except death and taxes.\"</p> <p>Example</p> <p>I am hungry and willing to pay \\(\\$10\\) for an apple. Someone out there would rather have \\(\\$0.99\\) than their apple. Should I buy the apple from them?</p> <ul> <li>In theory: Yes!</li> <li>In practice: How would I find them? How would I send them the money?</li> </ul> <p>Finding someone with an apple might cost more than \\(\\$10\\), even though they are out there. There is another form of transaction cost (distinct from taxes), associated with search, finding matches, and payment.</p> <p>Transaction Costs Mitigation Strategies</p> <p>Facilitating transactions matters because, even if in theory I would rather than an apple, and someone would rather have \\(\\$0.99\\), finding them and paying them is hard.</p> <p>Lots of companies facilitate transactions: dating apps, Couchsurfing, Airbnb, ride hailing apps, Craigslist, eBay, Amazon, TaskRabbit, StubHub, Mechanical Turk, etc.</p>"},{"location":"lecture7/#market-thickness","title":"Market Thickness","text":"<p>Market thinness is actually one reason for high transaction costs.</p> <p>Definition: Thick Market</p> <p>We say that a market is thick is there are many buyers and many sellers.</p> <p>In a thick market:</p> <ul> <li>Sellers and buyers have a lot of options.</li> <li>Prices are at (or close to) market equilibrium.</li> <li>The outcome is welfare-maximizing.</li> </ul> <p>Example of a Thick Market: A market in Lagos, Nigeria. There are lots of people with lots of options to go from, and a lot of competition, so we expect the price to be at or close to market equilibrium.</p> <p>Definition: Thin Market</p> <p>We say that a market is thin is there are few buyers and many sellers.</p> <p>In a thin market:</p> <ul> <li>There are no buyers in sight.</li> <li>If somes buyers do come, sellers monopolize.</li> <li>The outcome is sub-optimal.</li> </ul> <p>Example of a Thin Market: One motorcycle selling ice cream in the Faysoum Desert, Egypt. There is one seller, and zero buyer. If a buyer shows up in the middle of the desert and really wants ice cream, the seller will hike up the price and take advantage of the fact that there is only one ice cream seller in the desert.</p> <p>Why are monopolies a problem?</p> <p>When we consider sellers' incentives, the First Welfare Theorem extends to good with reserve prices to cover costs. For example, an Airbnb host should never rent a room below the total of their cleaning cost, insurance, and hotel tax.</p> <p>However, sellers setting reserve prices is not strategyproof (like hospitals in DA). Moreover, sellers have simple, obvious manipulations (unlike hospitals, who had to know what other hospitals and doctors were bidding, sellers do not need that information).</p> <p>Example: Monopoly </p> <p>Let's consider a scenario with 1 seller (\\(true cost = 0\\)) and 1 buyer (\\(value = 100\\)):</p> <ul> <li>Any price between \\(0\\) and \\(100\\) would be a competitive equilibrium.</li> <li>DA-with-prices would suggest \\(p = 0\\) (buyer-optimal).</li> <li>The seller wants to set the reserve price to \\(100\\).</li> </ul> <p>On Airbnb, hosts set prices: nobody is actually running DA-with-prices. Let's recall that DA-with-prices is buyer-strategy proof but super not seller-strategy proof anyway.</p> <p>This is why monopolies are a real problem, especially in a thin market. However, there are ways to get around it: - Thick(er) market: As a market becomes very thick (it is far from a monopoly), then it approaches seller-strategy proof. We say that it is strategyproof-in-the-large. - Information limitation: Sellers still need to know the buyer\u2019s value to set a good reserve price, so if they don\u2019t really have a lot of information, even with a monopoly, then they cannot set a very high reserve price, so information limitation helps.</p> <p>What can we do about it?</p> <p>** Market Thinness Mitigation Strategies**</p> <p>If monopolies and thin markets are a problem, then one solution is to build a thick market (or make a market thicker).</p> <p>Solutions include:</p> <ul> <li>Spend a lot of resources on recruiting a lot of early adopters to get a thick market, and then retaining them.</li> <li>Scale markets, for instance by merging existing markets (see example of kidney exchange programs).</li> <li>Batch transactions:<ul> <li>Every once in a while, there is a new patient and a kidney that are available for a match, and we may wonder when we should try to clear the market. In many countries, they wait 3-4 months to have a batch and try to find matches, while in the US, the frequency is much higher, i.e. around 1 week. One reason for this is that there is competition between markets, so if one market is matching every few months, and another is matching every few weeks, then you can match quicker in the latter, but it may not be an optimal match, so there is a trade-off.</li> <li>In the context of ride-sharing/ride-hailing, when you order an Uber ride, it might wait for a couple of minutes to pool you with other people who are ordering a car to find the best matches between drivers and riders.</li> </ul> </li> </ul>"},{"location":"lecture7/#timing-issues","title":"Timing Issues","text":"<p>Timing Issue #1: Committing to Contracts Too Early (Market Unravelling)</p> <p>In the medical residency (doctor-hospitals) job market pre-NRMP: - 1900's-1940's: The market was decentralized, and hospitals were racing to make offers earlier. - If other hospitals were making offers in December, then you had to make offers in November (slightly more uncertainty, but much less competition). - If other hospitals were making offers in Novenber, ... - ... by 1945, hospitals were making job offers to first-year students.</p> <p>This is a market failure because commitments were made too early: first-year students don\u2019t know which speciality they want and how good they really are. There was a lot of uncertainty, it was hard to tell whether doctors were a good match, yet because of uncertainty, hospitals were sending offers too early.</p> <p>However, now, with the centralized system, offers are sent during the final year of study, so doctors know better what they want.</p> <p>Timing Issue #2: Exploding Offers</p> <p>An exploding offer is like a job offer that requires a very short response.</p> <p>One reason why one wants to give exploding offers is that, if a job offer is turned down, an employer needs to scramble to find someone else to fill the position.</p> <p>This type of constraint is amplified when:</p> <ul> <li>There is a limited time window for making offers, since you don\u2019t have more time to turn around.</li> <li>An employer is constrained to only hire exactly one person. If you hire 10 people, you can make 20 offers, but if you hire 1 person, you need to know very fast if they are going to accept the offer or not, so you put the pressure on the candidate.</li> </ul> <p>The de Gea transfer</p> <ul> <li>24 hours is a short time window to make a goalie transfer decision worth millions of euros.</li> <li>The reason of this is that FIFA has a limited time window for when you are allowed to transfer soccer players.</li> <li>Each team has exactly one starting goalie, so they cannot make offers to three goalies (they don\u2019t want to have 2 and they don\u2019t want to have 0).</li> </ul> <p>Note: David de Gea's transfer to Real Madrid from Manchester United in 2015 failed due to administrative issues, specifically the late submission of paperwork. The deal was agreed upon, including a part-exchange deal with Real Madrid's Keylor Navas. However, the paperwork was not submitted in time for the La Liga deadline, leading to the transfer collapse.</p> <p>An Extreme Case</p> <p>According to a 2005 applicant for federal judicial clerkships:</p> <p>\"I received the offer via voicemail while I was in flight to my second interview. The judge actually left 3 messages: the first to make the offer, the second to tell me that I should respond soon, and the third to rescind the offer. It was a 35-minute flight.\"</p> <p>Timing Issue #3: Not Waiting For The Market To Clear</p> <p>This is related to hiring psychologists:</p> <ul> <li>Starting at 9am, employers can call psychologists to offer them a position.</li> <li>They may say yes, and then call back and decline because they got a better offer.</li> <li>Then employers had to go back to the next psychologist on their preference list.</li> </ul> <p>You would hope that this was an employer-optimal DA mechanism.</p> <p>In practice:</p> <ul> <li>Employers were worried that psychologists would reject them at 3:59pm, right before the 4:00pm deadline, so they asked that candidates commit to an accepted offer and do not switch.</li> <li>Candidates started accepting offers that may have been their safety choices rather than their favorite choice.</li> <li>Then, employers started making commitments earlier and earlier.</li> </ul> <p>This resulted in a suboptimal matching and it was far from being strategyproof because psychologists started to strategize and think about whether they should commit to an employer that was a safety choice even though they may like another employer better, and therefore, it was not stable.</p> <p>This is another example of market unraveling.</p> <p>What can we do about it?</p> <p>Timing Issues Mitigation Strategies</p> <p>Solutions to deal with timing issues include:</p> <ul> <li>Use centralized matching systems to prevent this type of market unraveling, like we have seen for doctors and hospitals.</li> <li>Set rules that do not allow agents to make offers before certain dates, but in practice, these rules tend to fail, because there are incentives to bend the rules and still make early offers and ask for early commitments</li> <li>In practice, what does seem to work better is to have rules that allow candidates to accept and then cancel exploding offers, because if that is the norm, then it takes the kick out of exploding offers (what is the point of exploding offers if candidates can say yes immediately and then back out?).</li> </ul> <p>Question: Why does eliminating exploding offers solve this issue?</p> <p>For example, in the US, PhD candidates must accept or decline offers by April 15, and everyone know this is the rule. If Stanford says that candidate need to respond by April 1st, then the market unravels.</p> <p>But if the norm is that everyone knows that students can accept Stanford by April 1st, but then still change their mind and go to Berkeley by April 14th, then there is no point for Stanford to even ask candidates to accept by April 1st.</p> <p>By eliminating the ability for candidates to commit to exploding offers, it takes away the ability from universities to make exploding offers in the first place, and prevents the market from unraveling.</p>"},{"location":"lecture7/#information-asymmetries","title":"Information Asymmetries","text":"<p>Cr/NC vs. Letter Grade Game (In Class)</p> <p>At LovesFun University, students can choose between Cr/NC (Pass/Fail) and a letter grade after seeing their final grade for a class. It always counts for their major.</p> <p>In this in-class game, groups were asked to pick a random mock grade and discuss whether they wanted to opt for Cr/NC or a letter grade.</p> <p>Then, the following discussion questions were asked:</p> <ol> <li> <p>When you see an internship applicant with a Cr/NC on their transcript, what do you think their letter grade must have been? Answer: Probably a B or a C.</p> </li> <li> <p>What is the equilibrium of this game? Answer: The equilibrium of this game is when only people who got a C- take the credit instead of the letter grade. One caveat though is that employers do not really read transcripts, they care about the GPA, so a credit can be more favorable than a grade that lowers the GPA.</p> </li> </ol> <p>Question: What if a team that has a C- wants to show a letter grade?</p> <p>In practice, it does not make sense, because they would prefer to be mixed with people who got a B or a C on average rather than show a worse grade.</p> <p>Example: Market for Lemons</p> <p>Suppose that you want to buy a used car. You are willing to payer more for a car in good condition. Sellers are willing to sell bad cars for less (they are eager to replace their car).</p> Car Condition Buyer's Value Seller's Value Good 12 10 Bad 6 4 <p>Regardless of whether a car is good or bad, there is a positive gain from trade, i.e. the seller and the buyer are happier if they transact. This is why in an optimal market, you buy some car.</p> <p>Here, we say that there is information asymmetry, because the seller knows the car condition, but the buyer doesn't.</p> <p>Let \\(g \\in \\[0,1\\]\\) be the fraction of cars in the market that are in good condition. Let \\(h \\leq g\\) be the fraction of cars in good condition also available for sale.</p> <p>What do we expect at market equilibrium?</p> <p>Possible bad equilibrium:</p> <ul> <li>\\(h = 0\\), i.e. only bad cars are for sale.</li> <li>\\(Price \\leq 6\\) since buyers assume that cars for sale are in bad condition (i. the price has to be between \\(4\\)\u2014how much sellers want for their bad cars\u2014and \\(6\\)\u2014how much sellers are willing to pay for bad cars).</li> <li>Good sellers don't want to sell their cars (which is why \\(h = 0\\) is indeed an equilibrium).</li> </ul> <p>Possible good equilibrium:</p> <ul> <li>\\(h = g\\), i.e. all sellers are trying to sell their cars.</li> <li>The buyers' willingness to pay is: \\(12g + 6 \\cdot (1-g) = 6 + 6g\\) (i.e. \\(12\\) times the fraction of good cars, plus \\(6\\) times the fraction of bad cars).</li> <li>For good sellers to stay in the market, we need to have \\(6 + 6g \\geq 10\\), i.e. \\(g \\geq \\frac{2}{3}\\) (the value that buyers are willing to pay for a car in an unknown condition must at least be the value that sellers of good cars are expecting to create an expectation of a good deal.)</li> </ul> <p>This is not great, but it can get worse.</p> <p>Example: Market for Lemons (Even Worse)</p> <p>Let's assume that the market is split \\(\\frac{1}{3}\\), \\(\\frac{1}{3}\\), \\(\\frac{1}{3}\\) between good cars, bad cars, and lemons:</p> Car Condition Buyer's Value Seller's Value Good 12 10 Bad 6 4 Lemons 0 0 <p>If everyone sells, the buyer's value is at most \\(\\frac{1}{3} \\cdot (12 + 6 + 0) = 6\\). So, good sellers exit the market.</p> <p>This means that the buyer's value becomes \\(\\frac{1}{2} \\cdot (6 + 0) = 3\\). So, bad sellers also exit the market.</p> <p>Example: Market for Health Insurance</p> <p>The good equilibrium is that everyone buys a reasonably priced health insurance and everyone who has health insurance is taken car of.</p> <p>The problem is that insurance companies have an incentive to sell only/mostly to healthy people. For instance, selling health insurance to college students is probably a good deal (they are younger and in better health than the average person).</p> <p>If companies can sell health insurance targeted at healthier individuals/demographics, then competitors are left with less healthy people who may be more expensive to insure, which drives the prices up.</p> <p>However, healthy people do not want to buy health insurance when the prices are high, only people who have higher health costs are going to buy when prices are high.</p> <p>That means that health insurance companies have to raise their prices even more. In the extreme, perhaps no one has health insurance, which is a very bad equilibrium, because only unhealthy people buy very expensive health insurance and no one else is insured because it\u2019s too expensive.</p> <p>Example: Clickbait Content</p> <p>Content creators know whether their content is actually good content or not.</p> <p>In the good equilibrium, we mostly have interesting content and a lot of audience.</p> <p>However, there is the something called moral hazard, which is that it is easier and cheaper to create bad content with a clickbait-y title.</p> <p>So, if the number of clicks remains the same, then creators are incentivized to resort to clickbait. As a consequence, perhaps the good creators who do not do this go bankrupt because they cannot meet the market prices (given all the clickbait creators who flood the market).</p> <p>This results in a bad equilibrium, where we have mostly clickbait and users are not interested in it, so they don\u2019t click it.</p> <p>Definition: Adverse Selection</p> <p>Only lemons stay in the market due to information asymmetry.</p> <p>What can we do about it?</p> <p>Adverse Selection Mitigation Strategies</p> <ul> <li>Provide more information to both sides of the market<ul> <li>For example, if you sell your car, there are mandatory disclosures, you are required by law in many states to say if you have problems with your car.</li> <li>Before Obamacare, by law, you had to declare pre-existing health conditions, and that could hike up health insurance prices.</li> <li>You can invest in reputation systems (rating on Yelp, Amazon), which comes with lots of great incentives in CS questions.</li> </ul> </li> <li>Disallow the use of information on both sides on both sides of the market<ul> <li>For instance, with universal healthcare, you have to give health insurance to everyone, which does not allow health insurance companies to use information to pick and choose college students.</li> <li>In the stock market, there are regulations about insider trading, where if you have insider information about whether a stock is going to go up or down, you cannot use this information to invest.</li> </ul> </li> <li>Disincentivize lemons<ul> <li>For example, if a dealer has to give a guarantee over a car, they do not have an incentive to sell lemon cars (this is why many states disallow selling cars \u201cas is\u201d).</li> <li>In online ads, ad platforms may not only consider the price of the ads, but also the perceived quality of the ads, for instance if it is for a better product, which incentivizes advertisers to create higher quality content (ads).</li> </ul> </li> </ul> <p>Question: Why do we prefer providing more information for cars and less information for health insurance?</p> <p>One perspective is that, when people buy used cars, the side with the least information (the buyer) is an individual, while when people buy health insurance, the side with the most information (the buyer) is an individual, and maybe we want to side with and protect individuals, rather than larger companies.</p> <p>There may also be a notion of luxury (for a car) vs. necessity (for healthy insurance), all the more since, with a car, some things will break based on how you drive (so you have incentives to drive well), while you cannot prevent yourself from getting sick (even though you still have incentives to eat well and exercise).</p>"},{"location":"lecture7/#recap","title":"Recap","text":"<p>Market Failures Recap</p> <p>A market failure occurs when a market fails to converge to an optimal outcome.</p> <p>We have seen five types of market failures:</p> <ol> <li>Externalities and public goods</li> <li>Transaction costs</li> <li>Market thinness</li> <li>Timing issues</li> <li>Information asymmetry</li> </ol> <p>Mitigation strategies exist for all five types of market failures.</p>"},{"location":"lecture8/","title":"Single-Unit Auctions","text":"<p>April 23, 2025</p> <p>Auctions are valuable in settings where price discovery is needed, such as:</p> <ul> <li>Monopolies: Wireless spectrum auctions.</li> <li>Niche products: Rare items on eBay.</li> <li>Specialized products: Ad auctions.</li> </ul> <p>Auctions intersect significantly with Computer Science:</p> <ul> <li>Ad auctions fund many CS researchers.</li> <li>Fast auctions necessitate algorithmic bidders.</li> <li>Complex auctions require algorithmic auctioneers.</li> </ul>"},{"location":"lecture8/#case-of-one-buyer","title":"Case of One Buyer","text":"<p>Motivation: Digital goods pricing for maximizing revenue.</p> <ul> <li>Demand curves derived from users' willingness to pay.</li> <li>Revenue = Price \u00d7 Number of buyers willing to pay that price.</li> <li>Roger Myerson (1981): For a demand curve \\(D\\), a formula \\(p(D)\\) exists that maximizes revenue (optimal reserve price).</li> </ul> <p>Application: Ad auctions with specialized advertisers (single bidder per spot) using prior beliefs as demand curves.</p>"},{"location":"lecture8/#case-of-multiple-buyers","title":"Case of Multiple Buyers","text":"<p>Model: 1. Set of bidders \\(I\\), each with valuation \\(v_i\\). 2. Seller doesn't know valuations but has prior beliefs. 3. Bidder payoff: \\(v_i - p_i\\) if they win, else \\(-p_i\\).</p>"},{"location":"lecture8/#first-price-auction","title":"First-Price Auction","text":"<ul> <li>Each bidder submits a sealed bid \\(b_i\\).</li> <li>Highest bidder wins and pays their bid.</li> </ul> <p>Note: Equilibrium bids are below true valuations (\\(b_i &lt; v_i\\)).</p>"},{"location":"lecture8/#all-pay-auction","title":"All-Pay Auction","text":"<ul> <li>Each bidder submits a sealed bid \\(b_i\\).</li> <li>Highest bidder wins the item.</li> <li>All bidders pay their bids.</li> </ul> <p>Used for modeling non-auction scenarios (political donations, animal contests). Equilibrium bids are lower than valuations and generally lower than in first-price auctions.</p>"},{"location":"lecture8/#second-price-auction","title":"Second-Price Auction","text":"<ul> <li>Each bidder submits a sealed bid \\(b_i\\).</li> <li>Highest bidder wins, paying the second-highest bid.</li> </ul> <p>Theorem: Equilibrium in Second-Price Auctions</p> <p>Second-price auctions are strategyproof: truthful bidding (\\(b_i = v_i\\)) is dominant and thus an equilibrium.</p> <p>Proof: Fix other bids \\(b_j\\), let \\(b^{(-i)} = \\max_{j \\ne i} b_j\\). Two cases:</p> <ol> <li>If \\(v_i &lt; b^{(-i)}\\), bidder \\(i\\) prefers losing; bidding truthfully is optimal.</li> <li>If \\(v_i &gt; b^{(-i)}\\), bidder \\(i\\) prefers winning; again, truthful bid is optimal.</li> </ol> <p>Theorem: Payoffs of Second-Price Auctions</p> <p>Second-price auctions are individually rational at equilibrium: \\(v_i - p_i \\geq 0\\).</p> <p>Proof: If bidder doesn't win, payoff = 0. If bidder wins, pays at most their valuation, thus non-negative payoff.</p> <p>Theorem: Optimality</p> <p>In equilibrium, second-price auctions allocate the good to the highest-valued bidder.</p> <p>Proof: In equilibrium \\(b_i = v_i\\), thus highest bidder corresponds to highest valuation.</p>"},{"location":"lecture8/#revenue-maximization","title":"Revenue Maximization","text":"<p>Example: Full Information Scenario</p> <ul> <li>\\(A\\) values item at 1, \\(B\\) values at 2.</li> <li>Second-price auction: \\(B\\) wins, pays 1.</li> <li>First-price auction: Equilibrium bids near 1, revenue approximately 1.</li> </ul> <p>To handle uncertainty, we define Bayesian Nash Equilibrium:</p> <p>Definition: Bayesian Nash Equilibrium</p> <p>A strategy profile where each player's strategy maximizes expected payoff given beliefs about others' strategies.</p> <p>Example: Bayesian Agents</p> <ul> <li>\\(A,B\\) values drawn uniformly from [0,1].</li> <li>Second-price auction expected revenue: \\(\\frac{1}{3}\\).</li> <li>First-price auction equilibrium bids: \\(b_i = \\frac{v_i}{2}\\), also yielding expected revenue \\(\\frac{1}{3}\\).</li> </ul> <p>Theorem: Revenue Equivalence</p> <p>At equilibrium, expected payments depend only on the auction\u2019s allocation rule.</p> <p>Corollary: First-price, second-price, and all-pay auctions yield identical equilibrium revenue under Bayes-Nash equilibria with standard allocation rules (highest bidder wins).</p>"},{"location":"lecture8/#deviations-from-optimal-allocation","title":"Deviations from Optimal Allocation","text":"<p>Auctions don't always allocate to the highest valuation bidder:</p> <ul> <li>Overbidding in second-price auctions (highest bidder might bid excessively).</li> <li>All-pay auctions may have equilibria not awarding highest value bidder.</li> <li>Reserve prices might result in no winner.</li> </ul>"}]}