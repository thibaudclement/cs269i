{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>These course notes for Stanford CS269I are based on the content of Aviad Rubinstein's lectures in the Spring 2025 quarter. This site is developed by Thibaud Clement, based on Eric Gao's original course notes from the Winter 2023 quarter.</p> <p>Computers/algorithms control a lot of economic processes, including online retail, online advertising, algo-trading, cryptocurrencies, etc. There is a need to jointly understand economics and computation to analyze these applications. While relying on an increase of computing power (more GPUs) may be an option to compensate for inefficiencies, some specific cases may actually justify the need for efficient algorithms.</p> <p>When it comes to incentives, leveraging more GPUs is not the solution. Computer scientists often need to build systems that interact with other agents, such as users, algorithms, Large Language Models, and even other systems. It is crucial to consider that these other agents are going to behave selfishly to predict how they are going to interact with the system, and design an implement said accordingly.</p> <p>CS269I aims to teach students how to think about incentives (through lagnauge, frameworks, and practice), to get familiar with economic theory concepts (i.e. stable matching, proof-of-work, etc.), and analyze case studies to understand potential gaps between theory and practice.</p>"},{"location":"lecture1/","title":"One-Sided Matching and Serial Dictatorship","text":"<p>(3/31/2025)</p> <p>The Stanford Undergrad Housing Problem:</p> <ul> <li>There are \\( n \\) students, each with some preference over dorms.</li> <li>There are \\( m \\) dorms, each with some capacity (for simplicity, assume capacity is 1 for every dorm).</li> </ul> <p>We want to assign students to dorms.</p>"},{"location":"lecture1/#max-weight-matching","title":"Max Weight Matching","text":"<p>One possible solution is to maximize total happiness:</p> <p>Mechanism: Happiness Maximization</p> <ol> <li>Create a bipartite graph where:<ul> <li>One side represents students.</li> <li>The other side represents dorms.</li> <li>Each edge from a student to a dorm represents how much that student likes that dorm.</li> </ul> </li> <li>Determine the max-weight matching (i.e., Hungarian Algorithm).</li> </ol> <p>Key question: How does the algorithm determine how much each student likes each residency? Only the students themselves know how much they like each dorm, so we need to rely on students to tell the algorithm.</p> <p>Challenge: Each student has an incentive to exaggerate how much they like their favorite dorm and undercut how much they like other dorms (using a \"0\" or \"negative infinity\" weight for those dorms). In other words: max-weight bipartite matching allows students to game the system.</p> <p>This occurs because finding the matching that maximizes total happiness is conceptually right, but this naive max-weight matching fails to take incentives into account. Another possible algorithm is Serial Dictatorship.</p>"},{"location":"lecture1/#serial-dictatorship","title":"Serial Dictatorship","text":"<p>Mechanism: Serial Dictatorship</p> <ol> <li>Sort students in some fixed order (random, seniority, alphabetically, etc.).</li> <li>Go through the list in that order and allow each student to select their most preferred available dorm.</li> </ol> <p>How is this better? There can still be students unhappy with their result. Indeed, a common complaint about Serial Dictatorship is unfairness: the first student chooses whichever dorm they want, while the last student gets only the last pick.</p> <p>When sorting is random, Serial Dictatorship guarantees ex-ante fairness: before the random sorting, all students have equal chances. However, after the sorting happens (even randomly), Serial Dictatorship loses fairness: there's no guarantee of ex-post fairness.</p> <p>Definition (Mechanism): A mechanism consists of three things:</p> <ol> <li>A method of collecting inputs from agents,</li> <li>An algorithm that acts on the inputs,</li> <li>An action taken based on the output of the algorithm.</li> </ol> <p>In our context (Stanford Housing Problem), based on inputs (dorm preferences) and the algorithm (available dorm after each student's pick), the mechanism assigns dorms.</p> <p>Note: All three components matter because there's a feedback loop: how we use inputs impacts what students report as preferences. The naive bipartite matching approach encourages students to game the system by misreporting their preferences.</p> <p>Thus, when designing a mechanism, we must consider: - The algorithm itself, - How/where inputs come from, - Actions taken based on inputs, - How promised actions affect reported inputs.</p> <p>Definition (Strategyproofness/Truthfulness): A mechanism is strategyproof if it's in every agent's best interest to act truthfully, i.e., to report true preferences.</p> <p>Definition (Dominant Strategy Incentive Compatible - DSIC): (Formal Game Theory concept) A mechanism is dominant strategy incentive compatible if truthfulness is a dominant strategy for each participant. That is, being truthful is a best response regardless of other players' actions.</p> <p>Theorem: You Cannot Game Serial Dictatorship</p> <p>It is in every student's best interest to choose their favorite available dorm in their turn.</p> <p>Proof: Your room choice doesn't affect room availability before your turn. When your turn arrives, your best action is choosing your favorite available room.</p> <p>Definition (Pareto Optimality): An assignment \\( A \\) is Pareto optimal if, for any other assignment \\( B \\), there's at least one participant strictly preferring \\( A \\) over \\( B \\).</p> <p>Theorem: You Cannot Make Everyone Happier Without Making Someone Sadder</p> <p>Serial Dictatorship assignments are Pareto Optimal.</p> <p>Proof: Assume, for contradiction, a different assignment exists making everyone as happy or happier. Consider the first student assigned differently: All better dorms are already assigned identically to students before them. Thus, the first differing student gets a worse dorm and becomes less happy\u2014a contradiction. Therefore, Serial Dictatorship is Pareto Optimal.</p>"},{"location":"lecture1/#additional-discussion","title":"Additional Discussion","text":"<p>Why is truthfulness good? It prevents corruption and ensures fairness by removing any insider advantage.</p> <p>When is truthfulness important? When optimizing happiness, truthful inputs ensure correct objectives.</p> <p>When is relaxing truthfulness acceptable?  In contexts with social norms where truthfulness isn't expected (e.g., poker games), strategyproofness isn't crucial.</p> <p>Fairness/equity issues with Serial Dictatorship: Fairness depends on sorting order. Seniority or randomness provide fairness ex-ante but not necessarily ex-post.</p>"}]}