
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
        <link rel="prev" href="../lecture2/">
      
      
        <link rel="next" href="../lecture3b/">
      
      
      <link rel="icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.12">
    
    
      
        <title>Online Learning and Regret Minimization - Stanford CS269I Course Notes</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.2afb09e1.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.css">
    
      <link rel="stylesheet" href="../stylesheets/extra.css">
    
    <script>__md_scope=new URL("..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="indigo" data-md-color-accent="indigo">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#online-learning-and-regret-minimization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="Stanford CS269I Course Notes" class="md-header__button md-logo" aria-label="Stanford CS269I Course Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Stanford CS269I Course Notes
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Online Learning and Regret Minimization
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
  </nav>
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
          
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    



<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="Stanford CS269I Course Notes" class="md-nav__button md-logo" aria-label="Stanford CS269I Course Notes" data-md-component="logo">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54"/></svg>

    </a>
    Stanford CS269I Course Notes
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    One-Sided Matching and Serial Dictatorship
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Stable Two-Sided Matchings
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Online Learning and Regret Minimization
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Online Learning and Regret Minimization
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#regret-minimization" class="md-nav__link">
    <span class="md-ellipsis">
      Regret Minimization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regret-minimization-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Regret Minimization Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#swap-regret-minimization-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Swap-Regret Minimization Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regret-minimization-with-bandit-feedback" class="md-nav__link">
    <span class="md-ellipsis">
      Regret Minimization with Bandit Feedback
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recap" class="md-nav__link">
    <span class="md-ellipsis">
      Recap
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture3b/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Top Trading Cycles
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture4/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Equilibria in Games
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture5/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    P2P File-sharing Dilemma
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture6/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Market Equilibrium
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture7/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Market Failures
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture8/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Auctions
    
  </span>
  

      </a>
    </li>
  

    
      
      
  
  
  
  
    <li class="md-nav__item">
      <a href="../lecture9/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Prediction Markets and Information Cascades
    
  </span>
  

      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#regret-minimization" class="md-nav__link">
    <span class="md-ellipsis">
      Regret Minimization
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regret-minimization-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Regret Minimization Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#swap-regret-minimization-algorithms" class="md-nav__link">
    <span class="md-ellipsis">
      Swap-Regret Minimization Algorithms
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#regret-minimization-with-bandit-feedback" class="md-nav__link">
    <span class="md-ellipsis">
      Regret Minimization with Bandit Feedback
    </span>
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#recap" class="md-nav__link">
    <span class="md-ellipsis">
      Recap
    </span>
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  



<h1 id="online-learning-and-regret-minimization">Online Learning and Regret Minimization</h1>
<p><em>April 9, 2025</em></p>
<p>So far in this class, agents had dominant strategies, such as doctor reporting their true preferences in DA, so we had a good guess of what they should do. However, what should agents do without dominant strategies or even knowledge of the game's rules? In other words, what should selfish agents do when the mechanism is not strategyproof, and possibly not fully specified? Let's explore this question from a single agent's perspective.</p>
<h2 id="regret-minimization">Regret Minimization</h2>
<p>If we know which stocks are going to go up and down, we should buy the stocks that are going to go up and sell them before they are going to go down. But what do we do when we don’t know that? We want an algorithm that will explain how to invest well in the stock market First, we will try to use historical performance of stocks to determine what to do tomorrow.</p>
<div class="example">
<p><strong>Scenario 1:</strong> Consider investing in stocks without knowledge of future performance:</p>
<ul>
<li>There are <span class="arithmatex">\(n\)</span> possible actions (stocks).</li>
<li>We run an algorithm over <span class="arithmatex">\(T\)</span> days.</li>
<li>On day <span class="arithmatex">\(t\)</span>, the algorithm picks action <span class="arithmatex">\(ALG(t)\)</span>.</li>
<li>Reward for action <span class="arithmatex">\(i\)</span> on day <span class="arithmatex">\(t\)</span> is <span class="arithmatex">\(r_{i,t}\)</span>, bounded by <span class="arithmatex">\(-1 \leq r_{i,t} \leq 1\)</span>.</li>
</ul>
<p>Our goal is to maximize the sum of the rewards of the actions the algorithm chooses, namely <span class="arithmatex">\(\sum r_{ALG(t),t}\)</span>.</p>
</div>
<div class="remark">
<p><strong>Key Idea #1:</strong> Use historical performance to inform decisions.</p>
</div>
<p>A pretty natural guess is to try a greedy algorithm, called <strong>Follow The Leader</strong>, where on every day, we just take the action that has generated the most reward so far.</p>
<div class="definition">
<p><strong>Algorithm: Follow The Leader (FTL)</strong></p>
<p>On day <span class="arithmatex">\(t\)</span>: Take the action with the highest total reward up to day <span class="arithmatex">\(t-1\)</span>. Break ties arbitrarily.</p>
</div>
<p>How can we reason about whether this first idea is a good idea or not? We measure the success of online learning algorithms in terms of <strong>regret</strong>. Our benchmark is a fixed action over time: we are comparing with the best stock overall, not the best stock every single day. The intuition is that, if the algorithm has low regret, then we are doing almost as well as the best stock.</p>
<div class="definition">
<p><strong>Definition: External Regret</strong></p>
<p>External Regret = <span class="arithmatex">\( \underset{i}{\max} \underset{t}{\sum} r_{i,t} - \underset{t}{\sum} r_{ALG(t),t} \)</span></p>
</div>
<p><strong>Question: Can regret be negative?</strong> Yes! That is a fantastic case, when we are doing better than the best stock.</p>
<p>So, is FTL (i.e. the greedy algorithm that takes the best action every time) a good idea?</p>
<div class="theorem">
<p><strong>Claim 1:</strong> For independently, identically distributed (iid) rewards, FTL's expected regret is <span class="arithmatex">\(O(\sqrt{T \log(n)})\)</span>.</p>
</div>
<p>In other words, if, for each action, the rewards on different days are <strong>independently, identically distributed</strong>, aka <strong>iid</strong> (i.e. every day, the rewards are drawn from the same distribution—some stocks tend to do better, some stocks tend to do worse), then FTL has an expected regret in the order of <span class="arithmatex">\(O(\sqrt{T \log(n)})\)</span>.</p>
<p><strong>What does this mean?</strong> It means that, as long as <span class="arithmatex">\(T\)</span> is much bigger than <span class="arithmatex">\(\log(n)\)</span>, the whole thing inside the square root is going to be less than <span class="arithmatex">\(T^2\)</span> (because <span class="arithmatex">\(\log(n)\)</span> is less than <span class="arithmatex">\(T\)</span>, so <span class="arithmatex">\(T \log(n)\)</span> is less than <span class="arithmatex">\(T \cdot T\)</span>, so the whole thing is less than <span class="arithmatex">\(T^2\)</span>.</p>
<p>Regret is less than 1 on average, so regret is less than the number of days, and regret per day is diminishing (going to 0).</p>
<p>For instance, the NYSE only lists 2,000 companies, and <span class="arithmatex">\(\log(2000) \approx 11\)</span>, so it takes about <span class="arithmatex">\(11\)</span> days to start doing as well as the best stock on the NYSE (we can replace <span class="arithmatex">\(11\)</span> days with <span class="arithmatex">\(11\)</span> seconds if we are trading really fast).</p>
<div class="theorem">
<p><strong>Claim 2:</strong> No algorithm can outperform <span class="arithmatex">\(\Omega(\sqrt{T \log(n)})\)</span> with iid rewards.</p>
</div>
<p><strong>Why is this claim true?</strong></p>
<p>Let’s assume that the reward for every action, every day, is just a random coin flip (<span class="arithmatex">\(+1\)</span> or <span class="arithmatex">\(-1\)</span>), completely independently at random. So, in any algorithm based on history, the next choice is going to be a random coin flip.</p>
<p>However, if we have <span class="arithmatex">\(n\)</span> actions, one of them is going to do a little bit better than average (each one has a <span class="arithmatex">\(50/50\)</span> expectation, but one of them is going to be a little bit better than average). If we do the math of how much better than average it is going to be, it comes out to roughly this magic number of <span class="arithmatex">\(O(\sqrt{T \log(n)})\)</span>.</p>
<p>The number <span class="arithmatex">\(O(\sqrt{T \log(n)})\)</span> itself is not super important. However, the intuition is that it is the right bound: <span class="arithmatex">\(O(\sqrt{T \log(n)})\)</span> converges very fast, and as long as <span class="arithmatex">\(T\)</span> is much bigger than <span class="arithmatex">\(\log(n)\)</span>. Since <span class="arithmatex">\(\log(n)\)</span> is a tiny number, the algorithm is doing really well.</p>
<div class="remark">
<p><strong>Key Idea #2:</strong> External regret is the difference between the algorithm's total reward and the reward from the single best-in-hindsight action, i.e. </p>
<p>External Regret = <span class="arithmatex">\( \underset{i}{\max} \underset{t}{\sum} r_{i,t} - \underset{t}{\sum} r_{ALG(t),t} \)</span>.</p>
</div>
<h2 id="regret-minimization-algorithms">Regret Minimization Algorithms</h2>
<p>So, we are doing almost as well as the best stock. The catch is that the stock market is not iid: the stocks may go up one day, two days, three days, but they can only go up so much.</p>
<p>In an adversarial context, it is as if someone is trying to make it hard for us. FTL does not do well with adversarial input. In fact, no deterministic algorithm does well with adversarial input.</p>
<div class="remark">
<p><strong>Key Idea #3:</strong> To minimize regret in an adversarial context, we introduce randomness in the algorithm.</p>
</div>
<p>However, choosing actions uniformly at random may also be a bad idea, because some actions are consistently worse than others.</p>
<div class="remark">
<p><strong>Key Idea #4:</strong> We need an algorithm with a good balance between having enough randomness and paying attention to historical performance.</p>
</div>
<p>One idea to minimize regret is to pick a distribution of actions rather than a pure action.</p>
<div class="definition">
<p><strong>Algorithm: Follow The Regularized Leader (FTRL)</strong></p>
<p>On day <span class="arithmatex">\(t\)</span>, choose distribution <span class="arithmatex">\(x\)</span> maximizing <span class="arithmatex">\(\underset{t}{\sum} (r_{x,t} - \frac{1}{\eta}\varphi(x))\)</span> where:</p>
<ul>
<li><span class="arithmatex">\(r_{x,t}\)</span> provides the historical performance,</li>
<li><span class="arithmatex">\(\eta\)</span> balances randomness and history,</li>
<li><span class="arithmatex">\(\varphi\)</span> is the regularizer, which penalizes unbalanced distributions.</li>
</ul>
</div>
<p><strong>How does <span class="arithmatex">\(\varphi\)</span> work?</strong> <span class="arithmatex">\(\varphi\)</span> is a (usually convex) function that “penalizes” unbalanced distributions. For instance, if we have a distribution that puts all the weight on one stock, then <span class="arithmatex">\(\varphi\)</span> is going to determine that it is a bad distribution and give it a bad score. We can pick different functions that are going to give different performance.</p>
<p>Now, let's consider another famous algorithm.</p>
<div class="definition">
<p><strong>Algorithm: Multiplicative Weight Update (MWU)</strong></p>
<ul>
<li>Initialize weights <span class="arithmatex">\(z_{i,0} = 1\)</span> for all <span class="arithmatex">\(i\)</span>.</li>
<li>At day <span class="arithmatex">\(t\)</span>:<ul>
<li>Choose action <span class="arithmatex">\(i\)</span> with probability <span class="arithmatex">\(\frac{z_{i,t}}{\underset{j}{\sum} z_{j,t}}\)</span>.</li>
<li>Update weights: <span class="arithmatex">\(z_{i,t+1} \leftarrow z_{i,t} e^{\eta r_{i,t}}\)</span>.</li>
</ul>
</li>
</ul>
</div>
<p>The idea is that, at the beginning, all the weights are going to be 1. Each day, we are going to choose an action with a probability proportional to the weight. Then, there are updates: we multiply the weight of each action by the reward the the action got. If an action got a big positive reward, we are multiplying its weight by a big number, and its weight next time is going to be bigger. If an action got a small or negative reward, we are multiplying its weight by a small number, and its weight next time is going to be smaller.</p>
<p><span class="arithmatex">\(\eta\)</span> is called “learning rate”. It is a parameter that balances between randomness and FTRL:</p>
<ul>
<li>If we have a higher <span class="arithmatex">\(\eta\)</span>, it is putting a lot of weight into learning really fast (fitting to historical performance).</li>
<li>If we have a smaller <span class="arithmatex">\(\eta\)</span>, we are staying very close for a long time to the original distribution, which is uniform over all actions, so it is more random.</li>
</ul>
<p>Although we are not going to prove this claim in lecture, it turns out that this Multiplicative weight update (MWU) algorithm is the same as FTL when using the entropy regularizer.</p>
<div class="theorem">
<p><strong>Claim:</strong> MWU = FTRL with entropy regularizer <span class="arithmatex">\(\varphi(x) = - \sum x_i \log(x_i) \)</span>.</p>
</div>
<p>The point is that these are two ways to look at the same algorithm, trying to balance randomness and historical performance to avoid an adversarial example.</p>
<div class="theorem">
<p><strong>Theorem:</strong> MWU and FTRL achieve expected regret <span class="arithmatex">\(O(\sqrt{T \log(n)})\)</span>.</p>
</div>
<p>This is optimal, even against adversarial input. As long as the adversary does not know the inside randomness of the algorithm, this algorithm is completely adversary proof.</p>
<h2 id="swap-regret-minimization-algorithms">Swap-Regret Minimization Algorithms</h2>
<p>So far, we have formalized the idea of measuring online algorithms with regret, and we have seen actual optimal algorithms that minimize regret even against very adversarial input. However, what happens if there is an adversary who can look at our algorithm’s choices, and use that to do better than us? This is embarrassing: how can avoid that?</p>
<p>In Judo, winning is not about our own strength, but about using our opponent’s strength to make them fall over. Similarly, in the stock market, winning is not about knowing the market ourselves, but instead using oour opponent’s power to beat them.</p>
<div class="definition">
<p><strong>Definition: Swap-Regret (Internal Regret)</strong></p>
<p>Swap-Regret = <span class="arithmatex">\( \underset{\Phi : [n] \to [n]}{\max} \underset{t}{\sum} (r_{\Phi(ALG(t)),t} - r_{ALG(t),t}) \)</span> where <span class="arithmatex">\(\Phi\)</span> maps each action to another action.</p>
</div>
<p>In the context of external regret, our algorithm was just competing with the best single action (i.e. the best stock). In the context of swap regret, our algorithm has to compete with a friend, who is seeing what our algorithm is recommending, and using that to do something else.</p>
<p><strong>How is the swap function determined?</strong> We are taking the best of all possible swap functions?</p>
<p><strong>Between external regret and swap regret, which one is higher?</strong> The swap regret is always at least as high as the external regret.</p>
<p><strong>Which kind of regret is it better to minimize?</strong> It is better to minimize swap regret.</p>
<p>One way to see this is that we can have everything mapping to the same action (i.e. the best action in hindsight) in the swap function. So, if we can minimize the swap regret, it is better, although it is also much harder because the benchmark is more complex.</p>
<p><strong>Why don’t we use the best possible scenario to calculate regret?</strong> Indeed, another benchmark we can use is the best action each day (rather than the best action overall). The benchmark is just too high. For instance, if everyday we are playing a game where we flip a coin, and we need to determine whether it is going to be head or tail, there is no good algorithm for this, it is helpless.</p>
<p>We saw that, for external regret, there is an algorithm that can do as well as the best stock, even with adversarial input. It turns out that with swap regret, there is also a pretty good algorithm.</p>
<div class="definition">
<p><strong>Algorithm: Swap-Regret Minimization</strong></p>
<ul>
<li>Define meta-actions for each possible swap choice <span class="arithmatex">\(\Phi\)</span>.</li>
<li>Run MWU/FTRL on meta-actions.</li>
<li>At each iteration, solve for distribution <span class="arithmatex">\(x_t = \mathbb{E}[\Phi(x_t)]\)</span>.</li>
</ul>
</div>
<div class="theorem">
<p><strong>Claim:</strong> The Total Swap-Regret is in the order of <span class="arithmatex">\(O(\sqrt{T n \log(n)})\)</span>).</p>
</div>
<p>The total swap regret looks like the external regret, except that instead of <span class="arithmatex">\(n\)</span> action, we have to take the <span class="arithmatex">\(\log\)</span> of <span class="arithmatex">\(n^n\)</span> actions. This is a higher regret, so it is going to take more time to achieve vanishing regret.</p>
<p>For instance, if we think about the NYSE, with 2,000 actions, instead of <span class="arithmatex">\(\log(2000) \approx 11\)</span>, it is going to be <span class="arithmatex">\(\log(2000^{2000})\)</span>, which is <span class="arithmatex">\(\log(2000) \cdot 2000 \approx 11 \cdot 2000 \approx 22000\)</span>.</p>
<p>So, for instance, if <span class="arithmatex">\(T\)</span> is in days, it is going to take <span class="arithmatex">\(60\)</span> years for the swap to beat the entire stock exchange, but if we are talking in seconds, then <span class="arithmatex">\(22000\)</span> seconds is not very long (about <span class="arithmatex">\(6\)</span> hours).</p>
<h2 id="regret-minimization-with-bandit-feedback">Regret Minimization with Bandit Feedback</h2>
<div class="example">
<p><strong>Scenario 2:</strong> Consider a new portal/feed editor:</p>
<ul>
<li>We run an algorithm over <span class="arithmatex">\(T\)</span> days.</li>
<li>There are <span class="arithmatex">\(n\)</span> possible "arms" (i.e. actions), where an "arm" is a news category or a reporter, for instance.</li>
<li>On the <span class="arithmatex">\(t\)</span>-th day, a user comes, and we can show them one news item.</li>
<li>The reward <span class="arithmatex">\(r_{i,t}\)</span> measures how long the user engaged with the news item.</li>
</ul>
<p>Our goal is to minimize regret, namely <span class="arithmatex">\(\underset{i}{max} \underset{t}{\sum}(r_{i,t} - r_{ALG(t),t})\)</span>.</p>
</div>
<p><strong>Challenge:</strong> With partial (bandit) feedback, we see how long a user engaged with what we showed them, but we don’t know long the user would have engaged with the content we did not show them. Yet, we still have to compete with all the other actions.</p>
<p><strong>Solution:</strong> We need to arbitrate how much we want to learn about how each arm is doing (i.e. exploration) and how much we want to extract reward from the best arm (i.e. exploitation).</p>
<div class="remark">
<p><strong>Key Idea #5:</strong> To account for bandit/partial feedback, we may apply a similar regret minimization framework, with a trade-off between exploration and exploitation.</p>
</div>
<div class="definition">
<p><strong>Mechanism: Multi-Armed Bandit Model Warm-Up Algorithm</strong></p>
<ul>
<li>Try all possible arms to see which one yields the best reward.</li>
<li>Keep pulling on the best arm identified.</li>
</ul>
</div>
<p>This approach works well if rewards are fixed (e.g., one arm consistently rewards highly). However, it struggles with randomness, leading to potential overfitting.</p>
<p>How much exploration is needed? By the <strong>Law of Large Numbers</strong>, the average reward converges to the expectation over time. <strong>Concentration Inequalities</strong> quantify this convergence rate.</p>
<p>Using the Hoeffding Inequality:</p>
<div class="arithmatex">\[
Pr\left[\text{expectation} &gt; \text{average of samples} + \sqrt{\frac{X}{\text{number of samples}}}\right] &lt; e^{-2X}
\]</div>
<p>To ensure confidence across <span class="arithmatex">\(T\)</span> iterations:</p>
<div class="arithmatex">\[
Pr\left[\text{expectation} &gt; \text{average of samples} + \sqrt{\frac{2\ln(T)}{\text{number of samples}}}\right] &lt; \frac{1}{T^4}
\]</div>
<p>By setting <span class="arithmatex">\(X = 2\ln(T)\)</span>, we derive an <strong>Upper Confidence Bound (UCB)</strong>. This ensures minimal mistakes over the algorithm's runtime.</p>
<div class="definition">
<p><strong>Mechanism: UCB1 Algorithm</strong></p>
<p>Let <span class="arithmatex">\(\text{UCB}_i = \text{(average of $i$'s samples)} + \sqrt{\frac{2\ln(T)}{\text{number of $i$'s samples}}}\)</span>.</p>
<p>On day <span class="arithmatex">\(t\)</span>:</p>
<ul>
<li>Compute UCB for each arm.</li>
<li>Pull the arm with the maximum UCB (breaking ties randomly).</li>
</ul>
</div>
<p>This greedy algorithm balances exploration and exploitation:</p>
<ul>
<li><strong>Under-explored arms</strong> (few samples) have a high exploration incentive.</li>
<li><strong>High-performing arms</strong> (high averages) encourage exploitation.</li>
</ul>
<p>The expected regret under iid rewards (between -1 and 1) is in the order of <span class="arithmatex">\(O(\sqrt{nT\log(T)})\)</span> as long as <span class="arithmatex">\(T &gt; n\log(n)\)</span>, which is not as good as full feedback scenarios remains but reasonable for bandit feedback.</p>
<p>In other words:</p>
<ul>
<li><strong>Good:</strong> Regret per day approaches 0 if <span class="arithmatex">\(T &gt; n\log(n)\)</span>.</li>
<li><strong>Bad:</strong> Not as optimal as full feedback.</li>
<li><strong>Ugly:</strong> Requires iid rewards.</li>
</ul>
<p><strong>Caveat:</strong> Because UCB1 is a greedy, deterministic algorithm (with no randomness), we can construct an adversarial input.</p>
<p>Let's explore another algorithm that works for bandit feedback AND against an adversarial input. The idea is to use MWU again. The challenge is that in MWU, every time we want to update the weights of the actions, we need to know the rewards, but we don’t have the rewards. Instead, we are going to feed the algorithm pseudo-rewards, something that is going to replace the reward and hopefully work as well as the reward.</p>
<div class="definition">
<p><strong>Mechanism: Exp3 Algorithm</strong></p>
<p>On day <span class="arithmatex">\(t\)</span>:</p>
<ul>
<li>MWU selects an arm.</li>
<li>Define pseudo-rewards:<ul>
<li>If arm <span class="arithmatex">\(i\)</span> is not selected: <span class="arithmatex">\(\hat{r}_{i,t} = 0\)</span>.</li>
<li>If arm <span class="arithmatex">\(i\)</span> is selected: <span class="arithmatex">\(\hat{r}_{i,t} = \frac{r_{i,t}}{Pr[\text{selecting } i]}\)</span>.</li>
</ul>
</li>
</ul>
</div>
<p><strong>Pseudo-reward rationale:</strong></p>
<ul>
<li>Unselected actions yield zero (unknown) rewards.</li>
<li>Selected actions are scaled to compensate selection probability, ensuring unbiased estimates (<strong>Inverse Propensity Score</strong>).</li>
</ul>
<div class="remark">
<p><strong>Key Idea #6:</strong> The Inverse Propensity Score means that <span class="arithmatex">\(\hat{r}_{i,t}\)</span> is an unbiased estimator, i.e. <span class="arithmatex">\(\mathbb{E}[\hat{r}_{i,t}] = \mathbb{E}[{r}_{i,t}]\)</span>.</p>
</div>
<p>The expected regret for Exp3 is <span class="arithmatex">\(O(\sqrt{nT\log(n)})\)</span>, slightly worse than MWU with full feedback (i.e. <span class="arithmatex">\(O(\sqrt{T\log(n)})\)</span>, though better algorithms exist to reduce variance.</p>
<h2 id="recap">Recap</h2>
<div class="summary">
<p><strong>Key Ideas</strong></p>
<ul>
<li><strong>Key Idea #1:</strong> Use historical performance to forecast the future.</li>
<li><strong>Key Idea #2:</strong> Use regret to measure the success of algorithms.</li>
<li><strong>Key Idea #3:</strong> Use randomness as safety against adversarial inputs.</li>
<li><strong>Key Idea #4:</strong> Balance randomness and optimization.</li>
<li><strong>Key Idea #5:</strong> Balance exploration and exploitation.</li>
<li><strong>Key Idea #6:</strong> Use unbiased estimates of rewards.</li>
</ul>
</div>












                
              </article>
            </div>
          
          
<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "..", "features": ["navigation.instant", "navigation.sections"], "search": "../assets/javascripts/workers/search.f8cc74c7.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.c8b220af.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/katex.min.js"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/katex@0.16.10/dist/contrib/auto-render.min.js"></script>
      
        <script src="../javascript/katex-init.js"></script>
      
    
  </body>
</html>